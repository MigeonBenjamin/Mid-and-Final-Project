{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MID_TERMS_PROJECT / MID_TERMS_EXAMEN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center> Goal:</center>**\n",
    "The goal of this competition is to predict MDS-UPDR scores, which measure progression in patients with Parkinson's disease. The Movement Disorder Society-Sponsored Revision of the Unified Parkinson's Disease Rating Scale (MDS-UPDRS) is a comprehensive assessment of both motor and non-motor symptoms associated with Parkinson's. You will develop a model trained on data of protein and peptide levels over time in subjects with Parkinson’s disease versus normal age-matched control subjects.\n",
    "\n",
    "Your work could help provide important breakthrough information about which molecules change as Parkinson’s disease progresses.\n",
    "\n",
    "Source : https://www.kaggle.com/competitions/amp-parkinsons-disease-progression-prediction/overview\n",
    "\n",
    "Documentation for the Model Keras : https://www.tensorflow.org/guide/keras/sequential_model?hl=fr\n",
    "\n",
    "Documentation Loss Function : https://keras.io/api/losses/\n",
    "\n",
    "*CV means Cross Validation. This is the score in your validation set. In a competition, the LB normally is computed only 20-30 % test data. Everyday you submit to get a high score in the LB, even your CV is not good. It makes your model overfits with 20-30 % which is used for the LB.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "train_clinical_data = pd.read_csv(\"../Mid_term_project/train_clinical_data.csv\")\n",
    "train_peptides_data = pd.read_csv(\"../Mid_term_project/train_peptides.csv\")\n",
    "train_protiens_data = pd.read_csv(\"../Mid_term_project/train_proteins.csv\")\n",
    "supplemental_clinical_data = pd.read_csv(\"../Mid_term_project/supplemental_clinical_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>visit_month</th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "      <th>updrs_4</th>\n",
       "      <th>upd23b_clinical_state_on_medication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55_0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55_3</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55_6</td>\n",
       "      <td>55</td>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55_9</td>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>On</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55_12</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>On</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>65043_48</td>\n",
       "      <td>65043</td>\n",
       "      <td>48</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>65043_54</td>\n",
       "      <td>65043</td>\n",
       "      <td>54</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>65043_60</td>\n",
       "      <td>65043</td>\n",
       "      <td>60</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>65043_72</td>\n",
       "      <td>65043</td>\n",
       "      <td>72</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>65043_84</td>\n",
       "      <td>65043</td>\n",
       "      <td>84</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2615 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      visit_id  patient_id  visit_month  updrs_1  updrs_2  updrs_3  updrs_4  \\\n",
       "0         55_0          55            0     10.0      6.0     15.0      NaN   \n",
       "1         55_3          55            3     10.0      7.0     25.0      NaN   \n",
       "2         55_6          55            6      8.0     10.0     34.0      NaN   \n",
       "3         55_9          55            9      8.0      9.0     30.0      0.0   \n",
       "4        55_12          55           12     10.0     10.0     41.0      0.0   \n",
       "...        ...         ...          ...      ...      ...      ...      ...   \n",
       "2610  65043_48       65043           48      7.0      6.0     13.0      0.0   \n",
       "2611  65043_54       65043           54      4.0      8.0     11.0      1.0   \n",
       "2612  65043_60       65043           60      6.0      6.0     16.0      1.0   \n",
       "2613  65043_72       65043           72      3.0      9.0     14.0      1.0   \n",
       "2614  65043_84       65043           84      7.0      9.0     20.0      3.0   \n",
       "\n",
       "     upd23b_clinical_state_on_medication  \n",
       "0                                    NaN  \n",
       "1                                    NaN  \n",
       "2                                    NaN  \n",
       "3                                     On  \n",
       "4                                     On  \n",
       "...                                  ...  \n",
       "2610                                 Off  \n",
       "2611                                 Off  \n",
       "2612                                 Off  \n",
       "2613                                 Off  \n",
       "2614                                 Off  \n",
       "\n",
       "[2615 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clinical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 248 unique patient_id values\n",
      "Found 17 unique visit_month values\n"
     ]
    }
   ],
   "source": [
    "print(\"Found {:,d} unique patient_id values\".format(train_clinical_data[\"patient_id\"].nunique()))\n",
    "print(\"Found {:,d} unique visit_month values\".format(train_clinical_data[\"visit_month\"].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_id</th>\n",
       "      <th>visit_month</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>UniProt</th>\n",
       "      <th>Peptide</th>\n",
       "      <th>PeptideAbundance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00391</td>\n",
       "      <td>NEQEQPLGQWHLS</td>\n",
       "      <td>11254.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00533</td>\n",
       "      <td>GNPEPTFSWTK</td>\n",
       "      <td>102060.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00533</td>\n",
       "      <td>IEIPSSVQQVPTIIK</td>\n",
       "      <td>174185.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00533</td>\n",
       "      <td>KPQSAVYSTGSNGILLC(UniMod_4)EAEGEPQPTIK</td>\n",
       "      <td>27278.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00533</td>\n",
       "      <td>SMEQNGPGLEYR</td>\n",
       "      <td>30838.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981829</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>108</td>\n",
       "      <td>58648</td>\n",
       "      <td>Q9UHG2</td>\n",
       "      <td>ILAGSADSEGVAAPR</td>\n",
       "      <td>202820.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981830</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>108</td>\n",
       "      <td>58648</td>\n",
       "      <td>Q9UKV8</td>\n",
       "      <td>SGNIPAGTTVDTK</td>\n",
       "      <td>105830.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981831</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>108</td>\n",
       "      <td>58648</td>\n",
       "      <td>Q9Y646</td>\n",
       "      <td>LALLVDTVGPR</td>\n",
       "      <td>21257.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981832</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>108</td>\n",
       "      <td>58648</td>\n",
       "      <td>Q9Y6R7</td>\n",
       "      <td>AGC(UniMod_4)VAESTAVC(UniMod_4)R</td>\n",
       "      <td>5127.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981833</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>108</td>\n",
       "      <td>58648</td>\n",
       "      <td>Q9Y6R7</td>\n",
       "      <td>GATTSPGVYELSSR</td>\n",
       "      <td>12825.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>981834 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         visit_id  visit_month  patient_id UniProt  \\\n",
       "0            55_0            0          55  O00391   \n",
       "1            55_0            0          55  O00533   \n",
       "2            55_0            0          55  O00533   \n",
       "3            55_0            0          55  O00533   \n",
       "4            55_0            0          55  O00533   \n",
       "...           ...          ...         ...     ...   \n",
       "981829  58648_108          108       58648  Q9UHG2   \n",
       "981830  58648_108          108       58648  Q9UKV8   \n",
       "981831  58648_108          108       58648  Q9Y646   \n",
       "981832  58648_108          108       58648  Q9Y6R7   \n",
       "981833  58648_108          108       58648  Q9Y6R7   \n",
       "\n",
       "                                       Peptide  PeptideAbundance  \n",
       "0                                NEQEQPLGQWHLS          11254.30  \n",
       "1                                  GNPEPTFSWTK         102060.00  \n",
       "2                              IEIPSSVQQVPTIIK         174185.00  \n",
       "3       KPQSAVYSTGSNGILLC(UniMod_4)EAEGEPQPTIK          27278.90  \n",
       "4                                 SMEQNGPGLEYR          30838.70  \n",
       "...                                        ...               ...  \n",
       "981829                         ILAGSADSEGVAAPR         202820.00  \n",
       "981830                           SGNIPAGTTVDTK         105830.00  \n",
       "981831                             LALLVDTVGPR          21257.60  \n",
       "981832        AGC(UniMod_4)VAESTAVC(UniMod_4)R           5127.26  \n",
       "981833                          GATTSPGVYELSSR          12825.90  \n",
       "\n",
       "[981834 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_peptides_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Found {:,d} unique patient_id values\".format(train_peptides_data[\"patient_id\"].nunique()))\n",
    "print(\"Found {:,d} unique visit_month values\".format(train_peptides_data[\"visit_month\"].nunique()))\n",
    "print(\"Found {:,d} unique peptide values\".format(train_peptides_data[\"Peptide\"].nunique()))\n",
    "print(\"Found {:,d} unique UniProt values\".format(train_peptides_data[\"UniProt\"].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_id</th>\n",
       "      <th>visit_month</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>UniProt</th>\n",
       "      <th>NPX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00391</td>\n",
       "      <td>11254.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00533</td>\n",
       "      <td>732430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00584</td>\n",
       "      <td>39585.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O14498</td>\n",
       "      <td>41526.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O14773</td>\n",
       "      <td>31238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232736</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>108</td>\n",
       "      <td>58648</td>\n",
       "      <td>Q9UBX5</td>\n",
       "      <td>27387.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232737</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>108</td>\n",
       "      <td>58648</td>\n",
       "      <td>Q9UHG2</td>\n",
       "      <td>369437.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232738</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>108</td>\n",
       "      <td>58648</td>\n",
       "      <td>Q9UKV8</td>\n",
       "      <td>105830.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232739</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>108</td>\n",
       "      <td>58648</td>\n",
       "      <td>Q9Y646</td>\n",
       "      <td>21257.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232740</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>108</td>\n",
       "      <td>58648</td>\n",
       "      <td>Q9Y6R7</td>\n",
       "      <td>17953.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232741 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         visit_id  visit_month  patient_id UniProt       NPX\n",
       "0            55_0            0          55  O00391   11254.3\n",
       "1            55_0            0          55  O00533  732430.0\n",
       "2            55_0            0          55  O00584   39585.8\n",
       "3            55_0            0          55  O14498   41526.9\n",
       "4            55_0            0          55  O14773   31238.0\n",
       "...           ...          ...         ...     ...       ...\n",
       "232736  58648_108          108       58648  Q9UBX5   27387.8\n",
       "232737  58648_108          108       58648  Q9UHG2  369437.0\n",
       "232738  58648_108          108       58648  Q9UKV8  105830.0\n",
       "232739  58648_108          108       58648  Q9Y646   21257.6\n",
       "232740  58648_108          108       58648  Q9Y6R7   17953.1\n",
       "\n",
       "[232741 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_protiens_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Found {:,d} unique patient_id values\".format(train_protiens_data[\"patient_id\"].nunique()))\n",
    "print(\"Found {:,d} unique visit_month values\".format(train_protiens_data[\"visit_month\"].nunique()))\n",
    "print(\"Found {:,d} unique UniProt values\".format(train_protiens_data[\"UniProt\"].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplemental_clinical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Found {:,d} unique patient_id values\".format(supplemental_clinical_data[\"patient_id\"].nunique()))\n",
    "print(\"Found {:,d} unique visit_month values\".format(supplemental_clinical_data[\"visit_month\"].nunique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center> Observations : </center>**\n",
    " \n",
    "**1. Train_clinical_data :**\n",
    "\n",
    "We have here 248 unique patients with each a certain degree of disease according to the updrs_[1-4].\n",
    " \n",
    " updrs_[1-4] - The patient's score for part N of the Unified Parkinson's Disease Rating Scale. Higher numbers indicate more severe symptoms. Each sub-section covers a distinct category of symptoms, such as mood and behavior for Part 1 and motor functions for Part 3.\n",
    "\n",
    "* Found 248 unique patient_id values\n",
    "* Found 17 unique visit_month values\n",
    " \n",
    "**2. Train_peptides_data :**\n",
    "\n",
    "UniProt - The UniProt ID code for the associated protein. There are often several peptides per protein.\n",
    "Peptide - The sequence of amino acids included in the peptide. Some rare annotations may not be included in the table. The test set may include peptides not found in the train set.\n",
    "\n",
    "* Found 248 unique patient_id values\n",
    "* Found 968 unique peptide values\n",
    "* Found 227 unique UniProt values\n",
    "\n",
    "**3. Train_protiens_data :**\n",
    "\n",
    "* Found 248 unique patient_id values\n",
    "* Found 227 unique UniProt values\n",
    "\n",
    "**4. Dupplemental_clinical_data :**\n",
    "\n",
    "There is no protein records. We can use this data to observe patient taking a PD (Parkinson's disease) medicine and the degree of disease updrs_[1-4].\n",
    "\n",
    "* Found 771 unique patient_id values\n",
    "* Found 8 unique visit_month values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of NaN in the train_clinial_data\n",
    "\n",
    "null_count = train_clinical_data.isna().sum().to_list()\n",
    "plt.bar(list(train_clinical_data),null_count)\n",
    "plt.xticks(list(train_clinical_data), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of NaN in the train_peptides_data\n",
    "\n",
    "null_count_1 = train_peptides_data.isna().sum().to_list()\n",
    "print(null_count_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of NaN in the train_protiens_data\n",
    "\n",
    "null_count_2 = train_protiens_data.isna().sum().to_list()\n",
    "print(null_count_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of NaN in the supplemental_clinical_data\n",
    "\n",
    "null_count_3 = supplemental_clinical_data.isna().sum().to_list()\n",
    "plt.bar(list(supplemental_clinical_data),null_count_3)\n",
    "plt.xticks(list(supplemental_clinical_data), rotation=90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center> Observations 1 : </center>**\n",
    "\n",
    "We can see that the train_peptides_data and train_protein_data have no NAN value which is great. \n",
    "\n",
    "However, we have a lot of rows from train_clinical_data with NAN value  : 1327 upd23b_clinical_state_on_medication / 2615 rows = 0.507, half of the rows have NAN value.\n",
    "\n",
    "However, we have a lot of rows from supplemental_clinical_data with NAN value  : 1101 upd23b_clinical_state_on_medication / 2223 rows = 0.495, half of the rows have NAN value.\n",
    "\n",
    "One solution of this problem is : replace the NaN value by the previous value\n",
    "\n",
    "**Now, I'm going to concat the 2 dataset from clinical and supplemental_clinical and see how the updrs is evolving over the months and remove the NaN values**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data = pd.concat([train_clinical_data, supplemental_clinical_data]).dropna()\n",
    "concat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Found {:,d} unique patient_id values\".format(concat_data[\"patient_id\"].nunique()))\n",
    "print(\"Found {:,d} unique vist_month values\".format(concat_data[\"visit_month\"].nunique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How are Updrs evolved over months for the concatenation data with considereing is medication on or off?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort data over months and plot the mean of Updrs from all patient.\n",
    "\n",
    "Medication on and off data were taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_data = concat_data.sort_values(by = ['visit_month'])   \n",
    "\n",
    "visit_month = clinical_data[\"visit_month\"].tolist()\n",
    "\n",
    "updrs_1 = clinical_data[\"updrs_1\"].tolist()\n",
    "updrs_2 = clinical_data[\"updrs_2\"].tolist()\n",
    "updrs_3 = clinical_data[\"updrs_3\"].tolist()\n",
    "updrs_4 = clinical_data[\"updrs_4\"].tolist()\n",
    "res = {}\n",
    "mois = []\n",
    "mean_updrs_1 = []\n",
    "mean_updrs_2 = []\n",
    "mean_updrs_3 = []\n",
    "mean_updrs_4 = []\n",
    "\n",
    "ancienne_position = 0\n",
    "\n",
    "for i in visit_month:\n",
    "    res[i] = visit_month.count(i)\n",
    "for i in res:\n",
    "    mois.append(i)\n",
    "\n",
    "for i in res:\n",
    "    sum = 0\n",
    "    for j in range(res[i]):\n",
    "        sum = updrs_1[j + ancienne_position] + sum\n",
    "    ancienne_position = res[i]\n",
    "    mean_updrs_1.append(sum/res[i])\n",
    "\n",
    "for i in res:\n",
    "    sum = 0\n",
    "    for j in range(res[i]):\n",
    "        sum = updrs_2[j + ancienne_position] + sum\n",
    "    ancienne_position = res[i]\n",
    "    mean_updrs_2.append(sum/res[i])\n",
    "\n",
    "for i in res:\n",
    "    sum = 0\n",
    "    for j in range(res[i]):\n",
    "        sum = updrs_3[j + ancienne_position] + sum\n",
    "    ancienne_position = res[i]\n",
    "    mean_updrs_3.append(sum/res[i])\n",
    "\n",
    "for i in res:\n",
    "    sum = 0\n",
    "    for j in range(res[i]):\n",
    "        sum = updrs_4[j + ancienne_position] + sum\n",
    "    ancienne_position = res[i]\n",
    "    mean_updrs_4.append(sum/res[i])\n",
    "\n",
    "print(\"Months:\",mois)\n",
    "\n",
    "figure, axis = plt.subplots(2, 2)\n",
    "\n",
    "axis[0, 0].scatter(mois, mean_updrs_1)\n",
    "axis[0, 0].set_title(\"mean_updrs_1 over months\")\n",
    "  \n",
    "\n",
    "axis[0, 1].scatter(mois, mean_updrs_2)\n",
    "axis[0, 1].set_title(\"mean_updrs_2 over months\")\n",
    "  \n",
    "\n",
    "axis[1, 0].scatter(mois, mean_updrs_3)\n",
    "axis[1, 0].set_title(\"mean_updrs_3 over months\")\n",
    "\n",
    "axis[1, 1].scatter(mois, mean_updrs_4)\n",
    "axis[1, 1].set_title(\"mean_updrs_4 over months\")\n",
    "  \n",
    "figure.tight_layout(pad=2.5)\n",
    "plt.show()\n",
    "\n",
    "figure1, axis1 = plt.subplots(2, 2)\n",
    "\n",
    "axis1[0, 0].plot(mois, mean_updrs_1)\n",
    "axis1[0, 0].set_title(\"mean_updrs_1 over months\")\n",
    "  \n",
    "\n",
    "axis1[0, 1].plot(mois, mean_updrs_2)\n",
    "axis1[0, 1].set_title(\"mean_updrs_2 over months\")\n",
    "  \n",
    "\n",
    "axis1[1, 0].plot(mois, mean_updrs_3)\n",
    "axis1[1, 0].set_title(\"mean_updrs_3 over months\")\n",
    "\n",
    "axis1[1, 1].plot(mois, mean_updrs_4)\n",
    "axis1[1, 1].set_title(\"mean_updrs_4 over months\")\n",
    "  \n",
    "figure1.tight_layout(pad=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort data over months and plot the mean of Updrs from all patient.\n",
    "\n",
    "Only Medication off data were taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_data = concat_data.sort_values(by = [\"visit_month\"])   \n",
    "\n",
    "visit_month_off = clinical_data[clinical_data[\"upd23b_clinical_state_on_medication\"]==\"Off\"][\"visit_month\"].tolist()\n",
    "\n",
    "updrs_1_off = clinical_data[clinical_data[\"upd23b_clinical_state_on_medication\"]==\"Off\"][\"updrs_1\"].tolist()\n",
    "updrs_2_off = clinical_data[clinical_data[\"upd23b_clinical_state_on_medication\"]==\"Off\"][\"updrs_2\"].tolist()\n",
    "updrs_3_off = clinical_data[clinical_data[\"upd23b_clinical_state_on_medication\"]==\"Off\"][\"updrs_3\"].tolist()\n",
    "updrs_4_off = clinical_data[clinical_data[\"upd23b_clinical_state_on_medication\"]==\"Off\"][\"updrs_4\"].tolist()\n",
    "res = {}\n",
    "mois = []\n",
    "mean_updrs_1_off = []\n",
    "mean_updrs_2_off = []\n",
    "mean_updrs_3_off = []\n",
    "mean_updrs_4_off = []\n",
    "\n",
    "ancienne_position = 0\n",
    "\n",
    "for i in visit_month_off:\n",
    "    res[i] = visit_month_off.count(i)\n",
    "for i in res:\n",
    "    mois.append(i)\n",
    "\n",
    "for i in res:\n",
    "    sum = 0\n",
    "    for j in range(res[i]):\n",
    "        sum = updrs_1_off[j + ancienne_position] + sum\n",
    "    ancienne_position = res[i]\n",
    "    mean_updrs_1_off.append(sum/res[i])\n",
    "\n",
    "for i in res:\n",
    "    sum = 0\n",
    "    for j in range(res[i]):\n",
    "        sum = updrs_2_off[j + ancienne_position] + sum\n",
    "    ancienne_position = res[i]\n",
    "    mean_updrs_2_off.append(sum/res[i])\n",
    "\n",
    "for i in res:\n",
    "    sum = 0\n",
    "    for j in range(res[i]):\n",
    "        sum = updrs_3_off[j + ancienne_position] + sum\n",
    "    ancienne_position = res[i]\n",
    "    mean_updrs_3_off.append(sum/res[i])\n",
    "\n",
    "for i in res:\n",
    "    sum = 0\n",
    "    for j in range(res[i]):\n",
    "        sum = updrs_4_off[j + ancienne_position] + sum\n",
    "    ancienne_position = res[i]\n",
    "    mean_updrs_4_off.append(sum/res[i])\n",
    "\n",
    "print(\"Months:\",mois)\n",
    "\n",
    "figure, axis = plt.subplots(2, 2)\n",
    "\n",
    "axis[0, 0].scatter(mois, mean_updrs_1_off)\n",
    "axis[0, 0].set_title(\"mean_updrs_1_off over months\")\n",
    "  \n",
    "\n",
    "axis[0, 1].scatter(mois, mean_updrs_2_off)\n",
    "axis[0, 1].set_title(\"mean_updrs_2_off over months\")\n",
    "  \n",
    "\n",
    "axis[1, 0].scatter(mois, mean_updrs_3_off)\n",
    "axis[1, 0].set_title(\"mean_updrs_3_off over months\")\n",
    "\n",
    "axis[1, 1].scatter(mois, mean_updrs_4_off)\n",
    "axis[1, 1].set_title(\"mean_updrs_4_off over months\")\n",
    "  \n",
    "figure.tight_layout(pad=2.5)\n",
    "plt.show()\n",
    "\n",
    "figure1, axis1 = plt.subplots(2, 2)\n",
    "\n",
    "axis1[0, 0].plot(mois, mean_updrs_1_off)\n",
    "axis1[0, 0].set_title(\"mean_updrs_1_off over months\")\n",
    "  \n",
    "\n",
    "axis1[0, 1].plot(mois, mean_updrs_2_off)\n",
    "axis1[0, 1].set_title(\"mean_updrs_2_off over months\")\n",
    "  \n",
    "\n",
    "axis1[1, 0].plot(mois, mean_updrs_3_off)\n",
    "axis1[1, 0].set_title(\"mean_updrs_3_off over months\")\n",
    "\n",
    "axis1[1, 1].plot(mois, mean_updrs_4_off)\n",
    "axis1[1, 1].set_title(\"mean_updrs_4_off over months\")\n",
    "  \n",
    "figure1.tight_layout(pad=2.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort data over months and plot the mean of Updrs from all patient.\n",
    "\n",
    "Only Medication on data were taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_data = concat_data.sort_values(by = [\"visit_month\"])   \n",
    "\n",
    "visit_month_on = clinical_data[clinical_data[\"upd23b_clinical_state_on_medication\"]==\"On\"][\"visit_month\"].tolist()\n",
    "\n",
    "updrs_1_on = clinical_data[clinical_data[\"upd23b_clinical_state_on_medication\"]==\"On\"][\"updrs_1\"].tolist()\n",
    "updrs_2_on = clinical_data[clinical_data[\"upd23b_clinical_state_on_medication\"]==\"On\"][\"updrs_2\"].tolist()\n",
    "updrs_3_on = clinical_data[clinical_data[\"upd23b_clinical_state_on_medication\"]==\"On\"][\"updrs_3\"].tolist()\n",
    "updrs_4_on = clinical_data[clinical_data[\"upd23b_clinical_state_on_medication\"]==\"On\"][\"updrs_4\"].tolist()\n",
    "res = {}\n",
    "mois = []\n",
    "mean_updrs_1_on = []\n",
    "mean_updrs_2_on = []\n",
    "mean_updrs_3_on = []\n",
    "mean_updrs_4_on = []\n",
    "\n",
    "ancienne_position = 0\n",
    "\n",
    "for i in visit_month_on:\n",
    "    res[i] = visit_month_on.count(i)\n",
    "for i in res:\n",
    "    mois.append(i)\n",
    "\n",
    "for i in res:\n",
    "    sum = 0\n",
    "    for j in range(res[i]):\n",
    "        sum = updrs_1_on[j + ancienne_position] + sum\n",
    "    ancienne_position = res[i]\n",
    "    mean_updrs_1_on.append(sum/res[i])\n",
    "\n",
    "for i in res:\n",
    "    sum = 0\n",
    "    for j in range(res[i]):\n",
    "        sum = updrs_2_on[j + ancienne_position] + sum\n",
    "    ancienne_position = res[i]\n",
    "    mean_updrs_2_on.append(sum/res[i])\n",
    "\n",
    "for i in res:\n",
    "    sum = 0\n",
    "    for j in range(res[i]):\n",
    "        sum = updrs_3_on[j + ancienne_position] + sum\n",
    "    ancienne_position = res[i]\n",
    "    mean_updrs_3_on.append(sum/res[i])\n",
    "\n",
    "for i in res:\n",
    "    sum = 0\n",
    "    for j in range(res[i]):\n",
    "        sum = updrs_4_on[j + ancienne_position] + sum\n",
    "    ancienne_position = res[i]\n",
    "    mean_updrs_4_on.append(sum/res[i])\n",
    "\n",
    "print(\"Months:\",mois)\n",
    "\n",
    "figure, axis = plt.subplots(2, 2)\n",
    "\n",
    "axis[0, 0].scatter(mois, mean_updrs_1_on)\n",
    "axis[0, 0].set_title(\"mean_updrs_1_on over months\")\n",
    "  \n",
    "\n",
    "axis[0, 1].scatter(mois, mean_updrs_2_on)\n",
    "axis[0, 1].set_title(\"mean_updrs_2_on over months\")\n",
    "  \n",
    "\n",
    "axis[1, 0].scatter(mois, mean_updrs_3_on)\n",
    "axis[1, 0].set_title(\"mean_updrs_3_on over months\")\n",
    "\n",
    "axis[1, 1].scatter(mois, mean_updrs_4_on)\n",
    "axis[1, 1].set_title(\"mean_updrs_4_on over months\")\n",
    "  \n",
    "figure.tight_layout(pad=2.5)\n",
    "plt.show()\n",
    "\n",
    "figure1, axis1 = plt.subplots(2, 2)\n",
    "\n",
    "axis1[0, 0].plot(mois, mean_updrs_1_on)\n",
    "axis1[0, 0].set_title(\"mean_updrs_1_on over months\")\n",
    "  \n",
    "\n",
    "axis1[0, 1].plot(mois, mean_updrs_2_on)\n",
    "axis1[0, 1].set_title(\"mean_updrs_2_on over months\")\n",
    "  \n",
    "\n",
    "axis1[1, 0].plot(mois, mean_updrs_3_on)\n",
    "axis1[1, 0].set_title(\"mean_updrs_3_on over months\")\n",
    "\n",
    "axis1[1, 1].plot(mois, mean_updrs_4_on)\n",
    "axis1[1, 1].set_title(\"mean_updrs_4_on over months\")\n",
    "  \n",
    "figure1.tight_layout(pad=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center> Observations 2 : </center>**\n",
    "\n",
    "If the medicine is off, the updrs[1_4] are constant of the months.\n",
    "\n",
    "If the medicine is on, the updrs[1_4] are slowed by the medicine, we can see an increase around the 75th month.\n",
    "\n",
    "In general, the updr[1_4] are increasing over the month, with or not the medicine.\n",
    "\n",
    "We can maybe think about a regression to predict the value of the updrs[1_4]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-arrange the data : Merging the percentage of protein and the updrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_id</th>\n",
       "      <th>visit_month</th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "      <th>updrs_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55_3</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55_6</td>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55_9</td>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55_12</td>\n",
       "      <td>12</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610</th>\n",
       "      <td>65043_48</td>\n",
       "      <td>48</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>65043_54</td>\n",
       "      <td>54</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2612</th>\n",
       "      <td>65043_60</td>\n",
       "      <td>60</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2613</th>\n",
       "      <td>65043_72</td>\n",
       "      <td>72</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>65043_84</td>\n",
       "      <td>84</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2615 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      visit_id  visit_month  updrs_1  updrs_2  updrs_3  updrs_4\n",
       "0         55_0            0     10.0      6.0     15.0      NaN\n",
       "1         55_3            3     10.0      7.0     25.0      NaN\n",
       "2         55_6            6      8.0     10.0     34.0      NaN\n",
       "3         55_9            9      8.0      9.0     30.0      0.0\n",
       "4        55_12           12     10.0     10.0     41.0      0.0\n",
       "...        ...          ...      ...      ...      ...      ...\n",
       "2610  65043_48           48      7.0      6.0     13.0      0.0\n",
       "2611  65043_54           54      4.0      8.0     11.0      1.0\n",
       "2612  65043_60           60      6.0      6.0     16.0      1.0\n",
       "2613  65043_72           72      3.0      9.0     14.0      1.0\n",
       "2614  65043_84           84      7.0      9.0     20.0      3.0\n",
       "\n",
       "[2615 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's dive into deep, and firstly I drop the state on medication and patient_id because I cant handle every parameters for nox.\n",
    "\n",
    "train = train_clinical_data.drop([\"upd23b_clinical_state_on_medication\", \"patient_id\"],axis=1)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 248 unique patient_id values\n",
      "Found 15 unique vist_month values\n"
     ]
    }
   ],
   "source": [
    "# Let's merge the train_protiens and train_peptides data.\n",
    "\n",
    "pep_pro = pd.merge(train_protiens_data, train_peptides_data, on=['visit_id', 'visit_month', 'patient_id', 'UniProt'])\n",
    "print(\"Found {:,d} unique patient_id values\".format(pep_pro[\"patient_id\"].nunique()))\n",
    "print(\"Found {:,d} unique vist_month values\".format(pep_pro[\"visit_month\"].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_id</th>\n",
       "      <th>visit_month</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>UniProt</th>\n",
       "      <th>NPX</th>\n",
       "      <th>Peptide</th>\n",
       "      <th>PeptideAbundance</th>\n",
       "      <th>Percentage_of_pep_in_Uniprot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00391</td>\n",
       "      <td>11254.3</td>\n",
       "      <td>NEQEQPLGQWHLS</td>\n",
       "      <td>11254.30</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00533</td>\n",
       "      <td>732430.0</td>\n",
       "      <td>GNPEPTFSWTK</td>\n",
       "      <td>102060.00</td>\n",
       "      <td>0.139344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00533</td>\n",
       "      <td>732430.0</td>\n",
       "      <td>IEIPSSVQQVPTIIK</td>\n",
       "      <td>174185.00</td>\n",
       "      <td>0.237818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00533</td>\n",
       "      <td>732430.0</td>\n",
       "      <td>KPQSAVYSTGSNGILLC(UniMod_4)EAEGEPQPTIK</td>\n",
       "      <td>27278.90</td>\n",
       "      <td>0.037244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55_0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>O00533</td>\n",
       "      <td>732430.0</td>\n",
       "      <td>SMEQNGPGLEYR</td>\n",
       "      <td>30838.70</td>\n",
       "      <td>0.042105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981829</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>108</td>\n",
       "      <td>58648</td>\n",
       "      <td>Q9UHG2</td>\n",
       "      <td>369437.0</td>\n",
       "      <td>ILAGSADSEGVAAPR</td>\n",
       "      <td>202820.00</td>\n",
       "      <td>0.548998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981830</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>108</td>\n",
       "      <td>58648</td>\n",
       "      <td>Q9UKV8</td>\n",
       "      <td>105830.0</td>\n",
       "      <td>SGNIPAGTTVDTK</td>\n",
       "      <td>105830.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981831</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>108</td>\n",
       "      <td>58648</td>\n",
       "      <td>Q9Y646</td>\n",
       "      <td>21257.6</td>\n",
       "      <td>LALLVDTVGPR</td>\n",
       "      <td>21257.60</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981832</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>108</td>\n",
       "      <td>58648</td>\n",
       "      <td>Q9Y6R7</td>\n",
       "      <td>17953.1</td>\n",
       "      <td>AGC(UniMod_4)VAESTAVC(UniMod_4)R</td>\n",
       "      <td>5127.26</td>\n",
       "      <td>0.285592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981833</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>108</td>\n",
       "      <td>58648</td>\n",
       "      <td>Q9Y6R7</td>\n",
       "      <td>17953.1</td>\n",
       "      <td>GATTSPGVYELSSR</td>\n",
       "      <td>12825.90</td>\n",
       "      <td>0.714411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>981834 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         visit_id  visit_month  patient_id UniProt       NPX  \\\n",
       "0            55_0            0          55  O00391   11254.3   \n",
       "1            55_0            0          55  O00533  732430.0   \n",
       "2            55_0            0          55  O00533  732430.0   \n",
       "3            55_0            0          55  O00533  732430.0   \n",
       "4            55_0            0          55  O00533  732430.0   \n",
       "...           ...          ...         ...     ...       ...   \n",
       "981829  58648_108          108       58648  Q9UHG2  369437.0   \n",
       "981830  58648_108          108       58648  Q9UKV8  105830.0   \n",
       "981831  58648_108          108       58648  Q9Y646   21257.6   \n",
       "981832  58648_108          108       58648  Q9Y6R7   17953.1   \n",
       "981833  58648_108          108       58648  Q9Y6R7   17953.1   \n",
       "\n",
       "                                       Peptide  PeptideAbundance  \\\n",
       "0                                NEQEQPLGQWHLS          11254.30   \n",
       "1                                  GNPEPTFSWTK         102060.00   \n",
       "2                              IEIPSSVQQVPTIIK         174185.00   \n",
       "3       KPQSAVYSTGSNGILLC(UniMod_4)EAEGEPQPTIK          27278.90   \n",
       "4                                 SMEQNGPGLEYR          30838.70   \n",
       "...                                        ...               ...   \n",
       "981829                         ILAGSADSEGVAAPR         202820.00   \n",
       "981830                           SGNIPAGTTVDTK         105830.00   \n",
       "981831                             LALLVDTVGPR          21257.60   \n",
       "981832        AGC(UniMod_4)VAESTAVC(UniMod_4)R           5127.26   \n",
       "981833                          GATTSPGVYELSSR          12825.90   \n",
       "\n",
       "        Percentage_of_pep_in_Uniprot  \n",
       "0                           1.000000  \n",
       "1                           0.139344  \n",
       "2                           0.237818  \n",
       "3                           0.037244  \n",
       "4                           0.042105  \n",
       "...                              ...  \n",
       "981829                      0.548998  \n",
       "981830                      1.000000  \n",
       "981831                      1.000000  \n",
       "981832                      0.285592  \n",
       "981833                      0.714411  \n",
       "\n",
       "[981834 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I calculate Percentage_of_pep_in_Uniprot \n",
    "# The Peptide = sequence of amino acids \n",
    "# When the sequence of amino acids is more than 10 the name become protein and protein contains more than on peptide...\n",
    "pep_pro[\"Percentage_of_pep_in_Uniprot\"] = pep_pro[\"PeptideAbundance\"] / pep_pro[\"NPX\"]\n",
    "pep_pro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example : ![image](Midterm_image.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_id</th>\n",
       "      <th>UniProt</th>\n",
       "      <th>NPX</th>\n",
       "      <th>Peptide</th>\n",
       "      <th>PeptideAbundance</th>\n",
       "      <th>Percentage_of_pep_in_Uniprot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55_0</td>\n",
       "      <td>O00391</td>\n",
       "      <td>11254.3</td>\n",
       "      <td>NEQEQPLGQWHLS</td>\n",
       "      <td>11254.30</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55_0</td>\n",
       "      <td>O00533</td>\n",
       "      <td>732430.0</td>\n",
       "      <td>GNPEPTFSWTK</td>\n",
       "      <td>102060.00</td>\n",
       "      <td>0.139344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55_0</td>\n",
       "      <td>O00533</td>\n",
       "      <td>732430.0</td>\n",
       "      <td>IEIPSSVQQVPTIIK</td>\n",
       "      <td>174185.00</td>\n",
       "      <td>0.237818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55_0</td>\n",
       "      <td>O00533</td>\n",
       "      <td>732430.0</td>\n",
       "      <td>KPQSAVYSTGSNGILLC(UniMod_4)EAEGEPQPTIK</td>\n",
       "      <td>27278.90</td>\n",
       "      <td>0.037244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55_0</td>\n",
       "      <td>O00533</td>\n",
       "      <td>732430.0</td>\n",
       "      <td>SMEQNGPGLEYR</td>\n",
       "      <td>30838.70</td>\n",
       "      <td>0.042105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981829</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>Q9UHG2</td>\n",
       "      <td>369437.0</td>\n",
       "      <td>ILAGSADSEGVAAPR</td>\n",
       "      <td>202820.00</td>\n",
       "      <td>0.548998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981830</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>Q9UKV8</td>\n",
       "      <td>105830.0</td>\n",
       "      <td>SGNIPAGTTVDTK</td>\n",
       "      <td>105830.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981831</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>Q9Y646</td>\n",
       "      <td>21257.6</td>\n",
       "      <td>LALLVDTVGPR</td>\n",
       "      <td>21257.60</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981832</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>Q9Y6R7</td>\n",
       "      <td>17953.1</td>\n",
       "      <td>AGC(UniMod_4)VAESTAVC(UniMod_4)R</td>\n",
       "      <td>5127.26</td>\n",
       "      <td>0.285592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981833</th>\n",
       "      <td>58648_108</td>\n",
       "      <td>Q9Y6R7</td>\n",
       "      <td>17953.1</td>\n",
       "      <td>GATTSPGVYELSSR</td>\n",
       "      <td>12825.90</td>\n",
       "      <td>0.714411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>981834 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         visit_id UniProt       NPX                                 Peptide  \\\n",
       "0            55_0  O00391   11254.3                           NEQEQPLGQWHLS   \n",
       "1            55_0  O00533  732430.0                             GNPEPTFSWTK   \n",
       "2            55_0  O00533  732430.0                         IEIPSSVQQVPTIIK   \n",
       "3            55_0  O00533  732430.0  KPQSAVYSTGSNGILLC(UniMod_4)EAEGEPQPTIK   \n",
       "4            55_0  O00533  732430.0                            SMEQNGPGLEYR   \n",
       "...           ...     ...       ...                                     ...   \n",
       "981829  58648_108  Q9UHG2  369437.0                         ILAGSADSEGVAAPR   \n",
       "981830  58648_108  Q9UKV8  105830.0                           SGNIPAGTTVDTK   \n",
       "981831  58648_108  Q9Y646   21257.6                             LALLVDTVGPR   \n",
       "981832  58648_108  Q9Y6R7   17953.1        AGC(UniMod_4)VAESTAVC(UniMod_4)R   \n",
       "981833  58648_108  Q9Y6R7   17953.1                          GATTSPGVYELSSR   \n",
       "\n",
       "        PeptideAbundance  Percentage_of_pep_in_Uniprot  \n",
       "0               11254.30                      1.000000  \n",
       "1              102060.00                      0.139344  \n",
       "2              174185.00                      0.237818  \n",
       "3               27278.90                      0.037244  \n",
       "4               30838.70                      0.042105  \n",
       "...                  ...                           ...  \n",
       "981829         202820.00                      0.548998  \n",
       "981830         105830.00                      1.000000  \n",
       "981831          21257.60                      1.000000  \n",
       "981832           5127.26                      0.285592  \n",
       "981833          12825.90                      0.714411  \n",
       "\n",
       "[981834 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dropping the patient_id and visith_month because I will re-integer them later.\n",
    "\n",
    "pep_pro = pep_pro.drop([\"patient_id\",\"visit_month\"], axis=1)\n",
    "pep_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"21\" halign=\"left\">Percentage_of_pep_in_Uniprot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peptide</th>\n",
       "      <th>AADDTWEPFASGK</th>\n",
       "      <th>AAFGQGSGPIMLDEVQC(UniMod_4)TGTEASLADC(UniMod_4)K</th>\n",
       "      <th>AAFTEC(UniMod_4)C(UniMod_4)QAADK</th>\n",
       "      <th>AANEVSSADVK</th>\n",
       "      <th>AATGEC(UniMod_4)TATVGKR</th>\n",
       "      <th>AATVGSLAGQPLQER</th>\n",
       "      <th>AAVYHHFISDGVR</th>\n",
       "      <th>ADDKETC(UniMod_4)FAEEGK</th>\n",
       "      <th>ADDKETC(UniMod_4)FAEEGKK</th>\n",
       "      <th>ADDLGKGGNEESTKTGNAGSR</th>\n",
       "      <th>...</th>\n",
       "      <th>YSLTYIYTGLSK</th>\n",
       "      <th>YTTEIIK</th>\n",
       "      <th>YVGGQEHFAHLLILR</th>\n",
       "      <th>YVM(UniMod_35)LPVADQDQC(UniMod_4)IR</th>\n",
       "      <th>YVMLPVADQDQC(UniMod_4)IR</th>\n",
       "      <th>YVNKEIQNAVNGVK</th>\n",
       "      <th>YWGVASFLQK</th>\n",
       "      <th>YYC(UniMod_4)FQGNQFLR</th>\n",
       "      <th>YYTYLIMNK</th>\n",
       "      <th>YYWGGQYTWDMAK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10053_0</th>\n",
       "      <td>0.396378</td>\n",
       "      <td>0.208956</td>\n",
       "      <td>0.028741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017462</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.908568</td>\n",
       "      <td>0.021554</td>\n",
       "      <td>0.162199</td>\n",
       "      <td>0.016149</td>\n",
       "      <td>0.466770</td>\n",
       "      <td>0.137420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10053_12</th>\n",
       "      <td>0.411091</td>\n",
       "      <td>0.342469</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017125</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.844482</td>\n",
       "      <td>0.018441</td>\n",
       "      <td>0.178483</td>\n",
       "      <td>0.021803</td>\n",
       "      <td>0.468994</td>\n",
       "      <td>0.122406</td>\n",
       "      <td>0.014664</td>\n",
       "      <td>0.072531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10053_18</th>\n",
       "      <td>0.334582</td>\n",
       "      <td>0.284857</td>\n",
       "      <td>0.025147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.017698</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>0.018175</td>\n",
       "      <td>0.580124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.845402</td>\n",
       "      <td>0.014540</td>\n",
       "      <td>0.184856</td>\n",
       "      <td>0.011034</td>\n",
       "      <td>0.272304</td>\n",
       "      <td>0.093746</td>\n",
       "      <td>0.014050</td>\n",
       "      <td>0.037555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10138_12</th>\n",
       "      <td>0.392485</td>\n",
       "      <td>0.200315</td>\n",
       "      <td>0.029025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012851</td>\n",
       "      <td>0.005416</td>\n",
       "      <td>0.012920</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>0.619942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220530</td>\n",
       "      <td>0.131455</td>\n",
       "      <td>0.758140</td>\n",
       "      <td>0.022214</td>\n",
       "      <td>0.151359</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.266508</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>0.015353</td>\n",
       "      <td>0.009632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10138_24</th>\n",
       "      <td>0.439461</td>\n",
       "      <td>0.231316</td>\n",
       "      <td>0.017401</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008454</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.008056</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.005335</td>\n",
       "      <td>0.550899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379578</td>\n",
       "      <td>0.094519</td>\n",
       "      <td>0.764656</td>\n",
       "      <td>0.023695</td>\n",
       "      <td>0.168184</td>\n",
       "      <td>0.009296</td>\n",
       "      <td>0.300633</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012467</td>\n",
       "      <td>0.005271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8699_24</th>\n",
       "      <td>0.339803</td>\n",
       "      <td>0.302381</td>\n",
       "      <td>0.035990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>0.011728</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.013485</td>\n",
       "      <td>0.601045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264711</td>\n",
       "      <td>0.083853</td>\n",
       "      <td>0.827905</td>\n",
       "      <td>0.008452</td>\n",
       "      <td>0.232965</td>\n",
       "      <td>0.017196</td>\n",
       "      <td>0.264829</td>\n",
       "      <td>0.104559</td>\n",
       "      <td>0.013896</td>\n",
       "      <td>0.058265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942_12</th>\n",
       "      <td>0.460767</td>\n",
       "      <td>0.263313</td>\n",
       "      <td>0.034176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017876</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>0.013435</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.675229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263012</td>\n",
       "      <td>0.092184</td>\n",
       "      <td>0.298893</td>\n",
       "      <td>0.023754</td>\n",
       "      <td>0.166296</td>\n",
       "      <td>0.010369</td>\n",
       "      <td>0.265062</td>\n",
       "      <td>0.116715</td>\n",
       "      <td>0.015017</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942_24</th>\n",
       "      <td>0.420181</td>\n",
       "      <td>0.178772</td>\n",
       "      <td>0.036785</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018348</td>\n",
       "      <td>0.006790</td>\n",
       "      <td>0.011577</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.013911</td>\n",
       "      <td>0.589112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296885</td>\n",
       "      <td>0.103732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024662</td>\n",
       "      <td>0.184619</td>\n",
       "      <td>0.012636</td>\n",
       "      <td>0.262533</td>\n",
       "      <td>0.127583</td>\n",
       "      <td>0.013546</td>\n",
       "      <td>0.044961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942_48</th>\n",
       "      <td>0.392781</td>\n",
       "      <td>0.153731</td>\n",
       "      <td>0.033186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019466</td>\n",
       "      <td>0.006967</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.015478</td>\n",
       "      <td>0.635481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191422</td>\n",
       "      <td>0.126835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020220</td>\n",
       "      <td>0.167023</td>\n",
       "      <td>0.010150</td>\n",
       "      <td>0.225040</td>\n",
       "      <td>0.106813</td>\n",
       "      <td>0.014133</td>\n",
       "      <td>0.040475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942_6</th>\n",
       "      <td>0.343522</td>\n",
       "      <td>0.224630</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013481</td>\n",
       "      <td>0.007007</td>\n",
       "      <td>0.010395</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.017104</td>\n",
       "      <td>0.581692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283831</td>\n",
       "      <td>0.343916</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018945</td>\n",
       "      <td>0.158267</td>\n",
       "      <td>0.011669</td>\n",
       "      <td>0.233967</td>\n",
       "      <td>0.099614</td>\n",
       "      <td>0.012169</td>\n",
       "      <td>0.031341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1113 rows × 968 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Percentage_of_pep_in_Uniprot  \\\n",
       "Peptide                 AADDTWEPFASGK   \n",
       "visit_id                                \n",
       "10053_0                      0.396378   \n",
       "10053_12                     0.411091   \n",
       "10053_18                     0.334582   \n",
       "10138_12                     0.392485   \n",
       "10138_24                     0.439461   \n",
       "...                               ...   \n",
       "8699_24                      0.339803   \n",
       "942_12                       0.460767   \n",
       "942_24                       0.420181   \n",
       "942_48                       0.392781   \n",
       "942_6                        0.343522   \n",
       "\n",
       "                                                           \\\n",
       "Peptide  AAFGQGSGPIMLDEVQC(UniMod_4)TGTEASLADC(UniMod_4)K   \n",
       "visit_id                                                    \n",
       "10053_0                                          0.208956   \n",
       "10053_12                                         0.342469   \n",
       "10053_18                                         0.284857   \n",
       "10138_12                                         0.200315   \n",
       "10138_24                                         0.231316   \n",
       "...                                                   ...   \n",
       "8699_24                                          0.302381   \n",
       "942_12                                           0.263313   \n",
       "942_24                                           0.178772   \n",
       "942_48                                           0.153731   \n",
       "942_6                                            0.224630   \n",
       "\n",
       "                                                                               \\\n",
       "Peptide  AAFTEC(UniMod_4)C(UniMod_4)QAADK AANEVSSADVK AATGEC(UniMod_4)TATVGKR   \n",
       "visit_id                                                                        \n",
       "10053_0                          0.028741         NaN                     NaN   \n",
       "10053_12                         0.019977         NaN                     NaN   \n",
       "10053_18                         0.025147         1.0                0.017698   \n",
       "10138_12                         0.029025         NaN                0.012851   \n",
       "10138_24                         0.017401         1.0                0.008454   \n",
       "...                                   ...         ...                     ...   \n",
       "8699_24                          0.035990         1.0                0.000818   \n",
       "942_12                           0.034176         NaN                0.017876   \n",
       "942_24                           0.036785         1.0                0.018348   \n",
       "942_48                           0.033186         NaN                0.019466   \n",
       "942_6                            0.012398         1.0                0.013481   \n",
       "\n",
       "                                                                \\\n",
       "Peptide  AATVGSLAGQPLQER AAVYHHFISDGVR ADDKETC(UniMod_4)FAEEGK   \n",
       "visit_id                                                         \n",
       "10053_0              NaN      0.017462                0.000877   \n",
       "10053_12             NaN      0.017125                0.000401   \n",
       "10053_18        0.002361      0.009662                0.002542   \n",
       "10138_12        0.005416      0.012920                0.000218   \n",
       "10138_24        0.006324      0.008056                0.000086   \n",
       "...                  ...           ...                     ...   \n",
       "8699_24         0.004565      0.011728                0.000940   \n",
       "942_12          0.006771      0.013435                0.001710   \n",
       "942_24          0.006790      0.011577                0.000251   \n",
       "942_48          0.006967      0.012906                0.000227   \n",
       "942_6           0.007007      0.010395                0.000220   \n",
       "\n",
       "                                                         ...               \\\n",
       "Peptide  ADDKETC(UniMod_4)FAEEGKK ADDLGKGGNEESTKTGNAGSR  ... YSLTYIYTGLSK   \n",
       "visit_id                                                 ...                \n",
       "10053_0                       NaN                   NaN  ...     0.277204   \n",
       "10053_12                 0.000170                   NaN  ...     0.209889   \n",
       "10053_18                 0.018175              0.580124  ...     0.266283   \n",
       "10138_12                 0.016963              0.619942  ...     0.220530   \n",
       "10138_24                 0.005335              0.550899  ...     0.379578   \n",
       "...                           ...                   ...  ...          ...   \n",
       "8699_24                  0.013485              0.601045  ...     0.264711   \n",
       "942_12                        NaN              0.675229  ...     0.263012   \n",
       "942_24                   0.013911              0.589112  ...     0.296885   \n",
       "942_48                   0.015478              0.635481  ...     0.191422   \n",
       "942_6                    0.017104              0.581692  ...     0.283831   \n",
       "\n",
       "                                                                        \\\n",
       "Peptide    YTTEIIK YVGGQEHFAHLLILR YVM(UniMod_35)LPVADQDQC(UniMod_4)IR   \n",
       "visit_id                                                                 \n",
       "10053_0        NaN        0.908568                            0.021554   \n",
       "10053_12       NaN        0.844482                            0.018441   \n",
       "10053_18       NaN        0.845402                            0.014540   \n",
       "10138_12  0.131455        0.758140                            0.022214   \n",
       "10138_24  0.094519        0.764656                            0.023695   \n",
       "...            ...             ...                                 ...   \n",
       "8699_24   0.083853        0.827905                            0.008452   \n",
       "942_12    0.092184        0.298893                            0.023754   \n",
       "942_24    0.103732             NaN                            0.024662   \n",
       "942_48    0.126835             NaN                            0.020220   \n",
       "942_6     0.343916             NaN                            0.018945   \n",
       "\n",
       "                                                             \\\n",
       "Peptide  YVMLPVADQDQC(UniMod_4)IR YVNKEIQNAVNGVK YWGVASFLQK   \n",
       "visit_id                                                      \n",
       "10053_0                  0.162199       0.016149   0.466770   \n",
       "10053_12                 0.178483       0.021803   0.468994   \n",
       "10053_18                 0.184856       0.011034   0.272304   \n",
       "10138_12                 0.151359       0.011931   0.266508   \n",
       "10138_24                 0.168184       0.009296   0.300633   \n",
       "...                           ...            ...        ...   \n",
       "8699_24                  0.232965       0.017196   0.264829   \n",
       "942_12                   0.166296       0.010369   0.265062   \n",
       "942_24                   0.184619       0.012636   0.262533   \n",
       "942_48                   0.167023       0.010150   0.225040   \n",
       "942_6                    0.158267       0.011669   0.233967   \n",
       "\n",
       "                                                        \n",
       "Peptide  YYC(UniMod_4)FQGNQFLR YYTYLIMNK YYWGGQYTWDMAK  \n",
       "visit_id                                                \n",
       "10053_0               0.137420       NaN      0.021444  \n",
       "10053_12              0.122406  0.014664      0.072531  \n",
       "10053_18              0.093746  0.014050      0.037555  \n",
       "10138_12              0.083300  0.015353      0.009632  \n",
       "10138_24                   NaN  0.012467      0.005271  \n",
       "...                        ...       ...           ...  \n",
       "8699_24               0.104559  0.013896      0.058265  \n",
       "942_12                0.116715  0.015017           NaN  \n",
       "942_24                0.127583  0.013546      0.044961  \n",
       "942_48                0.106813  0.014133      0.040475  \n",
       "942_6                 0.099614  0.012169      0.031341  \n",
       "\n",
       "[1113 rows x 968 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pep_pro = pep_pro.pivot(index=[\"visit_id\"], columns=[\"Peptide\"], values=([\"Percentage_of_pep_in_Uniprot\"]))\n",
    "pep_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Peptide</th>\n",
       "      <th>visit_id</th>\n",
       "      <th>AADDTWEPFASGK</th>\n",
       "      <th>AAFGQGSGPIMLDEVQC(UniMod_4)TGTEASLADC(UniMod_4)K</th>\n",
       "      <th>AAFTEC(UniMod_4)C(UniMod_4)QAADK</th>\n",
       "      <th>AANEVSSADVK</th>\n",
       "      <th>AATGEC(UniMod_4)TATVGKR</th>\n",
       "      <th>AATVGSLAGQPLQER</th>\n",
       "      <th>AAVYHHFISDGVR</th>\n",
       "      <th>ADDKETC(UniMod_4)FAEEGK</th>\n",
       "      <th>ADDKETC(UniMod_4)FAEEGKK</th>\n",
       "      <th>...</th>\n",
       "      <th>YSLTYIYTGLSK</th>\n",
       "      <th>YTTEIIK</th>\n",
       "      <th>YVGGQEHFAHLLILR</th>\n",
       "      <th>YVM(UniMod_35)LPVADQDQC(UniMod_4)IR</th>\n",
       "      <th>YVMLPVADQDQC(UniMod_4)IR</th>\n",
       "      <th>YVNKEIQNAVNGVK</th>\n",
       "      <th>YWGVASFLQK</th>\n",
       "      <th>YYC(UniMod_4)FQGNQFLR</th>\n",
       "      <th>YYTYLIMNK</th>\n",
       "      <th>YYWGGQYTWDMAK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10053_0</td>\n",
       "      <td>0.396378</td>\n",
       "      <td>0.208956</td>\n",
       "      <td>0.028741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017462</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.908568</td>\n",
       "      <td>0.021554</td>\n",
       "      <td>0.162199</td>\n",
       "      <td>0.016149</td>\n",
       "      <td>0.466770</td>\n",
       "      <td>0.137420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10053_12</td>\n",
       "      <td>0.411091</td>\n",
       "      <td>0.342469</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017125</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.844482</td>\n",
       "      <td>0.018441</td>\n",
       "      <td>0.178483</td>\n",
       "      <td>0.021803</td>\n",
       "      <td>0.468994</td>\n",
       "      <td>0.122406</td>\n",
       "      <td>0.014664</td>\n",
       "      <td>0.072531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10053_18</td>\n",
       "      <td>0.334582</td>\n",
       "      <td>0.284857</td>\n",
       "      <td>0.025147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.017698</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>0.018175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266283</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.845402</td>\n",
       "      <td>0.014540</td>\n",
       "      <td>0.184856</td>\n",
       "      <td>0.011034</td>\n",
       "      <td>0.272304</td>\n",
       "      <td>0.093746</td>\n",
       "      <td>0.014050</td>\n",
       "      <td>0.037555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10138_12</td>\n",
       "      <td>0.392485</td>\n",
       "      <td>0.200315</td>\n",
       "      <td>0.029025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012851</td>\n",
       "      <td>0.005416</td>\n",
       "      <td>0.012920</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.016963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220530</td>\n",
       "      <td>0.131455</td>\n",
       "      <td>0.758140</td>\n",
       "      <td>0.022214</td>\n",
       "      <td>0.151359</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.266508</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>0.015353</td>\n",
       "      <td>0.009632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10138_24</td>\n",
       "      <td>0.439461</td>\n",
       "      <td>0.231316</td>\n",
       "      <td>0.017401</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008454</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.008056</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.005335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379578</td>\n",
       "      <td>0.094519</td>\n",
       "      <td>0.764656</td>\n",
       "      <td>0.023695</td>\n",
       "      <td>0.168184</td>\n",
       "      <td>0.009296</td>\n",
       "      <td>0.300633</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012467</td>\n",
       "      <td>0.005271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>8699_24</td>\n",
       "      <td>0.339803</td>\n",
       "      <td>0.302381</td>\n",
       "      <td>0.035990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>0.011728</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.013485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264711</td>\n",
       "      <td>0.083853</td>\n",
       "      <td>0.827905</td>\n",
       "      <td>0.008452</td>\n",
       "      <td>0.232965</td>\n",
       "      <td>0.017196</td>\n",
       "      <td>0.264829</td>\n",
       "      <td>0.104559</td>\n",
       "      <td>0.013896</td>\n",
       "      <td>0.058265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>942_12</td>\n",
       "      <td>0.460767</td>\n",
       "      <td>0.263313</td>\n",
       "      <td>0.034176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017876</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>0.013435</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263012</td>\n",
       "      <td>0.092184</td>\n",
       "      <td>0.298893</td>\n",
       "      <td>0.023754</td>\n",
       "      <td>0.166296</td>\n",
       "      <td>0.010369</td>\n",
       "      <td>0.265062</td>\n",
       "      <td>0.116715</td>\n",
       "      <td>0.015017</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>942_24</td>\n",
       "      <td>0.420181</td>\n",
       "      <td>0.178772</td>\n",
       "      <td>0.036785</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018348</td>\n",
       "      <td>0.006790</td>\n",
       "      <td>0.011577</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.013911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296885</td>\n",
       "      <td>0.103732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024662</td>\n",
       "      <td>0.184619</td>\n",
       "      <td>0.012636</td>\n",
       "      <td>0.262533</td>\n",
       "      <td>0.127583</td>\n",
       "      <td>0.013546</td>\n",
       "      <td>0.044961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>942_48</td>\n",
       "      <td>0.392781</td>\n",
       "      <td>0.153731</td>\n",
       "      <td>0.033186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019466</td>\n",
       "      <td>0.006967</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.015478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191422</td>\n",
       "      <td>0.126835</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020220</td>\n",
       "      <td>0.167023</td>\n",
       "      <td>0.010150</td>\n",
       "      <td>0.225040</td>\n",
       "      <td>0.106813</td>\n",
       "      <td>0.014133</td>\n",
       "      <td>0.040475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>942_6</td>\n",
       "      <td>0.343522</td>\n",
       "      <td>0.224630</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013481</td>\n",
       "      <td>0.007007</td>\n",
       "      <td>0.010395</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.017104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283831</td>\n",
       "      <td>0.343916</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018945</td>\n",
       "      <td>0.158267</td>\n",
       "      <td>0.011669</td>\n",
       "      <td>0.233967</td>\n",
       "      <td>0.099614</td>\n",
       "      <td>0.012169</td>\n",
       "      <td>0.031341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1113 rows × 969 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Peptide  visit_id  AADDTWEPFASGK  \\\n",
       "0         10053_0       0.396378   \n",
       "1        10053_12       0.411091   \n",
       "2        10053_18       0.334582   \n",
       "3        10138_12       0.392485   \n",
       "4        10138_24       0.439461   \n",
       "...           ...            ...   \n",
       "1108      8699_24       0.339803   \n",
       "1109       942_12       0.460767   \n",
       "1110       942_24       0.420181   \n",
       "1111       942_48       0.392781   \n",
       "1112        942_6       0.343522   \n",
       "\n",
       "Peptide  AAFGQGSGPIMLDEVQC(UniMod_4)TGTEASLADC(UniMod_4)K  \\\n",
       "0                                                0.208956   \n",
       "1                                                0.342469   \n",
       "2                                                0.284857   \n",
       "3                                                0.200315   \n",
       "4                                                0.231316   \n",
       "...                                                   ...   \n",
       "1108                                             0.302381   \n",
       "1109                                             0.263313   \n",
       "1110                                             0.178772   \n",
       "1111                                             0.153731   \n",
       "1112                                             0.224630   \n",
       "\n",
       "Peptide  AAFTEC(UniMod_4)C(UniMod_4)QAADK  AANEVSSADVK  \\\n",
       "0                                0.028741          NaN   \n",
       "1                                0.019977          NaN   \n",
       "2                                0.025147          1.0   \n",
       "3                                0.029025          NaN   \n",
       "4                                0.017401          1.0   \n",
       "...                                   ...          ...   \n",
       "1108                             0.035990          1.0   \n",
       "1109                             0.034176          NaN   \n",
       "1110                             0.036785          1.0   \n",
       "1111                             0.033186          NaN   \n",
       "1112                             0.012398          1.0   \n",
       "\n",
       "Peptide  AATGEC(UniMod_4)TATVGKR  AATVGSLAGQPLQER  AAVYHHFISDGVR  \\\n",
       "0                            NaN              NaN       0.017462   \n",
       "1                            NaN              NaN       0.017125   \n",
       "2                       0.017698         0.002361       0.009662   \n",
       "3                       0.012851         0.005416       0.012920   \n",
       "4                       0.008454         0.006324       0.008056   \n",
       "...                          ...              ...            ...   \n",
       "1108                    0.000818         0.004565       0.011728   \n",
       "1109                    0.017876         0.006771       0.013435   \n",
       "1110                    0.018348         0.006790       0.011577   \n",
       "1111                    0.019466         0.006967       0.012906   \n",
       "1112                    0.013481         0.007007       0.010395   \n",
       "\n",
       "Peptide  ADDKETC(UniMod_4)FAEEGK  ADDKETC(UniMod_4)FAEEGKK  ...  YSLTYIYTGLSK  \\\n",
       "0                       0.000877                       NaN  ...      0.277204   \n",
       "1                       0.000401                  0.000170  ...      0.209889   \n",
       "2                       0.002542                  0.018175  ...      0.266283   \n",
       "3                       0.000218                  0.016963  ...      0.220530   \n",
       "4                       0.000086                  0.005335  ...      0.379578   \n",
       "...                          ...                       ...  ...           ...   \n",
       "1108                    0.000940                  0.013485  ...      0.264711   \n",
       "1109                    0.001710                       NaN  ...      0.263012   \n",
       "1110                    0.000251                  0.013911  ...      0.296885   \n",
       "1111                    0.000227                  0.015478  ...      0.191422   \n",
       "1112                    0.000220                  0.017104  ...      0.283831   \n",
       "\n",
       "Peptide   YTTEIIK  YVGGQEHFAHLLILR  YVM(UniMod_35)LPVADQDQC(UniMod_4)IR  \\\n",
       "0             NaN         0.908568                             0.021554   \n",
       "1             NaN         0.844482                             0.018441   \n",
       "2             NaN         0.845402                             0.014540   \n",
       "3        0.131455         0.758140                             0.022214   \n",
       "4        0.094519         0.764656                             0.023695   \n",
       "...           ...              ...                                  ...   \n",
       "1108     0.083853         0.827905                             0.008452   \n",
       "1109     0.092184         0.298893                             0.023754   \n",
       "1110     0.103732              NaN                             0.024662   \n",
       "1111     0.126835              NaN                             0.020220   \n",
       "1112     0.343916              NaN                             0.018945   \n",
       "\n",
       "Peptide  YVMLPVADQDQC(UniMod_4)IR  YVNKEIQNAVNGVK  YWGVASFLQK  \\\n",
       "0                        0.162199        0.016149    0.466770   \n",
       "1                        0.178483        0.021803    0.468994   \n",
       "2                        0.184856        0.011034    0.272304   \n",
       "3                        0.151359        0.011931    0.266508   \n",
       "4                        0.168184        0.009296    0.300633   \n",
       "...                           ...             ...         ...   \n",
       "1108                     0.232965        0.017196    0.264829   \n",
       "1109                     0.166296        0.010369    0.265062   \n",
       "1110                     0.184619        0.012636    0.262533   \n",
       "1111                     0.167023        0.010150    0.225040   \n",
       "1112                     0.158267        0.011669    0.233967   \n",
       "\n",
       "Peptide  YYC(UniMod_4)FQGNQFLR  YYTYLIMNK  YYWGGQYTWDMAK  \n",
       "0                     0.137420        NaN       0.021444  \n",
       "1                     0.122406   0.014664       0.072531  \n",
       "2                     0.093746   0.014050       0.037555  \n",
       "3                     0.083300   0.015353       0.009632  \n",
       "4                          NaN   0.012467       0.005271  \n",
       "...                        ...        ...            ...  \n",
       "1108                  0.104559   0.013896       0.058265  \n",
       "1109                  0.116715   0.015017            NaN  \n",
       "1110                  0.127583   0.013546       0.044961  \n",
       "1111                  0.106813   0.014133       0.040475  \n",
       "1112                  0.099614   0.012169       0.031341  \n",
       "\n",
       "[1113 rows x 969 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pep_pro.columns = pep_pro.columns.droplevel()\n",
    "pep_pro = pep_pro.reset_index()\n",
    "pep_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_month</th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "      <th>updrs_4</th>\n",
       "      <th>AADDTWEPFASGK</th>\n",
       "      <th>AAFGQGSGPIMLDEVQC(UniMod_4)TGTEASLADC(UniMod_4)K</th>\n",
       "      <th>AAFTEC(UniMod_4)C(UniMod_4)QAADK</th>\n",
       "      <th>AANEVSSADVK</th>\n",
       "      <th>AATGEC(UniMod_4)TATVGKR</th>\n",
       "      <th>...</th>\n",
       "      <th>YSLTYIYTGLSK</th>\n",
       "      <th>YTTEIIK</th>\n",
       "      <th>YVGGQEHFAHLLILR</th>\n",
       "      <th>YVM(UniMod_35)LPVADQDQC(UniMod_4)IR</th>\n",
       "      <th>YVMLPVADQDQC(UniMod_4)IR</th>\n",
       "      <th>YVNKEIQNAVNGVK</th>\n",
       "      <th>YWGVASFLQK</th>\n",
       "      <th>YYC(UniMod_4)FQGNQFLR</th>\n",
       "      <th>YYTYLIMNK</th>\n",
       "      <th>YYWGGQYTWDMAK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55_0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378517</td>\n",
       "      <td>0.200462</td>\n",
       "      <td>0.021986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218140</td>\n",
       "      <td>0.150558</td>\n",
       "      <td>0.682349</td>\n",
       "      <td>0.027018</td>\n",
       "      <td>0.146764</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>0.290456</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.011818</td>\n",
       "      <td>0.026223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_6</th>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.367787</td>\n",
       "      <td>0.179201</td>\n",
       "      <td>0.024904</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179524</td>\n",
       "      <td>0.124791</td>\n",
       "      <td>0.650899</td>\n",
       "      <td>0.028746</td>\n",
       "      <td>0.130530</td>\n",
       "      <td>0.010287</td>\n",
       "      <td>0.294590</td>\n",
       "      <td>0.081368</td>\n",
       "      <td>0.012196</td>\n",
       "      <td>0.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_9</th>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_12</th>\n",
       "      <td>12</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.361881</td>\n",
       "      <td>0.177326</td>\n",
       "      <td>0.023456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260621</td>\n",
       "      <td>0.158303</td>\n",
       "      <td>0.747632</td>\n",
       "      <td>0.028311</td>\n",
       "      <td>0.173299</td>\n",
       "      <td>0.012735</td>\n",
       "      <td>0.312116</td>\n",
       "      <td>0.075963</td>\n",
       "      <td>0.013082</td>\n",
       "      <td>0.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65043_48</th>\n",
       "      <td>48</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.374608</td>\n",
       "      <td>0.259382</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227781</td>\n",
       "      <td>0.115953</td>\n",
       "      <td>0.684073</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>0.142682</td>\n",
       "      <td>0.012314</td>\n",
       "      <td>0.275821</td>\n",
       "      <td>0.075535</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>0.033099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65043_54</th>\n",
       "      <td>54</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65043_60</th>\n",
       "      <td>60</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65043_72</th>\n",
       "      <td>72</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65043_84</th>\n",
       "      <td>84</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2615 rows × 973 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          visit_month  updrs_1  updrs_2  updrs_3  updrs_4  AADDTWEPFASGK  \\\n",
       "visit_id                                                                   \n",
       "55_0                0     10.0      6.0     15.0      NaN       0.378517   \n",
       "55_3                3     10.0      7.0     25.0      NaN            NaN   \n",
       "55_6                6      8.0     10.0     34.0      NaN       0.367787   \n",
       "55_9                9      8.0      9.0     30.0      0.0            NaN   \n",
       "55_12              12     10.0     10.0     41.0      0.0       0.361881   \n",
       "...               ...      ...      ...      ...      ...            ...   \n",
       "65043_48           48      7.0      6.0     13.0      0.0       0.374608   \n",
       "65043_54           54      4.0      8.0     11.0      1.0            NaN   \n",
       "65043_60           60      6.0      6.0     16.0      1.0            NaN   \n",
       "65043_72           72      3.0      9.0     14.0      1.0            NaN   \n",
       "65043_84           84      7.0      9.0     20.0      3.0            NaN   \n",
       "\n",
       "          AAFGQGSGPIMLDEVQC(UniMod_4)TGTEASLADC(UniMod_4)K  \\\n",
       "visit_id                                                     \n",
       "55_0                                              0.200462   \n",
       "55_3                                                   NaN   \n",
       "55_6                                              0.179201   \n",
       "55_9                                                   NaN   \n",
       "55_12                                             0.177326   \n",
       "...                                                    ...   \n",
       "65043_48                                          0.259382   \n",
       "65043_54                                               NaN   \n",
       "65043_60                                               NaN   \n",
       "65043_72                                               NaN   \n",
       "65043_84                                               NaN   \n",
       "\n",
       "          AAFTEC(UniMod_4)C(UniMod_4)QAADK  AANEVSSADVK  \\\n",
       "visit_id                                                  \n",
       "55_0                              0.021986          NaN   \n",
       "55_3                                   NaN          NaN   \n",
       "55_6                              0.024904          1.0   \n",
       "55_9                                   NaN          NaN   \n",
       "55_12                             0.023456          1.0   \n",
       "...                                    ...          ...   \n",
       "65043_48                          0.027886          NaN   \n",
       "65043_54                               NaN          NaN   \n",
       "65043_60                               NaN          NaN   \n",
       "65043_72                               NaN          NaN   \n",
       "65043_84                               NaN          NaN   \n",
       "\n",
       "          AATGEC(UniMod_4)TATVGKR  ...  YSLTYIYTGLSK   YTTEIIK  \\\n",
       "visit_id                           ...                           \n",
       "55_0                     0.016156  ...      0.218140  0.150558   \n",
       "55_3                          NaN  ...           NaN       NaN   \n",
       "55_6                     0.022779  ...      0.179524  0.124791   \n",
       "55_9                          NaN  ...           NaN       NaN   \n",
       "55_12                    0.013740  ...      0.260621  0.158303   \n",
       "...                           ...  ...           ...       ...   \n",
       "65043_48                 0.013325  ...      0.227781  0.115953   \n",
       "65043_54                      NaN  ...           NaN       NaN   \n",
       "65043_60                      NaN  ...           NaN       NaN   \n",
       "65043_72                      NaN  ...           NaN       NaN   \n",
       "65043_84                      NaN  ...           NaN       NaN   \n",
       "\n",
       "          YVGGQEHFAHLLILR  YVM(UniMod_35)LPVADQDQC(UniMod_4)IR  \\\n",
       "visit_id                                                         \n",
       "55_0             0.682349                             0.027018   \n",
       "55_3                  NaN                                  NaN   \n",
       "55_6             0.650899                             0.028746   \n",
       "55_9                  NaN                                  NaN   \n",
       "55_12            0.747632                             0.028311   \n",
       "...                   ...                                  ...   \n",
       "65043_48         0.684073                             0.013892   \n",
       "65043_54              NaN                                  NaN   \n",
       "65043_60              NaN                                  NaN   \n",
       "65043_72              NaN                                  NaN   \n",
       "65043_84              NaN                                  NaN   \n",
       "\n",
       "          YVMLPVADQDQC(UniMod_4)IR  YVNKEIQNAVNGVK  YWGVASFLQK  \\\n",
       "visit_id                                                         \n",
       "55_0                      0.146764        0.011330    0.290456   \n",
       "55_3                           NaN             NaN         NaN   \n",
       "55_6                      0.130530        0.010287    0.294590   \n",
       "55_9                           NaN             NaN         NaN   \n",
       "55_12                     0.173299        0.012735    0.312116   \n",
       "...                            ...             ...         ...   \n",
       "65043_48                  0.142682        0.012314    0.275821   \n",
       "65043_54                       NaN             NaN         NaN   \n",
       "65043_60                       NaN             NaN         NaN   \n",
       "65043_72                       NaN             NaN         NaN   \n",
       "65043_84                       NaN             NaN         NaN   \n",
       "\n",
       "          YYC(UniMod_4)FQGNQFLR  YYTYLIMNK  YYWGGQYTWDMAK  \n",
       "visit_id                                                   \n",
       "55_0                   0.091600   0.011818       0.026223  \n",
       "55_3                        NaN        NaN            NaN  \n",
       "55_6                   0.081368   0.012196       0.034900  \n",
       "55_9                        NaN        NaN            NaN  \n",
       "55_12                  0.075963   0.013082       0.037700  \n",
       "...                         ...        ...            ...  \n",
       "65043_48               0.075535   0.012647       0.033099  \n",
       "65043_54                    NaN        NaN            NaN  \n",
       "65043_60                    NaN        NaN            NaN  \n",
       "65043_72                    NaN        NaN            NaN  \n",
       "65043_84                    NaN        NaN            NaN  \n",
       "\n",
       "[2615 rows x 973 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the train_clinical_data called train here and the peptide and protein data.\n",
    "# We have back the visit_month which is a parameter to take inccount for the trainning features.\n",
    "df = pd.merge(train, pep_pro, on=\"visit_id\", how=\"left\")\n",
    "df = df.set_index('visit_id')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_month</th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "      <th>updrs_4</th>\n",
       "      <th>AADDTWEPFASGK</th>\n",
       "      <th>AAFGQGSGPIMLDEVQC(UniMod_4)TGTEASLADC(UniMod_4)K</th>\n",
       "      <th>AAFTEC(UniMod_4)C(UniMod_4)QAADK</th>\n",
       "      <th>AANEVSSADVK</th>\n",
       "      <th>AATGEC(UniMod_4)TATVGKR</th>\n",
       "      <th>...</th>\n",
       "      <th>YSLTYIYTGLSK</th>\n",
       "      <th>YTTEIIK</th>\n",
       "      <th>YVGGQEHFAHLLILR</th>\n",
       "      <th>YVM(UniMod_35)LPVADQDQC(UniMod_4)IR</th>\n",
       "      <th>YVMLPVADQDQC(UniMod_4)IR</th>\n",
       "      <th>YVNKEIQNAVNGVK</th>\n",
       "      <th>YWGVASFLQK</th>\n",
       "      <th>YYC(UniMod_4)FQGNQFLR</th>\n",
       "      <th>YYTYLIMNK</th>\n",
       "      <th>YYWGGQYTWDMAK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55_0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378517</td>\n",
       "      <td>0.200462</td>\n",
       "      <td>0.021986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218140</td>\n",
       "      <td>0.150558</td>\n",
       "      <td>0.682349</td>\n",
       "      <td>0.027018</td>\n",
       "      <td>0.146764</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>0.290456</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.011818</td>\n",
       "      <td>0.026223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_6</th>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.367787</td>\n",
       "      <td>0.179201</td>\n",
       "      <td>0.024904</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179524</td>\n",
       "      <td>0.124791</td>\n",
       "      <td>0.650899</td>\n",
       "      <td>0.028746</td>\n",
       "      <td>0.130530</td>\n",
       "      <td>0.010287</td>\n",
       "      <td>0.294590</td>\n",
       "      <td>0.081368</td>\n",
       "      <td>0.012196</td>\n",
       "      <td>0.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_9</th>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_12</th>\n",
       "      <td>12</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.361881</td>\n",
       "      <td>0.177326</td>\n",
       "      <td>0.023456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260621</td>\n",
       "      <td>0.158303</td>\n",
       "      <td>0.747632</td>\n",
       "      <td>0.028311</td>\n",
       "      <td>0.173299</td>\n",
       "      <td>0.012735</td>\n",
       "      <td>0.312116</td>\n",
       "      <td>0.075963</td>\n",
       "      <td>0.013082</td>\n",
       "      <td>0.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_18</th>\n",
       "      <td>18</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_24</th>\n",
       "      <td>24</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_30</th>\n",
       "      <td>30</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_36</th>\n",
       "      <td>36</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.434762</td>\n",
       "      <td>0.198397</td>\n",
       "      <td>0.022196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.017254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200227</td>\n",
       "      <td>0.188303</td>\n",
       "      <td>0.583497</td>\n",
       "      <td>0.022381</td>\n",
       "      <td>0.167151</td>\n",
       "      <td>0.010767</td>\n",
       "      <td>0.304103</td>\n",
       "      <td>0.074722</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.027481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_42</th>\n",
       "      <td>42</td>\n",
       "      <td>12.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_48</th>\n",
       "      <td>48</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_54</th>\n",
       "      <td>54</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_60</th>\n",
       "      <td>60</td>\n",
       "      <td>23.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 973 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          visit_month  updrs_1  updrs_2  updrs_3  updrs_4  AADDTWEPFASGK  \\\n",
       "visit_id                                                                   \n",
       "55_0                0     10.0      6.0     15.0      NaN       0.378517   \n",
       "55_3                3     10.0      7.0     25.0      NaN            NaN   \n",
       "55_6                6      8.0     10.0     34.0      NaN       0.367787   \n",
       "55_9                9      8.0      9.0     30.0      0.0            NaN   \n",
       "55_12              12     10.0     10.0     41.0      0.0       0.361881   \n",
       "55_18              18      7.0     13.0     38.0      0.0            NaN   \n",
       "55_24              24     16.0      9.0     49.0      0.0            NaN   \n",
       "55_30              30     14.0     13.0     49.0      0.0            NaN   \n",
       "55_36              36     17.0     18.0     51.0      0.0       0.434762   \n",
       "55_42              42     12.0     20.0     41.0      0.0            NaN   \n",
       "55_48              48     17.0     16.0     52.0      0.0            NaN   \n",
       "55_54              54     12.0     18.0     51.0      0.0            NaN   \n",
       "55_60              60     23.0     21.0     56.0      0.0            NaN   \n",
       "\n",
       "          AAFGQGSGPIMLDEVQC(UniMod_4)TGTEASLADC(UniMod_4)K  \\\n",
       "visit_id                                                     \n",
       "55_0                                              0.200462   \n",
       "55_3                                                   NaN   \n",
       "55_6                                              0.179201   \n",
       "55_9                                                   NaN   \n",
       "55_12                                             0.177326   \n",
       "55_18                                                  NaN   \n",
       "55_24                                                  NaN   \n",
       "55_30                                                  NaN   \n",
       "55_36                                             0.198397   \n",
       "55_42                                                  NaN   \n",
       "55_48                                                  NaN   \n",
       "55_54                                                  NaN   \n",
       "55_60                                                  NaN   \n",
       "\n",
       "          AAFTEC(UniMod_4)C(UniMod_4)QAADK  AANEVSSADVK  \\\n",
       "visit_id                                                  \n",
       "55_0                              0.021986          NaN   \n",
       "55_3                                   NaN          NaN   \n",
       "55_6                              0.024904          1.0   \n",
       "55_9                                   NaN          NaN   \n",
       "55_12                             0.023456          1.0   \n",
       "55_18                                  NaN          NaN   \n",
       "55_24                                  NaN          NaN   \n",
       "55_30                                  NaN          NaN   \n",
       "55_36                             0.022196          1.0   \n",
       "55_42                                  NaN          NaN   \n",
       "55_48                                  NaN          NaN   \n",
       "55_54                                  NaN          NaN   \n",
       "55_60                                  NaN          NaN   \n",
       "\n",
       "          AATGEC(UniMod_4)TATVGKR  ...  YSLTYIYTGLSK   YTTEIIK  \\\n",
       "visit_id                           ...                           \n",
       "55_0                     0.016156  ...      0.218140  0.150558   \n",
       "55_3                          NaN  ...           NaN       NaN   \n",
       "55_6                     0.022779  ...      0.179524  0.124791   \n",
       "55_9                          NaN  ...           NaN       NaN   \n",
       "55_12                    0.013740  ...      0.260621  0.158303   \n",
       "55_18                         NaN  ...           NaN       NaN   \n",
       "55_24                         NaN  ...           NaN       NaN   \n",
       "55_30                         NaN  ...           NaN       NaN   \n",
       "55_36                    0.017254  ...      0.200227  0.188303   \n",
       "55_42                         NaN  ...           NaN       NaN   \n",
       "55_48                         NaN  ...           NaN       NaN   \n",
       "55_54                         NaN  ...           NaN       NaN   \n",
       "55_60                         NaN  ...           NaN       NaN   \n",
       "\n",
       "          YVGGQEHFAHLLILR  YVM(UniMod_35)LPVADQDQC(UniMod_4)IR  \\\n",
       "visit_id                                                         \n",
       "55_0             0.682349                             0.027018   \n",
       "55_3                  NaN                                  NaN   \n",
       "55_6             0.650899                             0.028746   \n",
       "55_9                  NaN                                  NaN   \n",
       "55_12            0.747632                             0.028311   \n",
       "55_18                 NaN                                  NaN   \n",
       "55_24                 NaN                                  NaN   \n",
       "55_30                 NaN                                  NaN   \n",
       "55_36            0.583497                             0.022381   \n",
       "55_42                 NaN                                  NaN   \n",
       "55_48                 NaN                                  NaN   \n",
       "55_54                 NaN                                  NaN   \n",
       "55_60                 NaN                                  NaN   \n",
       "\n",
       "          YVMLPVADQDQC(UniMod_4)IR  YVNKEIQNAVNGVK  YWGVASFLQK  \\\n",
       "visit_id                                                         \n",
       "55_0                      0.146764        0.011330    0.290456   \n",
       "55_3                           NaN             NaN         NaN   \n",
       "55_6                      0.130530        0.010287    0.294590   \n",
       "55_9                           NaN             NaN         NaN   \n",
       "55_12                     0.173299        0.012735    0.312116   \n",
       "55_18                          NaN             NaN         NaN   \n",
       "55_24                          NaN             NaN         NaN   \n",
       "55_30                          NaN             NaN         NaN   \n",
       "55_36                     0.167151        0.010767    0.304103   \n",
       "55_42                          NaN             NaN         NaN   \n",
       "55_48                          NaN             NaN         NaN   \n",
       "55_54                          NaN             NaN         NaN   \n",
       "55_60                          NaN             NaN         NaN   \n",
       "\n",
       "          YYC(UniMod_4)FQGNQFLR  YYTYLIMNK  YYWGGQYTWDMAK  \n",
       "visit_id                                                   \n",
       "55_0                   0.091600   0.011818       0.026223  \n",
       "55_3                        NaN        NaN            NaN  \n",
       "55_6                   0.081368   0.012196       0.034900  \n",
       "55_9                        NaN        NaN            NaN  \n",
       "55_12                  0.075963   0.013082       0.037700  \n",
       "55_18                       NaN        NaN            NaN  \n",
       "55_24                       NaN        NaN            NaN  \n",
       "55_30                       NaN        NaN            NaN  \n",
       "55_36                  0.074722   0.011481       0.027481  \n",
       "55_42                       NaN        NaN            NaN  \n",
       "55_48                       NaN        NaN            NaN  \n",
       "55_54                       NaN        NaN            NaN  \n",
       "55_60                       NaN        NaN            NaN  \n",
       "\n",
       "[13 rows x 973 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selection only the patient ID = 55\n",
    "\n",
    "visit_id_specific = df.iloc[0:13]\n",
    "visit_id_specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can see that some of the value are missing over the month.\n",
    "\n",
    "visit_id_specific.plot(use_index=True, y=\"AADDTWEPFASGK\", kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_id_specific.plot(use_index=True, y=\"AAFGQGSGPIMLDEVQC(UniMod_4)TGTEASLADC(UniMod_4)K\", kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_month</th>\n",
       "      <th>updrs_1</th>\n",
       "      <th>updrs_2</th>\n",
       "      <th>updrs_3</th>\n",
       "      <th>updrs_4</th>\n",
       "      <th>AADDTWEPFASGK</th>\n",
       "      <th>AAFGQGSGPIMLDEVQC(UniMod_4)TGTEASLADC(UniMod_4)K</th>\n",
       "      <th>AAFTEC(UniMod_4)C(UniMod_4)QAADK</th>\n",
       "      <th>AANEVSSADVK</th>\n",
       "      <th>AATGEC(UniMod_4)TATVGKR</th>\n",
       "      <th>...</th>\n",
       "      <th>YSLTYIYTGLSK</th>\n",
       "      <th>YTTEIIK</th>\n",
       "      <th>YVGGQEHFAHLLILR</th>\n",
       "      <th>YVM(UniMod_35)LPVADQDQC(UniMod_4)IR</th>\n",
       "      <th>YVMLPVADQDQC(UniMod_4)IR</th>\n",
       "      <th>YVNKEIQNAVNGVK</th>\n",
       "      <th>YWGVASFLQK</th>\n",
       "      <th>YYC(UniMod_4)FQGNQFLR</th>\n",
       "      <th>YYTYLIMNK</th>\n",
       "      <th>YYWGGQYTWDMAK</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visit_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55_0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378517</td>\n",
       "      <td>0.200462</td>\n",
       "      <td>0.021986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218140</td>\n",
       "      <td>0.150558</td>\n",
       "      <td>0.682349</td>\n",
       "      <td>0.027018</td>\n",
       "      <td>0.146764</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>0.290456</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.011818</td>\n",
       "      <td>0.026223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378517</td>\n",
       "      <td>0.200462</td>\n",
       "      <td>0.021986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218140</td>\n",
       "      <td>0.150558</td>\n",
       "      <td>0.682349</td>\n",
       "      <td>0.027018</td>\n",
       "      <td>0.146764</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>0.290456</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.011818</td>\n",
       "      <td>0.026223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_6</th>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.367787</td>\n",
       "      <td>0.179201</td>\n",
       "      <td>0.024904</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179524</td>\n",
       "      <td>0.124791</td>\n",
       "      <td>0.650899</td>\n",
       "      <td>0.028746</td>\n",
       "      <td>0.130530</td>\n",
       "      <td>0.010287</td>\n",
       "      <td>0.294590</td>\n",
       "      <td>0.081368</td>\n",
       "      <td>0.012196</td>\n",
       "      <td>0.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_9</th>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.367787</td>\n",
       "      <td>0.179201</td>\n",
       "      <td>0.024904</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179524</td>\n",
       "      <td>0.124791</td>\n",
       "      <td>0.650899</td>\n",
       "      <td>0.028746</td>\n",
       "      <td>0.130530</td>\n",
       "      <td>0.010287</td>\n",
       "      <td>0.294590</td>\n",
       "      <td>0.081368</td>\n",
       "      <td>0.012196</td>\n",
       "      <td>0.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55_12</th>\n",
       "      <td>12</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.361881</td>\n",
       "      <td>0.177326</td>\n",
       "      <td>0.023456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260621</td>\n",
       "      <td>0.158303</td>\n",
       "      <td>0.747632</td>\n",
       "      <td>0.028311</td>\n",
       "      <td>0.173299</td>\n",
       "      <td>0.012735</td>\n",
       "      <td>0.312116</td>\n",
       "      <td>0.075963</td>\n",
       "      <td>0.013082</td>\n",
       "      <td>0.037700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65043_48</th>\n",
       "      <td>48</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.374608</td>\n",
       "      <td>0.259382</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227781</td>\n",
       "      <td>0.115953</td>\n",
       "      <td>0.684073</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>0.142682</td>\n",
       "      <td>0.012314</td>\n",
       "      <td>0.275821</td>\n",
       "      <td>0.075535</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>0.033099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65043_54</th>\n",
       "      <td>54</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.374608</td>\n",
       "      <td>0.259382</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227781</td>\n",
       "      <td>0.115953</td>\n",
       "      <td>0.684073</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>0.142682</td>\n",
       "      <td>0.012314</td>\n",
       "      <td>0.275821</td>\n",
       "      <td>0.075535</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>0.033099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65043_60</th>\n",
       "      <td>60</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.374608</td>\n",
       "      <td>0.259382</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227781</td>\n",
       "      <td>0.115953</td>\n",
       "      <td>0.684073</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>0.142682</td>\n",
       "      <td>0.012314</td>\n",
       "      <td>0.275821</td>\n",
       "      <td>0.075535</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>0.033099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65043_72</th>\n",
       "      <td>72</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.374608</td>\n",
       "      <td>0.259382</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227781</td>\n",
       "      <td>0.115953</td>\n",
       "      <td>0.684073</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>0.142682</td>\n",
       "      <td>0.012314</td>\n",
       "      <td>0.275821</td>\n",
       "      <td>0.075535</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>0.033099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65043_84</th>\n",
       "      <td>84</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.374608</td>\n",
       "      <td>0.259382</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.013325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227781</td>\n",
       "      <td>0.115953</td>\n",
       "      <td>0.684073</td>\n",
       "      <td>0.013892</td>\n",
       "      <td>0.142682</td>\n",
       "      <td>0.012314</td>\n",
       "      <td>0.275821</td>\n",
       "      <td>0.075535</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>0.033099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2615 rows × 973 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          visit_month  updrs_1  updrs_2  updrs_3  updrs_4  AADDTWEPFASGK  \\\n",
       "visit_id                                                                   \n",
       "55_0                0     10.0      6.0     15.0      NaN       0.378517   \n",
       "55_3                3     10.0      7.0     25.0      NaN       0.378517   \n",
       "55_6                6      8.0     10.0     34.0      NaN       0.367787   \n",
       "55_9                9      8.0      9.0     30.0      0.0       0.367787   \n",
       "55_12              12     10.0     10.0     41.0      0.0       0.361881   \n",
       "...               ...      ...      ...      ...      ...            ...   \n",
       "65043_48           48      7.0      6.0     13.0      0.0       0.374608   \n",
       "65043_54           54      4.0      8.0     11.0      1.0       0.374608   \n",
       "65043_60           60      6.0      6.0     16.0      1.0       0.374608   \n",
       "65043_72           72      3.0      9.0     14.0      1.0       0.374608   \n",
       "65043_84           84      7.0      9.0     20.0      3.0       0.374608   \n",
       "\n",
       "          AAFGQGSGPIMLDEVQC(UniMod_4)TGTEASLADC(UniMod_4)K  \\\n",
       "visit_id                                                     \n",
       "55_0                                              0.200462   \n",
       "55_3                                              0.200462   \n",
       "55_6                                              0.179201   \n",
       "55_9                                              0.179201   \n",
       "55_12                                             0.177326   \n",
       "...                                                    ...   \n",
       "65043_48                                          0.259382   \n",
       "65043_54                                          0.259382   \n",
       "65043_60                                          0.259382   \n",
       "65043_72                                          0.259382   \n",
       "65043_84                                          0.259382   \n",
       "\n",
       "          AAFTEC(UniMod_4)C(UniMod_4)QAADK  AANEVSSADVK  \\\n",
       "visit_id                                                  \n",
       "55_0                              0.021986          NaN   \n",
       "55_3                              0.021986          NaN   \n",
       "55_6                              0.024904          1.0   \n",
       "55_9                              0.024904          1.0   \n",
       "55_12                             0.023456          1.0   \n",
       "...                                    ...          ...   \n",
       "65043_48                          0.027886          1.0   \n",
       "65043_54                          0.027886          1.0   \n",
       "65043_60                          0.027886          1.0   \n",
       "65043_72                          0.027886          1.0   \n",
       "65043_84                          0.027886          1.0   \n",
       "\n",
       "          AATGEC(UniMod_4)TATVGKR  ...  YSLTYIYTGLSK   YTTEIIK  \\\n",
       "visit_id                           ...                           \n",
       "55_0                     0.016156  ...      0.218140  0.150558   \n",
       "55_3                     0.016156  ...      0.218140  0.150558   \n",
       "55_6                     0.022779  ...      0.179524  0.124791   \n",
       "55_9                     0.022779  ...      0.179524  0.124791   \n",
       "55_12                    0.013740  ...      0.260621  0.158303   \n",
       "...                           ...  ...           ...       ...   \n",
       "65043_48                 0.013325  ...      0.227781  0.115953   \n",
       "65043_54                 0.013325  ...      0.227781  0.115953   \n",
       "65043_60                 0.013325  ...      0.227781  0.115953   \n",
       "65043_72                 0.013325  ...      0.227781  0.115953   \n",
       "65043_84                 0.013325  ...      0.227781  0.115953   \n",
       "\n",
       "          YVGGQEHFAHLLILR  YVM(UniMod_35)LPVADQDQC(UniMod_4)IR  \\\n",
       "visit_id                                                         \n",
       "55_0             0.682349                             0.027018   \n",
       "55_3             0.682349                             0.027018   \n",
       "55_6             0.650899                             0.028746   \n",
       "55_9             0.650899                             0.028746   \n",
       "55_12            0.747632                             0.028311   \n",
       "...                   ...                                  ...   \n",
       "65043_48         0.684073                             0.013892   \n",
       "65043_54         0.684073                             0.013892   \n",
       "65043_60         0.684073                             0.013892   \n",
       "65043_72         0.684073                             0.013892   \n",
       "65043_84         0.684073                             0.013892   \n",
       "\n",
       "          YVMLPVADQDQC(UniMod_4)IR  YVNKEIQNAVNGVK  YWGVASFLQK  \\\n",
       "visit_id                                                         \n",
       "55_0                      0.146764        0.011330    0.290456   \n",
       "55_3                      0.146764        0.011330    0.290456   \n",
       "55_6                      0.130530        0.010287    0.294590   \n",
       "55_9                      0.130530        0.010287    0.294590   \n",
       "55_12                     0.173299        0.012735    0.312116   \n",
       "...                            ...             ...         ...   \n",
       "65043_48                  0.142682        0.012314    0.275821   \n",
       "65043_54                  0.142682        0.012314    0.275821   \n",
       "65043_60                  0.142682        0.012314    0.275821   \n",
       "65043_72                  0.142682        0.012314    0.275821   \n",
       "65043_84                  0.142682        0.012314    0.275821   \n",
       "\n",
       "          YYC(UniMod_4)FQGNQFLR  YYTYLIMNK  YYWGGQYTWDMAK  \n",
       "visit_id                                                   \n",
       "55_0                   0.091600   0.011818       0.026223  \n",
       "55_3                   0.091600   0.011818       0.026223  \n",
       "55_6                   0.081368   0.012196       0.034900  \n",
       "55_9                   0.081368   0.012196       0.034900  \n",
       "55_12                  0.075963   0.013082       0.037700  \n",
       "...                         ...        ...            ...  \n",
       "65043_48               0.075535   0.012647       0.033099  \n",
       "65043_54               0.075535   0.012647       0.033099  \n",
       "65043_60               0.075535   0.012647       0.033099  \n",
       "65043_72               0.075535   0.012647       0.033099  \n",
       "65043_84               0.075535   0.012647       0.033099  \n",
       "\n",
       "[2615 rows x 973 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.fillna(method=\"ffill\")\n",
    "df1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark : I fill all the NaN value by the previous one and use it in the model as we will see later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Mean absolute error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The mean absolute error is one of a number of ways of comparing forecasts with their eventual outcomes.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_dataset, test_dataset, label):\n",
    "    \n",
    "    # Break into target and features\n",
    "    train_dataset = train_dataset.dropna(subset=[label])\n",
    "    test_dataset = test_dataset.dropna(subset=[label])\n",
    "\n",
    "    # only the peptide percentage in the protein\n",
    "    train_features = train_dataset.drop(target, axis=1).copy()\n",
    "    test_features = test_dataset.drop(target, axis=1).copy()\n",
    "\n",
    "    # only the updrs\n",
    "    train_labels = train_dataset[label]\n",
    "    test_labels = test_dataset[label]\n",
    "    \n",
    "    # Fill the Nan values by the mean value of the column\n",
    "    for c in train_features.columns:\n",
    "        m = train_features[c].mean()\n",
    "        train_features[c] = train_features[c].fillna(m)\n",
    "\n",
    "    for c in test_features.columns:\n",
    "        m = test_features[c].mean()\n",
    "        test_features[c] = test_features[c].fillna(m)\n",
    "        \n",
    "    return train_features, test_features, train_labels, test_labels\n",
    "\n",
    "target = [\"updrs_1\", \"updrs_2\", \"updrs_3\", \"updrs_4\"]\n",
    "\n",
    "train_dataset = df.sample(frac=0.7, random_state=0)\n",
    "test_dataset = df.drop(train_dataset.index)\n",
    "\n",
    "for label in target:\n",
    "\n",
    "    train_features, test_features, train_labels, test_labels = prepare_data(train_dataset, test_dataset,label)\n",
    "    train__features_val = train_features[-200:]\n",
    "    train_labels_val = train_labels[-200:]\n",
    "    \n",
    "    # Normalization\n",
    "\n",
    "    features = np.array(train_features)\n",
    "    feat_normalizer = layers.Normalization(axis=-1)\n",
    "    feat_normalizer.adapt(features)\n",
    "\n",
    "    # Model\n",
    "    model = tf.keras.Sequential([\n",
    "        feat_normalizer,\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(484, activation='relu'),\n",
    "        layers.Dense(242, activation='relu'),\n",
    "        layers.Dense(121, activation='relu'),\n",
    "        layers.Dense(60, activation='relu'),\n",
    "        layers.Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    # Training\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=10e-3),\n",
    "        loss='mean_absolute_error',\n",
    "        metrics=\"mean_absolute_error\"\n",
    "        )\n",
    "\n",
    "    # Fitting\n",
    "    history = model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        batch_size=64,\n",
    "        epochs=100,\n",
    "        # Calculate validation results on 20% of the training data.\n",
    "        validation_split = 0.2,\n",
    "        )\n",
    "\n",
    "    # Evaluate the model on the test data using `evaluate`\n",
    "    print(\"Evaluate on test data\")\n",
    "    results = model.evaluate(test_features, test_labels, batch_size=128)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "\n",
    "    # Generate predictions (probabilities -- the output of the last layer)\n",
    "    # on new data using `predict`\n",
    "    print(\"Generate predictions for 3 samples\")\n",
    "    predictions = model.predict(test_features[:3])\n",
    "    print(\"predictions shape:\", predictions.shape\n",
    "    )\n",
    "\n",
    "    # summarize history for mean_absolute_percentage_error\n",
    "    plt.plot(history.history['mean_absolute_error'])\n",
    "    plt.plot(history.history['val_mean_absolute_error'])\n",
    "    plt.title('model mean_absolute_percentage_error')\n",
    "    plt.ylabel('mean_absolute_error')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_dataset, test_dataset, label):\n",
    "    \n",
    "    # Break into target and features\n",
    "    train_dataset = train_dataset.dropna(subset=[label])\n",
    "    test_dataset = test_dataset.dropna(subset=[label])\n",
    "\n",
    "    # only the peptide percentage in the protein\n",
    "    train_features = train_dataset.drop(target, axis=1).copy()\n",
    "    test_features = test_dataset.drop(target, axis=1).copy()\n",
    "\n",
    "    # only the updrs\n",
    "    train_labels = train_dataset[label]\n",
    "    test_labels = test_dataset[label]\n",
    "    \n",
    "    # Fill the Nan values by the mean value of the column\n",
    "    for c in train_features.columns:\n",
    "        m = train_features[c].mean()\n",
    "        train_features[c] = train_features[c].fillna(m)\n",
    "\n",
    "    for c in test_features.columns:\n",
    "        m = test_features[c].mean()\n",
    "        test_features[c] = test_features[c].fillna(m)\n",
    "        \n",
    "    return train_features, test_features, train_labels, test_labels\n",
    "\n",
    "target = [\"updrs_1\", \"updrs_2\", \"updrs_3\", \"updrs_4\"]\n",
    "\n",
    "train_dataset = df.sample(frac=0.7, random_state=0)\n",
    "test_dataset = df.drop(train_dataset.index)\n",
    "\n",
    "for label in target:\n",
    "\n",
    "    train_features, test_features, train_labels, test_labels = prepare_data(train_dataset, test_dataset,label)\n",
    "    train__features_val = train_features[-200:]\n",
    "    train_labels_val = train_labels[-200:]\n",
    "    \n",
    "    # Normalization\n",
    "\n",
    "    features = np.array(train_features)\n",
    "    feat_normalizer = layers.Normalization(axis=-1)\n",
    "    feat_normalizer.adapt(features)\n",
    "\n",
    "    # Model\n",
    "    model = tf.keras.Sequential([\n",
    "        feat_normalizer,\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(484, activation='relu'),\n",
    "        layers.Dense(242, activation='relu'),\n",
    "        layers.Dense(121, activation='relu'),\n",
    "        layers.Dense(60, activation='relu'),\n",
    "        layers.Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    # Training\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss='mean_absolute_error',\n",
    "        metrics=\"mean_absolute_error\"\n",
    "        )\n",
    "\n",
    "    # Fitting\n",
    "    history = model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        batch_size=64,\n",
    "        epochs=100,\n",
    "        # Calculate validation results on 20% of the training data.\n",
    "        validation_split = 0.2,\n",
    "        )\n",
    "\n",
    "    # Evaluate the model on the test data using `evaluate`\n",
    "    print(\"Evaluate on test data\")\n",
    "    results = model.evaluate(test_features, test_labels, batch_size=128)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "\n",
    "    # Generate predictions (probabilities -- the output of the last layer)\n",
    "    # on new data using `predict`\n",
    "    print(\"Generate predictions for 3 samples\")\n",
    "    predictions = model.predict(test_features[:3])\n",
    "    print(\"predictions shape:\", predictions.shape\n",
    "    )\n",
    "    \n",
    "    # summarize history for mean_absolute_percentage_error\n",
    "    plt.plot(history.history['mean_absolute_error'])\n",
    "    plt.plot(history.history['val_mean_absolute_error'])\n",
    "    plt.title('model mean_absolute_error')\n",
    "    plt.ylabel('mean_absolute_error')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_dataset, test_dataset, label):\n",
    "    \n",
    "    # Break into target and features\n",
    "    train_dataset = train_dataset.dropna(subset=[label])\n",
    "    test_dataset = test_dataset.dropna(subset=[label])\n",
    "\n",
    "    # only the peptide percentage in the protein\n",
    "    train_features = train_dataset.drop(target, axis=1).copy()\n",
    "    test_features = test_dataset.drop(target, axis=1).copy()\n",
    "\n",
    "    # only the updrs\n",
    "    train_labels = train_dataset[label]\n",
    "    test_labels = test_dataset[label]\n",
    "    \n",
    "    # Fill the Nan values by the mean value of the column\n",
    "    for c in train_features.columns:\n",
    "        m = train_features[c].mean()\n",
    "        train_features[c] = train_features[c].fillna(m)\n",
    "\n",
    "    for c in test_features.columns:\n",
    "        m = test_features[c].mean()\n",
    "        test_features[c] = test_features[c].fillna(m)\n",
    "        \n",
    "    return train_features, test_features, train_labels, test_labels\n",
    "\n",
    "target = [\"updrs_1\", \"updrs_2\", \"updrs_3\", \"updrs_4\"]\n",
    "\n",
    "train_dataset = df.sample(frac=0.7, random_state=0)\n",
    "test_dataset = df.drop(train_dataset.index)\n",
    "\n",
    "for label in target:\n",
    "\n",
    "    train_features, test_features, train_labels, test_labels = prepare_data(train_dataset, test_dataset,label)\n",
    "    train__features_val = train_features[-200:]\n",
    "    train_labels_val = train_labels[-200:]\n",
    "    \n",
    "    # Normalization\n",
    "\n",
    "    features = np.array(train_features)\n",
    "    feat_normalizer = layers.Normalization(axis=-1)\n",
    "    feat_normalizer.adapt(features)\n",
    "\n",
    "    # Model\n",
    "    model = tf.keras.Sequential([\n",
    "        feat_normalizer,\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(484, activation='relu'),\n",
    "        layers.Dense(242, activation='relu'),\n",
    "        layers.Dense(121, activation='relu'),\n",
    "        layers.Dense(60, activation='relu'),\n",
    "        layers.Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    # Training\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss='mean_absolute_error',\n",
    "        metrics=\"mean_absolute_error\"\n",
    "        )\n",
    "\n",
    "    # Fitting\n",
    "    history = model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        batch_size=64,\n",
    "        epochs=100,\n",
    "        # Calculate validation results on 20% of the training data.\n",
    "        validation_split = 0.2,\n",
    "        )\n",
    "\n",
    "    # Evaluate the model on the test data using `evaluate`\n",
    "    print(\"Evaluate on test data\")\n",
    "    results = model.evaluate(test_features, test_labels, batch_size=128)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "\n",
    "    # Generate predictions (probabilities -- the output of the last layer)\n",
    "    # on new data using `predict`\n",
    "    print(\"Generate predictions for 3 samples\")\n",
    "    predictions = model.predict(test_features[:3])\n",
    "    print(\"predictions shape:\", predictions.shape\n",
    "    )\n",
    "    \n",
    "    # summarize history for mean_absolute_percentage_error\n",
    "    plt.plot(history.history['mean_absolute_percentage_error'])\n",
    "    plt.plot(history.history['val_mean_absolute_percentage_error'])\n",
    "    plt.title('model mean_absolute_error')\n",
    "    plt.ylabel('mean_absolute_error')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 % data train and 50 % data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_dataset, test_dataset, label):\n",
    "    \n",
    "    # Break into target and features\n",
    "    train_dataset = train_dataset.dropna(subset=[label])\n",
    "    test_dataset = test_dataset.dropna(subset=[label])\n",
    "\n",
    "    # only the peptide percentage in the protein\n",
    "    train_features = train_dataset.drop(target, axis=1).copy()\n",
    "    test_features = test_dataset.drop(target, axis=1).copy()\n",
    "\n",
    "    # only the updrs\n",
    "    train_labels = train_dataset[label]\n",
    "    test_labels = test_dataset[label]\n",
    "    \n",
    "    # Fill the Nan values by the mean value of the column\n",
    "    for c in train_features.columns:\n",
    "        m = train_features[c].mean()\n",
    "        train_features[c] = train_features[c].fillna(m)\n",
    "\n",
    "    for c in test_features.columns:\n",
    "        m = test_features[c].mean()\n",
    "        test_features[c] = test_features[c].fillna(m)\n",
    "        \n",
    "    return train_features, test_features, train_labels, test_labels\n",
    "\n",
    "target = [\"updrs_1\", \"updrs_2\", \"updrs_3\", \"updrs_4\"]\n",
    "\n",
    "train_dataset = df.sample(frac=0.5, random_state=0)\n",
    "test_dataset = df.drop(train_dataset.index)\n",
    "\n",
    "for label in target:\n",
    "\n",
    "    train_features, test_features, train_labels, test_labels = prepare_data(train_dataset, test_dataset,label)\n",
    "    train__features_val = train_features[-200:]\n",
    "    train_labels_val = train_labels[-200:]\n",
    "    \n",
    "    # Normalization\n",
    "\n",
    "    features = np.array(train_features)\n",
    "    feat_normalizer = layers.Normalization(axis=-1)\n",
    "    feat_normalizer.adapt(features)\n",
    "\n",
    "    # Model\n",
    "    model = tf.keras.Sequential([\n",
    "        feat_normalizer,\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(484, activation='relu'),\n",
    "        layers.Dense(242, activation='relu'),\n",
    "        layers.Dense(121, activation='relu'),\n",
    "        layers.Dense(60, activation='relu'),\n",
    "        layers.Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    # Training\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss='mean_absolute_error',\n",
    "        metrics=\"mean_absolute_percentage_error\"\n",
    "        )\n",
    "\n",
    "    # Fitting\n",
    "    history = model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        batch_size=64,\n",
    "        epochs=100,\n",
    "        # Calculate validation results on 20% of the training data.\n",
    "        validation_split = 0.2,\n",
    "        )\n",
    "\n",
    "    # Evaluate the model on the test data using `evaluate`\n",
    "    print(\"Evaluate on test data\")\n",
    "    results = model.evaluate(test_features, test_labels, batch_size=128)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "\n",
    "    # Generate predictions (probabilities -- the output of the last layer)\n",
    "    # on new data using `predict`\n",
    "    print(\"Generate predictions for 3 samples\")\n",
    "    predictions = model.predict(test_features[:3])\n",
    "    print(\"predictions shape:\", predictions.shape\n",
    "    )\n",
    "    \n",
    "    # summarize history for mean_absolute_percentage_error\n",
    "    plt.plot(history.history['mean_absolute_error'])\n",
    "    plt.plot(history.history['val_mean_absolute_error'])\n",
    "    plt.title('model mean_absolute_error')\n",
    "    plt.ylabel('mean_absolute_error')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce again the learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_dataset, test_dataset, label):\n",
    "    \n",
    "    # Break into target and features\n",
    "    train_dataset = train_dataset.dropna(subset=[label])\n",
    "    test_dataset = test_dataset.dropna(subset=[label])\n",
    "\n",
    "    # only the peptide percentage in the protein\n",
    "    train_features = train_dataset.drop(target, axis=1).copy()\n",
    "    test_features = test_dataset.drop(target, axis=1).copy()\n",
    "\n",
    "    # only the updrs\n",
    "    train_labels = train_dataset[label]\n",
    "    test_labels = test_dataset[label]\n",
    "    \n",
    "    # Fill the Nan values by the mean value of the column\n",
    "    for c in train_features.columns:\n",
    "        m = train_features[c].mean()\n",
    "        train_features[c] = train_features[c].fillna(m)\n",
    "\n",
    "    for c in test_features.columns:\n",
    "        m = test_features[c].mean()\n",
    "        test_features[c] = test_features[c].fillna(m)\n",
    "        \n",
    "    return train_features, test_features, train_labels, test_labels\n",
    "\n",
    "target = [\"updrs_1\", \"updrs_2\", \"updrs_3\", \"updrs_4\"]\n",
    "\n",
    "train_dataset = df.sample(frac=0.5, random_state=0)\n",
    "test_dataset = df.drop(train_dataset.index)\n",
    "\n",
    "for label in target:\n",
    "\n",
    "    train_features, test_features, train_labels, test_labels = prepare_data(train_dataset, test_dataset,label)\n",
    "    train__features_val = train_features[-200:]\n",
    "    train_labels_val = train_labels[-200:]\n",
    "    \n",
    "    # Normalization\n",
    "\n",
    "    features = np.array(train_features)\n",
    "    feat_normalizer = layers.Normalization(axis=-1)\n",
    "    feat_normalizer.adapt(features)\n",
    "\n",
    "    # Model\n",
    "    model = tf.keras.Sequential([\n",
    "        feat_normalizer,\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(484, activation='relu'),\n",
    "        layers.Dense(242, activation='relu'),\n",
    "        layers.Dense(121, activation='relu'),\n",
    "        layers.Dense(60, activation='relu'),\n",
    "        layers.Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    # Training\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-8),\n",
    "        loss='mean_absolute_error',\n",
    "        metrics=\"mean_absolute_error\"\n",
    "        )\n",
    "\n",
    "    # Fitting\n",
    "    history = model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        batch_size=64,\n",
    "        epochs=100,\n",
    "        # Calculate validation results on 20% of the training data.\n",
    "        validation_split = 0.2,\n",
    "        )\n",
    "\n",
    "    # Evaluate the model on the test data using `evaluate`\n",
    "    print(\"Evaluate on test data\")\n",
    "    results = model.evaluate(test_features, test_labels, batch_size=128)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "\n",
    "    # Generate predictions (probabilities -- the output of the last layer)\n",
    "    # on new data using `predict`\n",
    "    print(\"Generate predictions for 3 samples\")\n",
    "    predictions = model.predict(test_features[:3])\n",
    "    print(\"predictions shape:\", predictions.shape\n",
    "    )\n",
    "    \n",
    "    # summarize history for mean_absolute_percentage_error\n",
    "    plt.plot(history.history['mean_absolute_error'])\n",
    "    plt.plot(history.history['val_mean_absolute_error'])\n",
    "    plt.title('model mean_absolute_error')\n",
    "    plt.ylabel('mean_absolute_error')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clearly seems a good way to reach the zero loss and zero mean absolute error, however we need maybe 10 000 epochs. It takes a long time.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the df1 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "17/17 [==============================] - 4s 126ms/step - loss: 2.2983 - mean_absolute_error: 2.2983 - val_loss: 2.2930 - val_mean_absolute_error: 2.2930\n",
      "Epoch 2/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 2.2927 - mean_absolute_error: 2.2927 - val_loss: 2.2913 - val_mean_absolute_error: 2.2913\n",
      "Epoch 3/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 2.2916 - mean_absolute_error: 2.2916 - val_loss: 2.2908 - val_mean_absolute_error: 2.2908\n",
      "Epoch 4/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 2.2908 - mean_absolute_error: 2.2908 - val_loss: 2.2905 - val_mean_absolute_error: 2.2905\n",
      "Epoch 5/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 2.2901 - mean_absolute_error: 2.2901 - val_loss: 2.2903 - val_mean_absolute_error: 2.2903\n",
      "Epoch 6/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 2.2894 - mean_absolute_error: 2.2894 - val_loss: 2.2900 - val_mean_absolute_error: 2.2900\n",
      "Epoch 7/1000\n",
      "17/17 [==============================] - 2s 148ms/step - loss: 2.2888 - mean_absolute_error: 2.2888 - val_loss: 2.2897 - val_mean_absolute_error: 2.2897\n",
      "Epoch 8/1000\n",
      "17/17 [==============================] - 3s 165ms/step - loss: 2.2881 - mean_absolute_error: 2.2881 - val_loss: 2.2895 - val_mean_absolute_error: 2.2895\n",
      "Epoch 9/1000\n",
      "17/17 [==============================] - 6s 349ms/step - loss: 2.2875 - mean_absolute_error: 2.2875 - val_loss: 2.2893 - val_mean_absolute_error: 2.2893\n",
      "Epoch 10/1000\n",
      "17/17 [==============================] - 3s 177ms/step - loss: 2.2869 - mean_absolute_error: 2.2869 - val_loss: 2.2890 - val_mean_absolute_error: 2.2890\n",
      "Epoch 11/1000\n",
      "17/17 [==============================] - 3s 158ms/step - loss: 2.2863 - mean_absolute_error: 2.2863 - val_loss: 2.2888 - val_mean_absolute_error: 2.2888\n",
      "Epoch 12/1000\n",
      "17/17 [==============================] - 3s 157ms/step - loss: 2.2858 - mean_absolute_error: 2.2858 - val_loss: 2.2886 - val_mean_absolute_error: 2.2886\n",
      "Epoch 13/1000\n",
      "17/17 [==============================] - 3s 157ms/step - loss: 2.2852 - mean_absolute_error: 2.2852 - val_loss: 2.2883 - val_mean_absolute_error: 2.2883\n",
      "Epoch 14/1000\n",
      "17/17 [==============================] - 3s 188ms/step - loss: 2.2847 - mean_absolute_error: 2.2847 - val_loss: 2.2881 - val_mean_absolute_error: 2.2881\n",
      "Epoch 15/1000\n",
      "17/17 [==============================] - 3s 172ms/step - loss: 2.2842 - mean_absolute_error: 2.2842 - val_loss: 2.2879 - val_mean_absolute_error: 2.2879\n",
      "Epoch 16/1000\n",
      "17/17 [==============================] - 3s 178ms/step - loss: 2.2837 - mean_absolute_error: 2.2837 - val_loss: 2.2877 - val_mean_absolute_error: 2.2877\n",
      "Epoch 17/1000\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 2.2832 - mean_absolute_error: 2.2832 - val_loss: 2.2875 - val_mean_absolute_error: 2.2875\n",
      "Epoch 18/1000\n",
      "17/17 [==============================] - 4s 229ms/step - loss: 2.2826 - mean_absolute_error: 2.2826 - val_loss: 2.2873 - val_mean_absolute_error: 2.2873\n",
      "Epoch 19/1000\n",
      "17/17 [==============================] - 3s 186ms/step - loss: 2.2821 - mean_absolute_error: 2.2821 - val_loss: 2.2871 - val_mean_absolute_error: 2.2871\n",
      "Epoch 20/1000\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 2.2816 - mean_absolute_error: 2.2816 - val_loss: 2.2869 - val_mean_absolute_error: 2.2869\n",
      "Epoch 21/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 2.2810 - mean_absolute_error: 2.2810 - val_loss: 2.2867 - val_mean_absolute_error: 2.2867\n",
      "Epoch 22/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 2.2805 - mean_absolute_error: 2.2805 - val_loss: 2.2865 - val_mean_absolute_error: 2.2865\n",
      "Epoch 23/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 2.2800 - mean_absolute_error: 2.2800 - val_loss: 2.2863 - val_mean_absolute_error: 2.2863\n",
      "Epoch 24/1000\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 2.2794 - mean_absolute_error: 2.2794 - val_loss: 2.2860 - val_mean_absolute_error: 2.2860\n",
      "Epoch 25/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 2.2788 - mean_absolute_error: 2.2788 - val_loss: 2.2858 - val_mean_absolute_error: 2.2858\n",
      "Epoch 26/1000\n",
      "17/17 [==============================] - 4s 234ms/step - loss: 2.2782 - mean_absolute_error: 2.2782 - val_loss: 2.2856 - val_mean_absolute_error: 2.2856\n",
      "Epoch 27/1000\n",
      "17/17 [==============================] - 4s 212ms/step - loss: 2.2776 - mean_absolute_error: 2.2776 - val_loss: 2.2854 - val_mean_absolute_error: 2.2854\n",
      "Epoch 28/1000\n",
      "17/17 [==============================] - 2s 137ms/step - loss: 2.2770 - mean_absolute_error: 2.2770 - val_loss: 2.2851 - val_mean_absolute_error: 2.2851\n",
      "Epoch 29/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 2.2764 - mean_absolute_error: 2.2764 - val_loss: 2.2849 - val_mean_absolute_error: 2.2849\n",
      "Epoch 30/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 2.2758 - mean_absolute_error: 2.2758 - val_loss: 2.2846 - val_mean_absolute_error: 2.2846\n",
      "Epoch 31/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 2.2751 - mean_absolute_error: 2.2751 - val_loss: 2.2844 - val_mean_absolute_error: 2.2844\n",
      "Epoch 32/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 2.2743 - mean_absolute_error: 2.2743 - val_loss: 2.2841 - val_mean_absolute_error: 2.2841\n",
      "Epoch 33/1000\n",
      "17/17 [==============================] - 4s 227ms/step - loss: 2.2737 - mean_absolute_error: 2.2737 - val_loss: 2.2838 - val_mean_absolute_error: 2.2838\n",
      "Epoch 34/1000\n",
      "17/17 [==============================] - 3s 181ms/step - loss: 2.2729 - mean_absolute_error: 2.2729 - val_loss: 2.2835 - val_mean_absolute_error: 2.2835\n",
      "Epoch 35/1000\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 2.2721 - mean_absolute_error: 2.2721 - val_loss: 2.2832 - val_mean_absolute_error: 2.2832\n",
      "Epoch 36/1000\n",
      "17/17 [==============================] - 2s 137ms/step - loss: 2.2712 - mean_absolute_error: 2.2712 - val_loss: 2.2829 - val_mean_absolute_error: 2.2829\n",
      "Epoch 37/1000\n",
      "17/17 [==============================] - 3s 149ms/step - loss: 2.2703 - mean_absolute_error: 2.2703 - val_loss: 2.2825 - val_mean_absolute_error: 2.2825\n",
      "Epoch 38/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 2.2695 - mean_absolute_error: 2.2695 - val_loss: 2.2821 - val_mean_absolute_error: 2.2821\n",
      "Epoch 39/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 2.2687 - mean_absolute_error: 2.2687 - val_loss: 2.2817 - val_mean_absolute_error: 2.2817\n",
      "Epoch 40/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 2.2676 - mean_absolute_error: 2.2676 - val_loss: 2.2814 - val_mean_absolute_error: 2.2814\n",
      "Epoch 41/1000\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 2.2666 - mean_absolute_error: 2.2666 - val_loss: 2.2809 - val_mean_absolute_error: 2.2809\n",
      "Epoch 42/1000\n",
      "17/17 [==============================] - 3s 146ms/step - loss: 2.2654 - mean_absolute_error: 2.2654 - val_loss: 2.2806 - val_mean_absolute_error: 2.2806\n",
      "Epoch 43/1000\n",
      "17/17 [==============================] - 3s 163ms/step - loss: 2.2643 - mean_absolute_error: 2.2643 - val_loss: 2.2801 - val_mean_absolute_error: 2.2801\n",
      "Epoch 44/1000\n",
      "17/17 [==============================] - 3s 164ms/step - loss: 2.2631 - mean_absolute_error: 2.2631 - val_loss: 2.2795 - val_mean_absolute_error: 2.2795\n",
      "Epoch 45/1000\n",
      "17/17 [==============================] - 2s 146ms/step - loss: 2.2619 - mean_absolute_error: 2.2619 - val_loss: 2.2790 - val_mean_absolute_error: 2.2790\n",
      "Epoch 46/1000\n",
      "17/17 [==============================] - 3s 149ms/step - loss: 2.2607 - mean_absolute_error: 2.2607 - val_loss: 2.2784 - val_mean_absolute_error: 2.2784\n",
      "Epoch 47/1000\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 2.2593 - mean_absolute_error: 2.2593 - val_loss: 2.2778 - val_mean_absolute_error: 2.2778\n",
      "Epoch 48/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 2.2579 - mean_absolute_error: 2.2579 - val_loss: 2.2773 - val_mean_absolute_error: 2.2773\n",
      "Epoch 49/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 2.2566 - mean_absolute_error: 2.2566 - val_loss: 2.2768 - val_mean_absolute_error: 2.2768\n",
      "Epoch 50/1000\n",
      "17/17 [==============================] - 2s 139ms/step - loss: 2.2551 - mean_absolute_error: 2.2551 - val_loss: 2.2761 - val_mean_absolute_error: 2.2761\n",
      "Epoch 51/1000\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 2.2534 - mean_absolute_error: 2.2534 - val_loss: 2.2754 - val_mean_absolute_error: 2.2754\n",
      "Epoch 52/1000\n",
      "17/17 [==============================] - 2s 141ms/step - loss: 2.2517 - mean_absolute_error: 2.2517 - val_loss: 2.2747 - val_mean_absolute_error: 2.2747\n",
      "Epoch 53/1000\n",
      "17/17 [==============================] - 2s 140ms/step - loss: 2.2499 - mean_absolute_error: 2.2499 - val_loss: 2.2740 - val_mean_absolute_error: 2.2740\n",
      "Epoch 54/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 2.2481 - mean_absolute_error: 2.2481 - val_loss: 2.2733 - val_mean_absolute_error: 2.2733\n",
      "Epoch 55/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 2.2462 - mean_absolute_error: 2.2462 - val_loss: 2.2724 - val_mean_absolute_error: 2.2724\n",
      "Epoch 56/1000\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 2.2442 - mean_absolute_error: 2.2442 - val_loss: 2.2716 - val_mean_absolute_error: 2.2716\n",
      "Epoch 57/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 2.2420 - mean_absolute_error: 2.2420 - val_loss: 2.2707 - val_mean_absolute_error: 2.2707\n",
      "Epoch 58/1000\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 2.2398 - mean_absolute_error: 2.2398 - val_loss: 2.2697 - val_mean_absolute_error: 2.2697\n",
      "Epoch 59/1000\n",
      "17/17 [==============================] - 2s 142ms/step - loss: 2.2375 - mean_absolute_error: 2.2375 - val_loss: 2.2688 - val_mean_absolute_error: 2.2688\n",
      "Epoch 60/1000\n",
      "17/17 [==============================] - 3s 176ms/step - loss: 2.2353 - mean_absolute_error: 2.2353 - val_loss: 2.2679 - val_mean_absolute_error: 2.2679\n",
      "Epoch 61/1000\n",
      "17/17 [==============================] - 3s 155ms/step - loss: 2.2326 - mean_absolute_error: 2.2326 - val_loss: 2.2667 - val_mean_absolute_error: 2.2667\n",
      "Epoch 62/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 2.2298 - mean_absolute_error: 2.2298 - val_loss: 2.2655 - val_mean_absolute_error: 2.2655\n",
      "Epoch 63/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 2.2268 - mean_absolute_error: 2.2268 - val_loss: 2.2642 - val_mean_absolute_error: 2.2642\n",
      "Epoch 64/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 2.2238 - mean_absolute_error: 2.2238 - val_loss: 2.2629 - val_mean_absolute_error: 2.2629\n",
      "Epoch 65/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 2.2205 - mean_absolute_error: 2.2205 - val_loss: 2.2614 - val_mean_absolute_error: 2.2614\n",
      "Epoch 66/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 2.2168 - mean_absolute_error: 2.2168 - val_loss: 2.2601 - val_mean_absolute_error: 2.2601\n",
      "Epoch 67/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 2.2131 - mean_absolute_error: 2.2131 - val_loss: 2.2585 - val_mean_absolute_error: 2.2585\n",
      "Epoch 68/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 2.2093 - mean_absolute_error: 2.2093 - val_loss: 2.2570 - val_mean_absolute_error: 2.2570\n",
      "Epoch 69/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 2.2052 - mean_absolute_error: 2.2052 - val_loss: 2.2551 - val_mean_absolute_error: 2.2551\n",
      "Epoch 70/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 2.2005 - mean_absolute_error: 2.2005 - val_loss: 2.2532 - val_mean_absolute_error: 2.2532\n",
      "Epoch 71/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 2.1956 - mean_absolute_error: 2.1956 - val_loss: 2.2512 - val_mean_absolute_error: 2.2512\n",
      "Epoch 72/1000\n",
      "17/17 [==============================] - 2s 138ms/step - loss: 2.1904 - mean_absolute_error: 2.1904 - val_loss: 2.2493 - val_mean_absolute_error: 2.2493\n",
      "Epoch 73/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 2.1852 - mean_absolute_error: 2.1852 - val_loss: 2.2468 - val_mean_absolute_error: 2.2468\n",
      "Epoch 74/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 2.1787 - mean_absolute_error: 2.1787 - val_loss: 2.2440 - val_mean_absolute_error: 2.2440\n",
      "Epoch 75/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 2.1717 - mean_absolute_error: 2.1717 - val_loss: 2.2408 - val_mean_absolute_error: 2.2408\n",
      "Epoch 76/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 2.1640 - mean_absolute_error: 2.1640 - val_loss: 2.2377 - val_mean_absolute_error: 2.2377\n",
      "Epoch 77/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 2.1561 - mean_absolute_error: 2.1561 - val_loss: 2.2344 - val_mean_absolute_error: 2.2344\n",
      "Epoch 78/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 2.1476 - mean_absolute_error: 2.1476 - val_loss: 2.2310 - val_mean_absolute_error: 2.2310\n",
      "Epoch 79/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 2.1388 - mean_absolute_error: 2.1388 - val_loss: 2.2273 - val_mean_absolute_error: 2.2273\n",
      "Epoch 80/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 2.1293 - mean_absolute_error: 2.1293 - val_loss: 2.2229 - val_mean_absolute_error: 2.2229\n",
      "Epoch 81/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 2.1191 - mean_absolute_error: 2.1191 - val_loss: 2.2186 - val_mean_absolute_error: 2.2186\n",
      "Epoch 82/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 2.1078 - mean_absolute_error: 2.1078 - val_loss: 2.2140 - val_mean_absolute_error: 2.2140\n",
      "Epoch 83/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 2.0965 - mean_absolute_error: 2.0965 - val_loss: 2.2093 - val_mean_absolute_error: 2.2093\n",
      "Epoch 84/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 2.0841 - mean_absolute_error: 2.0841 - val_loss: 2.2037 - val_mean_absolute_error: 2.2037\n",
      "Epoch 85/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 2.0711 - mean_absolute_error: 2.0711 - val_loss: 2.1981 - val_mean_absolute_error: 2.1981\n",
      "Epoch 86/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 2.0563 - mean_absolute_error: 2.0563 - val_loss: 2.1918 - val_mean_absolute_error: 2.1918\n",
      "Epoch 87/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 2.0412 - mean_absolute_error: 2.0412 - val_loss: 2.1849 - val_mean_absolute_error: 2.1849\n",
      "Epoch 88/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 2.0248 - mean_absolute_error: 2.0248 - val_loss: 2.1775 - val_mean_absolute_error: 2.1775\n",
      "Epoch 89/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 2.0078 - mean_absolute_error: 2.0078 - val_loss: 2.1709 - val_mean_absolute_error: 2.1709\n",
      "Epoch 90/1000\n",
      "17/17 [==============================] - 3s 158ms/step - loss: 1.9897 - mean_absolute_error: 1.9897 - val_loss: 2.1628 - val_mean_absolute_error: 2.1628\n",
      "Epoch 91/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 1.9694 - mean_absolute_error: 1.9694 - val_loss: 2.1547 - val_mean_absolute_error: 2.1547\n",
      "Epoch 92/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 1.9490 - mean_absolute_error: 1.9490 - val_loss: 2.1475 - val_mean_absolute_error: 2.1475\n",
      "Epoch 93/1000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 1.9296 - mean_absolute_error: 1.9296 - val_loss: 2.1398 - val_mean_absolute_error: 2.1398\n",
      "Epoch 94/1000\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 1.9080 - mean_absolute_error: 1.9080 - val_loss: 2.1310 - val_mean_absolute_error: 2.1310\n",
      "Epoch 95/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 1.8893 - mean_absolute_error: 1.8893 - val_loss: 2.1225 - val_mean_absolute_error: 2.1225\n",
      "Epoch 96/1000\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 1.8689 - mean_absolute_error: 1.8689 - val_loss: 2.1148 - val_mean_absolute_error: 2.1148\n",
      "Epoch 97/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 1.8490 - mean_absolute_error: 1.8490 - val_loss: 2.1054 - val_mean_absolute_error: 2.1054\n",
      "Epoch 98/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 1.8290 - mean_absolute_error: 1.8290 - val_loss: 2.0962 - val_mean_absolute_error: 2.0962\n",
      "Epoch 99/1000\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 1.8088 - mean_absolute_error: 1.8088 - val_loss: 2.0906 - val_mean_absolute_error: 2.0906\n",
      "Epoch 100/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 1.7896 - mean_absolute_error: 1.7896 - val_loss: 2.0809 - val_mean_absolute_error: 2.0809\n",
      "Epoch 101/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 1.7687 - mean_absolute_error: 1.7687 - val_loss: 2.0723 - val_mean_absolute_error: 2.0723\n",
      "Epoch 102/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 1.7477 - mean_absolute_error: 1.7477 - val_loss: 2.0652 - val_mean_absolute_error: 2.0652\n",
      "Epoch 103/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 1.7280 - mean_absolute_error: 1.7280 - val_loss: 2.0594 - val_mean_absolute_error: 2.0594\n",
      "Epoch 104/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 1.7078 - mean_absolute_error: 1.7078 - val_loss: 2.0480 - val_mean_absolute_error: 2.0480\n",
      "Epoch 105/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 1.6880 - mean_absolute_error: 1.6880 - val_loss: 2.0427 - val_mean_absolute_error: 2.0427\n",
      "Epoch 106/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 1.6666 - mean_absolute_error: 1.6666 - val_loss: 2.0393 - val_mean_absolute_error: 2.0393\n",
      "Epoch 107/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 1.6470 - mean_absolute_error: 1.6470 - val_loss: 2.0350 - val_mean_absolute_error: 2.0350\n",
      "Epoch 108/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 1.6257 - mean_absolute_error: 1.6257 - val_loss: 2.0293 - val_mean_absolute_error: 2.0293\n",
      "Epoch 109/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 1.6087 - mean_absolute_error: 1.6087 - val_loss: 2.0285 - val_mean_absolute_error: 2.0285\n",
      "Epoch 110/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 1.5885 - mean_absolute_error: 1.5885 - val_loss: 2.0209 - val_mean_absolute_error: 2.0209\n",
      "Epoch 111/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 1.5683 - mean_absolute_error: 1.5683 - val_loss: 2.0171 - val_mean_absolute_error: 2.0171\n",
      "Epoch 112/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 1.5493 - mean_absolute_error: 1.5493 - val_loss: 2.0142 - val_mean_absolute_error: 2.0142\n",
      "Epoch 113/1000\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 1.5301 - mean_absolute_error: 1.5301 - val_loss: 2.0101 - val_mean_absolute_error: 2.0101\n",
      "Epoch 114/1000\n",
      "17/17 [==============================] - 3s 190ms/step - loss: 1.5090 - mean_absolute_error: 1.5090 - val_loss: 2.0069 - val_mean_absolute_error: 2.0069\n",
      "Epoch 115/1000\n",
      "17/17 [==============================] - 7s 395ms/step - loss: 1.4902 - mean_absolute_error: 1.4902 - val_loss: 2.0031 - val_mean_absolute_error: 2.0031\n",
      "Epoch 116/1000\n",
      "17/17 [==============================] - 4s 252ms/step - loss: 1.4704 - mean_absolute_error: 1.4704 - val_loss: 2.0014 - val_mean_absolute_error: 2.0014\n",
      "Epoch 117/1000\n",
      "17/17 [==============================] - 4s 207ms/step - loss: 1.4534 - mean_absolute_error: 1.4534 - val_loss: 1.9979 - val_mean_absolute_error: 1.9979\n",
      "Epoch 118/1000\n",
      "17/17 [==============================] - 3s 190ms/step - loss: 1.4327 - mean_absolute_error: 1.4327 - val_loss: 1.9945 - val_mean_absolute_error: 1.9945\n",
      "Epoch 119/1000\n",
      "17/17 [==============================] - 3s 175ms/step - loss: 1.4185 - mean_absolute_error: 1.4185 - val_loss: 1.9897 - val_mean_absolute_error: 1.9897\n",
      "Epoch 120/1000\n",
      "17/17 [==============================] - 3s 200ms/step - loss: 1.3993 - mean_absolute_error: 1.3993 - val_loss: 1.9882 - val_mean_absolute_error: 1.9882\n",
      "Epoch 121/1000\n",
      "17/17 [==============================] - 3s 169ms/step - loss: 1.3830 - mean_absolute_error: 1.3830 - val_loss: 1.9812 - val_mean_absolute_error: 1.9812\n",
      "Epoch 122/1000\n",
      "17/17 [==============================] - 3s 156ms/step - loss: 1.3673 - mean_absolute_error: 1.3673 - val_loss: 1.9809 - val_mean_absolute_error: 1.9809\n",
      "Epoch 123/1000\n",
      "17/17 [==============================] - 2s 142ms/step - loss: 1.3545 - mean_absolute_error: 1.3545 - val_loss: 1.9766 - val_mean_absolute_error: 1.9766\n",
      "Epoch 124/1000\n",
      "17/17 [==============================] - 3s 171ms/step - loss: 1.3375 - mean_absolute_error: 1.3375 - val_loss: 1.9709 - val_mean_absolute_error: 1.9709\n",
      "Epoch 125/1000\n",
      "17/17 [==============================] - 3s 164ms/step - loss: 1.3216 - mean_absolute_error: 1.3216 - val_loss: 1.9692 - val_mean_absolute_error: 1.9692\n",
      "Epoch 126/1000\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 1.3065 - mean_absolute_error: 1.3065 - val_loss: 1.9626 - val_mean_absolute_error: 1.9626\n",
      "Epoch 127/1000\n",
      "17/17 [==============================] - 3s 157ms/step - loss: 1.2921 - mean_absolute_error: 1.2921 - val_loss: 1.9590 - val_mean_absolute_error: 1.9590\n",
      "Epoch 128/1000\n",
      "17/17 [==============================] - 2s 142ms/step - loss: 1.2784 - mean_absolute_error: 1.2784 - val_loss: 1.9575 - val_mean_absolute_error: 1.9575\n",
      "Epoch 129/1000\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 1.2685 - mean_absolute_error: 1.2685 - val_loss: 1.9498 - val_mean_absolute_error: 1.9498\n",
      "Epoch 130/1000\n",
      "17/17 [==============================] - 2s 141ms/step - loss: 1.2545 - mean_absolute_error: 1.2545 - val_loss: 1.9468 - val_mean_absolute_error: 1.9468\n",
      "Epoch 131/1000\n",
      "17/17 [==============================] - 2s 142ms/step - loss: 1.2406 - mean_absolute_error: 1.2406 - val_loss: 1.9434 - val_mean_absolute_error: 1.9434\n",
      "Epoch 132/1000\n",
      "17/17 [==============================] - 2s 140ms/step - loss: 1.2293 - mean_absolute_error: 1.2293 - val_loss: 1.9388 - val_mean_absolute_error: 1.9388\n",
      "Epoch 133/1000\n",
      "17/17 [==============================] - 2s 138ms/step - loss: 1.2204 - mean_absolute_error: 1.2204 - val_loss: 1.9340 - val_mean_absolute_error: 1.9340\n",
      "Epoch 134/1000\n",
      "17/17 [==============================] - 2s 146ms/step - loss: 1.2073 - mean_absolute_error: 1.2073 - val_loss: 1.9340 - val_mean_absolute_error: 1.9340\n",
      "Epoch 135/1000\n",
      "17/17 [==============================] - 3s 155ms/step - loss: 1.1960 - mean_absolute_error: 1.1960 - val_loss: 1.9282 - val_mean_absolute_error: 1.9282\n",
      "Epoch 136/1000\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 1.1850 - mean_absolute_error: 1.1850 - val_loss: 1.9234 - val_mean_absolute_error: 1.9234\n",
      "Epoch 137/1000\n",
      "17/17 [==============================] - 3s 148ms/step - loss: 1.1730 - mean_absolute_error: 1.1730 - val_loss: 1.9198 - val_mean_absolute_error: 1.9198\n",
      "Epoch 138/1000\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 1.1623 - mean_absolute_error: 1.1623 - val_loss: 1.9156 - val_mean_absolute_error: 1.9156\n",
      "Epoch 139/1000\n",
      "17/17 [==============================] - 3s 177ms/step - loss: 1.1526 - mean_absolute_error: 1.1526 - val_loss: 1.9128 - val_mean_absolute_error: 1.9128\n",
      "Epoch 140/1000\n",
      "17/17 [==============================] - 3s 173ms/step - loss: 1.1441 - mean_absolute_error: 1.1441 - val_loss: 1.9097 - val_mean_absolute_error: 1.9097\n",
      "Epoch 141/1000\n",
      "17/17 [==============================] - 3s 196ms/step - loss: 1.1331 - mean_absolute_error: 1.1331 - val_loss: 1.9068 - val_mean_absolute_error: 1.9068\n",
      "Epoch 142/1000\n",
      "17/17 [==============================] - 3s 160ms/step - loss: 1.1258 - mean_absolute_error: 1.1258 - val_loss: 1.9021 - val_mean_absolute_error: 1.9021\n",
      "Epoch 143/1000\n",
      "17/17 [==============================] - 2s 145ms/step - loss: 1.1165 - mean_absolute_error: 1.1165 - val_loss: 1.9003 - val_mean_absolute_error: 1.9003\n",
      "Epoch 144/1000\n",
      "17/17 [==============================] - 3s 148ms/step - loss: 1.1083 - mean_absolute_error: 1.1083 - val_loss: 1.8998 - val_mean_absolute_error: 1.8998\n",
      "Epoch 145/1000\n",
      "17/17 [==============================] - 2s 142ms/step - loss: 1.1015 - mean_absolute_error: 1.1015 - val_loss: 1.8941 - val_mean_absolute_error: 1.8941\n",
      "Epoch 146/1000\n",
      "17/17 [==============================] - 2s 140ms/step - loss: 1.0924 - mean_absolute_error: 1.0924 - val_loss: 1.8916 - val_mean_absolute_error: 1.8916\n",
      "Epoch 147/1000\n",
      "17/17 [==============================] - 2s 138ms/step - loss: 1.0842 - mean_absolute_error: 1.0842 - val_loss: 1.8885 - val_mean_absolute_error: 1.8885\n",
      "Epoch 148/1000\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 1.0796 - mean_absolute_error: 1.0796 - val_loss: 1.8849 - val_mean_absolute_error: 1.8849\n",
      "Epoch 149/1000\n",
      "17/17 [==============================] - 2s 138ms/step - loss: 1.0696 - mean_absolute_error: 1.0696 - val_loss: 1.8808 - val_mean_absolute_error: 1.8808\n",
      "Epoch 150/1000\n",
      "17/17 [==============================] - 2s 138ms/step - loss: 1.0623 - mean_absolute_error: 1.0623 - val_loss: 1.8765 - val_mean_absolute_error: 1.8765\n",
      "Epoch 151/1000\n",
      "17/17 [==============================] - 2s 147ms/step - loss: 1.0533 - mean_absolute_error: 1.0533 - val_loss: 1.8737 - val_mean_absolute_error: 1.8737\n",
      "Epoch 152/1000\n",
      "17/17 [==============================] - 3s 155ms/step - loss: 1.0463 - mean_absolute_error: 1.0463 - val_loss: 1.8719 - val_mean_absolute_error: 1.8719\n",
      "Epoch 153/1000\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 1.0389 - mean_absolute_error: 1.0389 - val_loss: 1.8686 - val_mean_absolute_error: 1.8686\n",
      "Epoch 154/1000\n",
      "17/17 [==============================] - 2s 146ms/step - loss: 1.0321 - mean_absolute_error: 1.0321 - val_loss: 1.8668 - val_mean_absolute_error: 1.8668\n",
      "Epoch 155/1000\n",
      "17/17 [==============================] - 2s 146ms/step - loss: 1.0266 - mean_absolute_error: 1.0266 - val_loss: 1.8651 - val_mean_absolute_error: 1.8651\n",
      "Epoch 156/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 1.0220 - mean_absolute_error: 1.0220 - val_loss: 1.8637 - val_mean_absolute_error: 1.8637\n",
      "Epoch 157/1000\n",
      "17/17 [==============================] - 2s 142ms/step - loss: 1.0180 - mean_absolute_error: 1.0180 - val_loss: 1.8615 - val_mean_absolute_error: 1.8615\n",
      "Epoch 158/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 1.0095 - mean_absolute_error: 1.0095 - val_loss: 1.8578 - val_mean_absolute_error: 1.8578\n",
      "Epoch 159/1000\n",
      "17/17 [==============================] - 2s 146ms/step - loss: 1.0015 - mean_absolute_error: 1.0015 - val_loss: 1.8531 - val_mean_absolute_error: 1.8531\n",
      "Epoch 160/1000\n",
      "17/17 [==============================] - 2s 139ms/step - loss: 0.9963 - mean_absolute_error: 0.9963 - val_loss: 1.8559 - val_mean_absolute_error: 1.8559\n",
      "Epoch 161/1000\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.9971 - mean_absolute_error: 0.9971 - val_loss: 1.8519 - val_mean_absolute_error: 1.8519\n",
      "Epoch 162/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.9861 - mean_absolute_error: 0.9861 - val_loss: 1.8449 - val_mean_absolute_error: 1.8449\n",
      "Epoch 163/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.9786 - mean_absolute_error: 0.9786 - val_loss: 1.8431 - val_mean_absolute_error: 1.8431\n",
      "Epoch 164/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.9742 - mean_absolute_error: 0.9742 - val_loss: 1.8427 - val_mean_absolute_error: 1.8427\n",
      "Epoch 165/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.9711 - mean_absolute_error: 0.9711 - val_loss: 1.8403 - val_mean_absolute_error: 1.8403\n",
      "Epoch 166/1000\n",
      "17/17 [==============================] - 2s 138ms/step - loss: 0.9642 - mean_absolute_error: 0.9642 - val_loss: 1.8385 - val_mean_absolute_error: 1.8385\n",
      "Epoch 167/1000\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 0.9590 - mean_absolute_error: 0.9590 - val_loss: 1.8416 - val_mean_absolute_error: 1.8416\n",
      "Epoch 168/1000\n",
      "17/17 [==============================] - 2s 142ms/step - loss: 0.9555 - mean_absolute_error: 0.9555 - val_loss: 1.8341 - val_mean_absolute_error: 1.8341\n",
      "Epoch 169/1000\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 0.9490 - mean_absolute_error: 0.9490 - val_loss: 1.8337 - val_mean_absolute_error: 1.8337\n",
      "Epoch 170/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.9424 - mean_absolute_error: 0.9424 - val_loss: 1.8298 - val_mean_absolute_error: 1.8298\n",
      "Epoch 171/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.9385 - mean_absolute_error: 0.9385 - val_loss: 1.8327 - val_mean_absolute_error: 1.8327\n",
      "Epoch 172/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.9337 - mean_absolute_error: 0.9337 - val_loss: 1.8270 - val_mean_absolute_error: 1.8270\n",
      "Epoch 173/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.9271 - mean_absolute_error: 0.9271 - val_loss: 1.8251 - val_mean_absolute_error: 1.8251\n",
      "Epoch 174/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.9227 - mean_absolute_error: 0.9227 - val_loss: 1.8227 - val_mean_absolute_error: 1.8227\n",
      "Epoch 175/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.9190 - mean_absolute_error: 0.9190 - val_loss: 1.8225 - val_mean_absolute_error: 1.8225\n",
      "Epoch 176/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.9131 - mean_absolute_error: 0.9131 - val_loss: 1.8196 - val_mean_absolute_error: 1.8196\n",
      "Epoch 177/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.9094 - mean_absolute_error: 0.9094 - val_loss: 1.8189 - val_mean_absolute_error: 1.8189\n",
      "Epoch 178/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.9054 - mean_absolute_error: 0.9054 - val_loss: 1.8165 - val_mean_absolute_error: 1.8165\n",
      "Epoch 179/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.8979 - mean_absolute_error: 0.8979 - val_loss: 1.8156 - val_mean_absolute_error: 1.8156\n",
      "Epoch 180/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.8911 - mean_absolute_error: 0.8911 - val_loss: 1.8126 - val_mean_absolute_error: 1.8126\n",
      "Epoch 181/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.8864 - mean_absolute_error: 0.8864 - val_loss: 1.8112 - val_mean_absolute_error: 1.8112\n",
      "Epoch 182/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.8833 - mean_absolute_error: 0.8833 - val_loss: 1.8091 - val_mean_absolute_error: 1.8091\n",
      "Epoch 183/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.8771 - mean_absolute_error: 0.8771 - val_loss: 1.8088 - val_mean_absolute_error: 1.8088\n",
      "Epoch 184/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.8716 - mean_absolute_error: 0.8716 - val_loss: 1.8070 - val_mean_absolute_error: 1.8070\n",
      "Epoch 185/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.8669 - mean_absolute_error: 0.8669 - val_loss: 1.8061 - val_mean_absolute_error: 1.8061\n",
      "Epoch 186/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.8618 - mean_absolute_error: 0.8618 - val_loss: 1.8043 - val_mean_absolute_error: 1.8043\n",
      "Epoch 187/1000\n",
      "17/17 [==============================] - 3s 148ms/step - loss: 0.8587 - mean_absolute_error: 0.8587 - val_loss: 1.8025 - val_mean_absolute_error: 1.8025\n",
      "Epoch 188/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.8536 - mean_absolute_error: 0.8536 - val_loss: 1.8008 - val_mean_absolute_error: 1.8008\n",
      "Epoch 189/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.8490 - mean_absolute_error: 0.8490 - val_loss: 1.8023 - val_mean_absolute_error: 1.8023\n",
      "Epoch 190/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.8447 - mean_absolute_error: 0.8447 - val_loss: 1.8005 - val_mean_absolute_error: 1.8005\n",
      "Epoch 191/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.8387 - mean_absolute_error: 0.8387 - val_loss: 1.7958 - val_mean_absolute_error: 1.7958\n",
      "Epoch 192/1000\n",
      "17/17 [==============================] - 2s 140ms/step - loss: 0.8340 - mean_absolute_error: 0.8340 - val_loss: 1.7974 - val_mean_absolute_error: 1.7974\n",
      "Epoch 193/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.8281 - mean_absolute_error: 0.8281 - val_loss: 1.7956 - val_mean_absolute_error: 1.7956\n",
      "Epoch 194/1000\n",
      "17/17 [==============================] - 3s 148ms/step - loss: 0.8231 - mean_absolute_error: 0.8231 - val_loss: 1.7945 - val_mean_absolute_error: 1.7945\n",
      "Epoch 195/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.8202 - mean_absolute_error: 0.8202 - val_loss: 1.7933 - val_mean_absolute_error: 1.7933\n",
      "Epoch 196/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.8140 - mean_absolute_error: 0.8140 - val_loss: 1.7932 - val_mean_absolute_error: 1.7932\n",
      "Epoch 197/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.8079 - mean_absolute_error: 0.8079 - val_loss: 1.7924 - val_mean_absolute_error: 1.7924\n",
      "Epoch 198/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.8035 - mean_absolute_error: 0.8035 - val_loss: 1.7919 - val_mean_absolute_error: 1.7919\n",
      "Epoch 199/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.7998 - mean_absolute_error: 0.7998 - val_loss: 1.7870 - val_mean_absolute_error: 1.7870\n",
      "Epoch 200/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.7937 - mean_absolute_error: 0.7937 - val_loss: 1.7869 - val_mean_absolute_error: 1.7869\n",
      "Epoch 201/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.7907 - mean_absolute_error: 0.7907 - val_loss: 1.7876 - val_mean_absolute_error: 1.7876\n",
      "Epoch 202/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.7864 - mean_absolute_error: 0.7864 - val_loss: 1.7876 - val_mean_absolute_error: 1.7876\n",
      "Epoch 203/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.7811 - mean_absolute_error: 0.7811 - val_loss: 1.7866 - val_mean_absolute_error: 1.7866\n",
      "Epoch 204/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.7774 - mean_absolute_error: 0.7774 - val_loss: 1.7840 - val_mean_absolute_error: 1.7840\n",
      "Epoch 205/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.7722 - mean_absolute_error: 0.7722 - val_loss: 1.7840 - val_mean_absolute_error: 1.7840\n",
      "Epoch 206/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.7693 - mean_absolute_error: 0.7693 - val_loss: 1.7819 - val_mean_absolute_error: 1.7819\n",
      "Epoch 207/1000\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.7650 - mean_absolute_error: 0.7650 - val_loss: 1.7820 - val_mean_absolute_error: 1.7820\n",
      "Epoch 208/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.7641 - mean_absolute_error: 0.7641 - val_loss: 1.7811 - val_mean_absolute_error: 1.7811\n",
      "Epoch 209/1000\n",
      "17/17 [==============================] - 2s 137ms/step - loss: 0.7571 - mean_absolute_error: 0.7571 - val_loss: 1.7813 - val_mean_absolute_error: 1.7813\n",
      "Epoch 210/1000\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.7532 - mean_absolute_error: 0.7532 - val_loss: 1.7795 - val_mean_absolute_error: 1.7795\n",
      "Epoch 211/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.7478 - mean_absolute_error: 0.7478 - val_loss: 1.7783 - val_mean_absolute_error: 1.7783\n",
      "Epoch 212/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.7456 - mean_absolute_error: 0.7456 - val_loss: 1.7771 - val_mean_absolute_error: 1.7771\n",
      "Epoch 213/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.7521 - mean_absolute_error: 0.7521 - val_loss: 1.7785 - val_mean_absolute_error: 1.7785\n",
      "Epoch 214/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.7432 - mean_absolute_error: 0.7432 - val_loss: 1.7807 - val_mean_absolute_error: 1.7807\n",
      "Epoch 215/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.7382 - mean_absolute_error: 0.7382 - val_loss: 1.7793 - val_mean_absolute_error: 1.7793\n",
      "Epoch 216/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.7311 - mean_absolute_error: 0.7311 - val_loss: 1.7748 - val_mean_absolute_error: 1.7748\n",
      "Epoch 217/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.7272 - mean_absolute_error: 0.7272 - val_loss: 1.7747 - val_mean_absolute_error: 1.7747\n",
      "Epoch 218/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.7238 - mean_absolute_error: 0.7238 - val_loss: 1.7718 - val_mean_absolute_error: 1.7718\n",
      "Epoch 219/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.7214 - mean_absolute_error: 0.7214 - val_loss: 1.7704 - val_mean_absolute_error: 1.7704\n",
      "Epoch 220/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.7180 - mean_absolute_error: 0.7180 - val_loss: 1.7701 - val_mean_absolute_error: 1.7701\n",
      "Epoch 221/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.7161 - mean_absolute_error: 0.7161 - val_loss: 1.7682 - val_mean_absolute_error: 1.7682\n",
      "Epoch 222/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.7153 - mean_absolute_error: 0.7153 - val_loss: 1.7691 - val_mean_absolute_error: 1.7691\n",
      "Epoch 223/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.7089 - mean_absolute_error: 0.7089 - val_loss: 1.7720 - val_mean_absolute_error: 1.7720\n",
      "Epoch 224/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.7054 - mean_absolute_error: 0.7054 - val_loss: 1.7698 - val_mean_absolute_error: 1.7698\n",
      "Epoch 225/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.7008 - mean_absolute_error: 0.7008 - val_loss: 1.7694 - val_mean_absolute_error: 1.7694\n",
      "Epoch 226/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.6995 - mean_absolute_error: 0.6995 - val_loss: 1.7721 - val_mean_absolute_error: 1.7721\n",
      "Epoch 227/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.6943 - mean_absolute_error: 0.6943 - val_loss: 1.7691 - val_mean_absolute_error: 1.7691\n",
      "Epoch 228/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.6904 - mean_absolute_error: 0.6904 - val_loss: 1.7669 - val_mean_absolute_error: 1.7669\n",
      "Epoch 229/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.6859 - mean_absolute_error: 0.6859 - val_loss: 1.7683 - val_mean_absolute_error: 1.7683\n",
      "Epoch 230/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.6814 - mean_absolute_error: 0.6814 - val_loss: 1.7660 - val_mean_absolute_error: 1.7660\n",
      "Epoch 231/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.6786 - mean_absolute_error: 0.6786 - val_loss: 1.7662 - val_mean_absolute_error: 1.7662\n",
      "Epoch 232/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.6735 - mean_absolute_error: 0.6735 - val_loss: 1.7673 - val_mean_absolute_error: 1.7673\n",
      "Epoch 233/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.6735 - mean_absolute_error: 0.6735 - val_loss: 1.7651 - val_mean_absolute_error: 1.7651\n",
      "Epoch 234/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.6707 - mean_absolute_error: 0.6707 - val_loss: 1.7650 - val_mean_absolute_error: 1.7650\n",
      "Epoch 235/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.6662 - mean_absolute_error: 0.6662 - val_loss: 1.7668 - val_mean_absolute_error: 1.7668\n",
      "Epoch 236/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.6643 - mean_absolute_error: 0.6643 - val_loss: 1.7620 - val_mean_absolute_error: 1.7620\n",
      "Epoch 237/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.6686 - mean_absolute_error: 0.6686 - val_loss: 1.7608 - val_mean_absolute_error: 1.7608\n",
      "Epoch 238/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.6638 - mean_absolute_error: 0.6638 - val_loss: 1.7610 - val_mean_absolute_error: 1.7610\n",
      "Epoch 239/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.6592 - mean_absolute_error: 0.6592 - val_loss: 1.7688 - val_mean_absolute_error: 1.7688\n",
      "Epoch 240/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.6589 - mean_absolute_error: 0.6589 - val_loss: 1.7631 - val_mean_absolute_error: 1.7631\n",
      "Epoch 241/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.6500 - mean_absolute_error: 0.6500 - val_loss: 1.7595 - val_mean_absolute_error: 1.7595\n",
      "Epoch 242/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.6471 - mean_absolute_error: 0.6471 - val_loss: 1.7627 - val_mean_absolute_error: 1.7627\n",
      "Epoch 243/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.6433 - mean_absolute_error: 0.6433 - val_loss: 1.7586 - val_mean_absolute_error: 1.7586\n",
      "Epoch 244/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.6401 - mean_absolute_error: 0.6401 - val_loss: 1.7589 - val_mean_absolute_error: 1.7589\n",
      "Epoch 245/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.6362 - mean_absolute_error: 0.6362 - val_loss: 1.7593 - val_mean_absolute_error: 1.7593\n",
      "Epoch 246/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.6337 - mean_absolute_error: 0.6337 - val_loss: 1.7582 - val_mean_absolute_error: 1.7582\n",
      "Epoch 247/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.6331 - mean_absolute_error: 0.6331 - val_loss: 1.7579 - val_mean_absolute_error: 1.7579\n",
      "Epoch 248/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.6287 - mean_absolute_error: 0.6287 - val_loss: 1.7591 - val_mean_absolute_error: 1.7591\n",
      "Epoch 249/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.6295 - mean_absolute_error: 0.6295 - val_loss: 1.7543 - val_mean_absolute_error: 1.7543\n",
      "Epoch 250/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.6273 - mean_absolute_error: 0.6273 - val_loss: 1.7571 - val_mean_absolute_error: 1.7571\n",
      "Epoch 251/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.6221 - mean_absolute_error: 0.6221 - val_loss: 1.7557 - val_mean_absolute_error: 1.7557\n",
      "Epoch 252/1000\n",
      "17/17 [==============================] - 2s 138ms/step - loss: 0.6171 - mean_absolute_error: 0.6171 - val_loss: 1.7542 - val_mean_absolute_error: 1.7542\n",
      "Epoch 253/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.6156 - mean_absolute_error: 0.6156 - val_loss: 1.7570 - val_mean_absolute_error: 1.7570\n",
      "Epoch 254/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.6154 - mean_absolute_error: 0.6154 - val_loss: 1.7556 - val_mean_absolute_error: 1.7556\n",
      "Epoch 255/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.6113 - mean_absolute_error: 0.6113 - val_loss: 1.7545 - val_mean_absolute_error: 1.7545\n",
      "Epoch 256/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.6071 - mean_absolute_error: 0.6071 - val_loss: 1.7527 - val_mean_absolute_error: 1.7527\n",
      "Epoch 257/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.6072 - mean_absolute_error: 0.6072 - val_loss: 1.7535 - val_mean_absolute_error: 1.7535\n",
      "Epoch 258/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.6033 - mean_absolute_error: 0.6033 - val_loss: 1.7522 - val_mean_absolute_error: 1.7522\n",
      "Epoch 259/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.6002 - mean_absolute_error: 0.6002 - val_loss: 1.7521 - val_mean_absolute_error: 1.7521\n",
      "Epoch 260/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.5995 - mean_absolute_error: 0.5995 - val_loss: 1.7513 - val_mean_absolute_error: 1.7513\n",
      "Epoch 261/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.5963 - mean_absolute_error: 0.5963 - val_loss: 1.7524 - val_mean_absolute_error: 1.7524\n",
      "Epoch 262/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.5938 - mean_absolute_error: 0.5938 - val_loss: 1.7524 - val_mean_absolute_error: 1.7524\n",
      "Epoch 263/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.5932 - mean_absolute_error: 0.5932 - val_loss: 1.7510 - val_mean_absolute_error: 1.7510\n",
      "Epoch 264/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.5889 - mean_absolute_error: 0.5889 - val_loss: 1.7515 - val_mean_absolute_error: 1.7515\n",
      "Epoch 265/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.5862 - mean_absolute_error: 0.5862 - val_loss: 1.7528 - val_mean_absolute_error: 1.7528\n",
      "Epoch 266/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.5833 - mean_absolute_error: 0.5833 - val_loss: 1.7514 - val_mean_absolute_error: 1.7514\n",
      "Epoch 267/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.5827 - mean_absolute_error: 0.5827 - val_loss: 1.7527 - val_mean_absolute_error: 1.7527\n",
      "Epoch 268/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.5861 - mean_absolute_error: 0.5861 - val_loss: 1.7597 - val_mean_absolute_error: 1.7597\n",
      "Epoch 269/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.5835 - mean_absolute_error: 0.5835 - val_loss: 1.7539 - val_mean_absolute_error: 1.7539\n",
      "Epoch 270/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.5793 - mean_absolute_error: 0.5793 - val_loss: 1.7591 - val_mean_absolute_error: 1.7591\n",
      "Epoch 271/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.5766 - mean_absolute_error: 0.5766 - val_loss: 1.7529 - val_mean_absolute_error: 1.7529\n",
      "Epoch 272/1000\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.5723 - mean_absolute_error: 0.5723 - val_loss: 1.7535 - val_mean_absolute_error: 1.7535\n",
      "Epoch 273/1000\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.5689 - mean_absolute_error: 0.5689 - val_loss: 1.7506 - val_mean_absolute_error: 1.7506\n",
      "Epoch 274/1000\n",
      "17/17 [==============================] - 3s 148ms/step - loss: 0.5667 - mean_absolute_error: 0.5667 - val_loss: 1.7528 - val_mean_absolute_error: 1.7528\n",
      "Epoch 275/1000\n",
      "17/17 [==============================] - 2s 138ms/step - loss: 0.5674 - mean_absolute_error: 0.5674 - val_loss: 1.7573 - val_mean_absolute_error: 1.7573\n",
      "Epoch 276/1000\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.5636 - mean_absolute_error: 0.5636 - val_loss: 1.7555 - val_mean_absolute_error: 1.7555\n",
      "Epoch 277/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.5619 - mean_absolute_error: 0.5619 - val_loss: 1.7515 - val_mean_absolute_error: 1.7515\n",
      "Epoch 278/1000\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.5630 - mean_absolute_error: 0.5630 - val_loss: 1.7654 - val_mean_absolute_error: 1.7654\n",
      "Epoch 279/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.5617 - mean_absolute_error: 0.5617 - val_loss: 1.7550 - val_mean_absolute_error: 1.7550\n",
      "Epoch 280/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.5605 - mean_absolute_error: 0.5605 - val_loss: 1.7608 - val_mean_absolute_error: 1.7608\n",
      "Epoch 281/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.5569 - mean_absolute_error: 0.5569 - val_loss: 1.7545 - val_mean_absolute_error: 1.7545\n",
      "Epoch 282/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.5533 - mean_absolute_error: 0.5533 - val_loss: 1.7580 - val_mean_absolute_error: 1.7580\n",
      "Epoch 283/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.5496 - mean_absolute_error: 0.5496 - val_loss: 1.7561 - val_mean_absolute_error: 1.7561\n",
      "Epoch 284/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.5477 - mean_absolute_error: 0.5477 - val_loss: 1.7594 - val_mean_absolute_error: 1.7594\n",
      "Epoch 285/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.5467 - mean_absolute_error: 0.5467 - val_loss: 1.7554 - val_mean_absolute_error: 1.7554\n",
      "Epoch 286/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.5437 - mean_absolute_error: 0.5437 - val_loss: 1.7542 - val_mean_absolute_error: 1.7542\n",
      "Epoch 287/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.5431 - mean_absolute_error: 0.5431 - val_loss: 1.7563 - val_mean_absolute_error: 1.7563\n",
      "Epoch 288/1000\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 0.5409 - mean_absolute_error: 0.5409 - val_loss: 1.7568 - val_mean_absolute_error: 1.7568\n",
      "Epoch 289/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.5411 - mean_absolute_error: 0.5411 - val_loss: 1.7556 - val_mean_absolute_error: 1.7556\n",
      "Epoch 290/1000\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.5363 - mean_absolute_error: 0.5363 - val_loss: 1.7573 - val_mean_absolute_error: 1.7573\n",
      "Epoch 291/1000\n",
      "17/17 [==============================] - 2s 147ms/step - loss: 0.5367 - mean_absolute_error: 0.5367 - val_loss: 1.7559 - val_mean_absolute_error: 1.7559\n",
      "Epoch 292/1000\n",
      "17/17 [==============================] - 2s 140ms/step - loss: 0.5340 - mean_absolute_error: 0.5340 - val_loss: 1.7580 - val_mean_absolute_error: 1.7580\n",
      "Epoch 293/1000\n",
      "17/17 [==============================] - 3s 160ms/step - loss: 0.5318 - mean_absolute_error: 0.5318 - val_loss: 1.7581 - val_mean_absolute_error: 1.7581\n",
      "Epoch 294/1000\n",
      "17/17 [==============================] - 3s 185ms/step - loss: 0.5301 - mean_absolute_error: 0.5301 - val_loss: 1.7567 - val_mean_absolute_error: 1.7567\n",
      "Epoch 295/1000\n",
      "17/17 [==============================] - 3s 185ms/step - loss: 0.5270 - mean_absolute_error: 0.5270 - val_loss: 1.7558 - val_mean_absolute_error: 1.7558\n",
      "Epoch 296/1000\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 0.5272 - mean_absolute_error: 0.5272 - val_loss: 1.7571 - val_mean_absolute_error: 1.7571\n",
      "Epoch 297/1000\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.5248 - mean_absolute_error: 0.5248 - val_loss: 1.7584 - val_mean_absolute_error: 1.7584\n",
      "Epoch 298/1000\n",
      "17/17 [==============================] - 2s 137ms/step - loss: 0.5231 - mean_absolute_error: 0.5231 - val_loss: 1.7562 - val_mean_absolute_error: 1.7562\n",
      "Epoch 299/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.5198 - mean_absolute_error: 0.5198 - val_loss: 1.7598 - val_mean_absolute_error: 1.7598\n",
      "Epoch 300/1000\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 0.5227 - mean_absolute_error: 0.5227 - val_loss: 1.7586 - val_mean_absolute_error: 1.7586\n",
      "Epoch 301/1000\n",
      "17/17 [==============================] - 3s 171ms/step - loss: 0.5224 - mean_absolute_error: 0.5224 - val_loss: 1.7568 - val_mean_absolute_error: 1.7568\n",
      "Epoch 302/1000\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 0.5167 - mean_absolute_error: 0.5167 - val_loss: 1.7553 - val_mean_absolute_error: 1.7553\n",
      "Epoch 303/1000\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 0.5145 - mean_absolute_error: 0.5145 - val_loss: 1.7552 - val_mean_absolute_error: 1.7552\n",
      "Epoch 304/1000\n",
      "17/17 [==============================] - 3s 161ms/step - loss: 0.5125 - mean_absolute_error: 0.5125 - val_loss: 1.7551 - val_mean_absolute_error: 1.7551\n",
      "Epoch 305/1000\n",
      "17/17 [==============================] - 3s 158ms/step - loss: 0.5115 - mean_absolute_error: 0.5115 - val_loss: 1.7550 - val_mean_absolute_error: 1.7550\n",
      "Epoch 306/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.5098 - mean_absolute_error: 0.5098 - val_loss: 1.7527 - val_mean_absolute_error: 1.7527\n",
      "Epoch 307/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.5136 - mean_absolute_error: 0.5136 - val_loss: 1.7596 - val_mean_absolute_error: 1.7596\n",
      "Epoch 308/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.5101 - mean_absolute_error: 0.5101 - val_loss: 1.7530 - val_mean_absolute_error: 1.7530\n",
      "Epoch 309/1000\n",
      "17/17 [==============================] - 3s 158ms/step - loss: 0.5074 - mean_absolute_error: 0.5074 - val_loss: 1.7565 - val_mean_absolute_error: 1.7565\n",
      "Epoch 310/1000\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 0.5074 - mean_absolute_error: 0.5074 - val_loss: 1.7528 - val_mean_absolute_error: 1.7528\n",
      "Epoch 311/1000\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 0.5117 - mean_absolute_error: 0.5117 - val_loss: 1.7542 - val_mean_absolute_error: 1.7542\n",
      "Epoch 312/1000\n",
      "17/17 [==============================] - 3s 148ms/step - loss: 0.5104 - mean_absolute_error: 0.5104 - val_loss: 1.7539 - val_mean_absolute_error: 1.7539\n",
      "Epoch 313/1000\n",
      "17/17 [==============================] - 3s 147ms/step - loss: 0.5127 - mean_absolute_error: 0.5127 - val_loss: 1.7546 - val_mean_absolute_error: 1.7546\n",
      "Epoch 314/1000\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 0.5038 - mean_absolute_error: 0.5038 - val_loss: 1.7570 - val_mean_absolute_error: 1.7570\n",
      "Epoch 315/1000\n",
      "17/17 [==============================] - 2s 140ms/step - loss: 0.5004 - mean_absolute_error: 0.5004 - val_loss: 1.7504 - val_mean_absolute_error: 1.7504\n",
      "Epoch 316/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.5012 - mean_absolute_error: 0.5012 - val_loss: 1.7497 - val_mean_absolute_error: 1.7497\n",
      "Epoch 317/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.5006 - mean_absolute_error: 0.5006 - val_loss: 1.7567 - val_mean_absolute_error: 1.7567\n",
      "Epoch 318/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.5018 - mean_absolute_error: 0.5018 - val_loss: 1.7590 - val_mean_absolute_error: 1.7590\n",
      "Epoch 319/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.4947 - mean_absolute_error: 0.4947 - val_loss: 1.7531 - val_mean_absolute_error: 1.7531\n",
      "Epoch 320/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.4917 - mean_absolute_error: 0.4917 - val_loss: 1.7527 - val_mean_absolute_error: 1.7527\n",
      "Epoch 321/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.4919 - mean_absolute_error: 0.4919 - val_loss: 1.7531 - val_mean_absolute_error: 1.7531\n",
      "Epoch 322/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.4923 - mean_absolute_error: 0.4923 - val_loss: 1.7551 - val_mean_absolute_error: 1.7551\n",
      "Epoch 323/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.4917 - mean_absolute_error: 0.4917 - val_loss: 1.7512 - val_mean_absolute_error: 1.7512\n",
      "Epoch 324/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.4871 - mean_absolute_error: 0.4871 - val_loss: 1.7507 - val_mean_absolute_error: 1.7507\n",
      "Epoch 325/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.4848 - mean_absolute_error: 0.4848 - val_loss: 1.7487 - val_mean_absolute_error: 1.7487\n",
      "Epoch 326/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.4833 - mean_absolute_error: 0.4833 - val_loss: 1.7493 - val_mean_absolute_error: 1.7493\n",
      "Epoch 327/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.4835 - mean_absolute_error: 0.4835 - val_loss: 1.7483 - val_mean_absolute_error: 1.7483\n",
      "Epoch 328/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.4839 - mean_absolute_error: 0.4839 - val_loss: 1.7487 - val_mean_absolute_error: 1.7487\n",
      "Epoch 329/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.4841 - mean_absolute_error: 0.4841 - val_loss: 1.7493 - val_mean_absolute_error: 1.7493\n",
      "Epoch 330/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.4819 - mean_absolute_error: 0.4819 - val_loss: 1.7497 - val_mean_absolute_error: 1.7497\n",
      "Epoch 331/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.4808 - mean_absolute_error: 0.4808 - val_loss: 1.7473 - val_mean_absolute_error: 1.7473\n",
      "Epoch 332/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.4813 - mean_absolute_error: 0.4813 - val_loss: 1.7507 - val_mean_absolute_error: 1.7507\n",
      "Epoch 333/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.4771 - mean_absolute_error: 0.4771 - val_loss: 1.7487 - val_mean_absolute_error: 1.7487\n",
      "Epoch 334/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.4771 - mean_absolute_error: 0.4771 - val_loss: 1.7493 - val_mean_absolute_error: 1.7493\n",
      "Epoch 335/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.4749 - mean_absolute_error: 0.4749 - val_loss: 1.7483 - val_mean_absolute_error: 1.7483\n",
      "Epoch 336/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.4769 - mean_absolute_error: 0.4769 - val_loss: 1.7506 - val_mean_absolute_error: 1.7506\n",
      "Epoch 337/1000\n",
      "17/17 [==============================] - 2s 137ms/step - loss: 0.4797 - mean_absolute_error: 0.4797 - val_loss: 1.7470 - val_mean_absolute_error: 1.7470\n",
      "Epoch 338/1000\n",
      "17/17 [==============================] - 3s 168ms/step - loss: 0.4748 - mean_absolute_error: 0.4748 - val_loss: 1.7490 - val_mean_absolute_error: 1.7490\n",
      "Epoch 339/1000\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 0.4704 - mean_absolute_error: 0.4704 - val_loss: 1.7464 - val_mean_absolute_error: 1.7464\n",
      "Epoch 340/1000\n",
      "17/17 [==============================] - 2s 137ms/step - loss: 0.4700 - mean_absolute_error: 0.4700 - val_loss: 1.7469 - val_mean_absolute_error: 1.7469\n",
      "Epoch 341/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.4690 - mean_absolute_error: 0.4690 - val_loss: 1.7456 - val_mean_absolute_error: 1.7456\n",
      "Epoch 342/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.4666 - mean_absolute_error: 0.4666 - val_loss: 1.7443 - val_mean_absolute_error: 1.7443\n",
      "Epoch 343/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.4672 - mean_absolute_error: 0.4672 - val_loss: 1.7477 - val_mean_absolute_error: 1.7477\n",
      "Epoch 344/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.4683 - mean_absolute_error: 0.4683 - val_loss: 1.7471 - val_mean_absolute_error: 1.7471\n",
      "Epoch 345/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.4644 - mean_absolute_error: 0.4644 - val_loss: 1.7447 - val_mean_absolute_error: 1.7447\n",
      "Epoch 346/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.4671 - mean_absolute_error: 0.4671 - val_loss: 1.7454 - val_mean_absolute_error: 1.7454\n",
      "Epoch 347/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.4640 - mean_absolute_error: 0.4640 - val_loss: 1.7445 - val_mean_absolute_error: 1.7445\n",
      "Epoch 348/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.4614 - mean_absolute_error: 0.4614 - val_loss: 1.7413 - val_mean_absolute_error: 1.7413\n",
      "Epoch 349/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.4616 - mean_absolute_error: 0.4616 - val_loss: 1.7401 - val_mean_absolute_error: 1.7401\n",
      "Epoch 350/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.4608 - mean_absolute_error: 0.4608 - val_loss: 1.7399 - val_mean_absolute_error: 1.7399\n",
      "Epoch 351/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.4577 - mean_absolute_error: 0.4577 - val_loss: 1.7414 - val_mean_absolute_error: 1.7414\n",
      "Epoch 352/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.4591 - mean_absolute_error: 0.4591 - val_loss: 1.7405 - val_mean_absolute_error: 1.7405\n",
      "Epoch 353/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.4586 - mean_absolute_error: 0.4586 - val_loss: 1.7478 - val_mean_absolute_error: 1.7478\n",
      "Epoch 354/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.4576 - mean_absolute_error: 0.4576 - val_loss: 1.7447 - val_mean_absolute_error: 1.7447\n",
      "Epoch 355/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.4550 - mean_absolute_error: 0.4550 - val_loss: 1.7408 - val_mean_absolute_error: 1.7408\n",
      "Epoch 356/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.4527 - mean_absolute_error: 0.4527 - val_loss: 1.7404 - val_mean_absolute_error: 1.7404\n",
      "Epoch 357/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.4512 - mean_absolute_error: 0.4512 - val_loss: 1.7427 - val_mean_absolute_error: 1.7427\n",
      "Epoch 358/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.4510 - mean_absolute_error: 0.4510 - val_loss: 1.7408 - val_mean_absolute_error: 1.7408\n",
      "Epoch 359/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.4510 - mean_absolute_error: 0.4510 - val_loss: 1.7408 - val_mean_absolute_error: 1.7408\n",
      "Epoch 360/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.4487 - mean_absolute_error: 0.4487 - val_loss: 1.7394 - val_mean_absolute_error: 1.7394\n",
      "Epoch 361/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.4497 - mean_absolute_error: 0.4497 - val_loss: 1.7402 - val_mean_absolute_error: 1.7402\n",
      "Epoch 362/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.4482 - mean_absolute_error: 0.4482 - val_loss: 1.7423 - val_mean_absolute_error: 1.7423\n",
      "Epoch 363/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.4508 - mean_absolute_error: 0.4508 - val_loss: 1.7403 - val_mean_absolute_error: 1.7403\n",
      "Epoch 364/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.4491 - mean_absolute_error: 0.4491 - val_loss: 1.7434 - val_mean_absolute_error: 1.7434\n",
      "Epoch 365/1000\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.4472 - mean_absolute_error: 0.4472 - val_loss: 1.7402 - val_mean_absolute_error: 1.7402\n",
      "Epoch 366/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.4440 - mean_absolute_error: 0.4440 - val_loss: 1.7407 - val_mean_absolute_error: 1.7407\n",
      "Epoch 367/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.4435 - mean_absolute_error: 0.4435 - val_loss: 1.7454 - val_mean_absolute_error: 1.7454\n",
      "Epoch 368/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.4431 - mean_absolute_error: 0.4431 - val_loss: 1.7413 - val_mean_absolute_error: 1.7413\n",
      "Epoch 369/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.4438 - mean_absolute_error: 0.4438 - val_loss: 1.7399 - val_mean_absolute_error: 1.7399\n",
      "Epoch 370/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.4412 - mean_absolute_error: 0.4412 - val_loss: 1.7388 - val_mean_absolute_error: 1.7388\n",
      "Epoch 371/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.4393 - mean_absolute_error: 0.4393 - val_loss: 1.7412 - val_mean_absolute_error: 1.7412\n",
      "Epoch 372/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.4399 - mean_absolute_error: 0.4399 - val_loss: 1.7431 - val_mean_absolute_error: 1.7431\n",
      "Epoch 373/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.4427 - mean_absolute_error: 0.4427 - val_loss: 1.7416 - val_mean_absolute_error: 1.7416\n",
      "Epoch 374/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.4406 - mean_absolute_error: 0.4406 - val_loss: 1.7361 - val_mean_absolute_error: 1.7361\n",
      "Epoch 375/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.4420 - mean_absolute_error: 0.4420 - val_loss: 1.7422 - val_mean_absolute_error: 1.7422\n",
      "Epoch 376/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.4392 - mean_absolute_error: 0.4392 - val_loss: 1.7387 - val_mean_absolute_error: 1.7387\n",
      "Epoch 377/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.4343 - mean_absolute_error: 0.4343 - val_loss: 1.7388 - val_mean_absolute_error: 1.7388\n",
      "Epoch 378/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.4329 - mean_absolute_error: 0.4329 - val_loss: 1.7373 - val_mean_absolute_error: 1.7373\n",
      "Epoch 379/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.4308 - mean_absolute_error: 0.4308 - val_loss: 1.7352 - val_mean_absolute_error: 1.7352\n",
      "Epoch 380/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.4330 - mean_absolute_error: 0.4330 - val_loss: 1.7391 - val_mean_absolute_error: 1.7391\n",
      "Epoch 381/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.4311 - mean_absolute_error: 0.4311 - val_loss: 1.7392 - val_mean_absolute_error: 1.7392\n",
      "Epoch 382/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.4285 - mean_absolute_error: 0.4285 - val_loss: 1.7377 - val_mean_absolute_error: 1.7377\n",
      "Epoch 383/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.4285 - mean_absolute_error: 0.4285 - val_loss: 1.7367 - val_mean_absolute_error: 1.7367\n",
      "Epoch 384/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.4277 - mean_absolute_error: 0.4277 - val_loss: 1.7419 - val_mean_absolute_error: 1.7419\n",
      "Epoch 385/1000\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 0.4312 - mean_absolute_error: 0.4312 - val_loss: 1.7392 - val_mean_absolute_error: 1.7392\n",
      "Epoch 386/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.4287 - mean_absolute_error: 0.4287 - val_loss: 1.7377 - val_mean_absolute_error: 1.7377\n",
      "Epoch 387/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.4249 - mean_absolute_error: 0.4249 - val_loss: 1.7369 - val_mean_absolute_error: 1.7369\n",
      "Epoch 388/1000\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.4263 - mean_absolute_error: 0.4263 - val_loss: 1.7351 - val_mean_absolute_error: 1.7351\n",
      "Epoch 389/1000\n",
      "17/17 [==============================] - 2s 141ms/step - loss: 0.4258 - mean_absolute_error: 0.4258 - val_loss: 1.7341 - val_mean_absolute_error: 1.7341\n",
      "Epoch 390/1000\n",
      "17/17 [==============================] - 2s 147ms/step - loss: 0.4277 - mean_absolute_error: 0.4277 - val_loss: 1.7410 - val_mean_absolute_error: 1.7410\n",
      "Epoch 391/1000\n",
      "17/17 [==============================] - 3s 145ms/step - loss: 0.4271 - mean_absolute_error: 0.4271 - val_loss: 1.7342 - val_mean_absolute_error: 1.7342\n",
      "Epoch 392/1000\n",
      "17/17 [==============================] - 3s 158ms/step - loss: 0.4237 - mean_absolute_error: 0.4237 - val_loss: 1.7371 - val_mean_absolute_error: 1.7371\n",
      "Epoch 393/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.4206 - mean_absolute_error: 0.4206 - val_loss: 1.7349 - val_mean_absolute_error: 1.7349\n",
      "Epoch 394/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.4186 - mean_absolute_error: 0.4186 - val_loss: 1.7368 - val_mean_absolute_error: 1.7368\n",
      "Epoch 395/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.4177 - mean_absolute_error: 0.4177 - val_loss: 1.7349 - val_mean_absolute_error: 1.7349\n",
      "Epoch 396/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.4177 - mean_absolute_error: 0.4177 - val_loss: 1.7370 - val_mean_absolute_error: 1.7370\n",
      "Epoch 397/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.4176 - mean_absolute_error: 0.4176 - val_loss: 1.7356 - val_mean_absolute_error: 1.7356\n",
      "Epoch 398/1000\n",
      "17/17 [==============================] - 2s 138ms/step - loss: 0.4188 - mean_absolute_error: 0.4188 - val_loss: 1.7316 - val_mean_absolute_error: 1.7316\n",
      "Epoch 399/1000\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.4192 - mean_absolute_error: 0.4192 - val_loss: 1.7368 - val_mean_absolute_error: 1.7368\n",
      "Epoch 400/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.4164 - mean_absolute_error: 0.4164 - val_loss: 1.7348 - val_mean_absolute_error: 1.7348\n",
      "Epoch 401/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.4152 - mean_absolute_error: 0.4152 - val_loss: 1.7354 - val_mean_absolute_error: 1.7354\n",
      "Epoch 402/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.4146 - mean_absolute_error: 0.4146 - val_loss: 1.7348 - val_mean_absolute_error: 1.7348\n",
      "Epoch 403/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.4129 - mean_absolute_error: 0.4129 - val_loss: 1.7329 - val_mean_absolute_error: 1.7329\n",
      "Epoch 404/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.4115 - mean_absolute_error: 0.4115 - val_loss: 1.7364 - val_mean_absolute_error: 1.7364\n",
      "Epoch 405/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.4103 - mean_absolute_error: 0.4103 - val_loss: 1.7338 - val_mean_absolute_error: 1.7338\n",
      "Epoch 406/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.4122 - mean_absolute_error: 0.4122 - val_loss: 1.7337 - val_mean_absolute_error: 1.7337\n",
      "Epoch 407/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.4104 - mean_absolute_error: 0.4104 - val_loss: 1.7384 - val_mean_absolute_error: 1.7384\n",
      "Epoch 408/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.4111 - mean_absolute_error: 0.4111 - val_loss: 1.7355 - val_mean_absolute_error: 1.7355\n",
      "Epoch 409/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.4096 - mean_absolute_error: 0.4096 - val_loss: 1.7342 - val_mean_absolute_error: 1.7342\n",
      "Epoch 410/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.4078 - mean_absolute_error: 0.4078 - val_loss: 1.7323 - val_mean_absolute_error: 1.7323\n",
      "Epoch 411/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.4113 - mean_absolute_error: 0.4113 - val_loss: 1.7358 - val_mean_absolute_error: 1.7358\n",
      "Epoch 412/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.4088 - mean_absolute_error: 0.4088 - val_loss: 1.7333 - val_mean_absolute_error: 1.7333\n",
      "Epoch 413/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.4070 - mean_absolute_error: 0.4070 - val_loss: 1.7360 - val_mean_absolute_error: 1.7360\n",
      "Epoch 414/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.4052 - mean_absolute_error: 0.4052 - val_loss: 1.7355 - val_mean_absolute_error: 1.7355\n",
      "Epoch 415/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.4040 - mean_absolute_error: 0.4040 - val_loss: 1.7359 - val_mean_absolute_error: 1.7359\n",
      "Epoch 416/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.4028 - mean_absolute_error: 0.4028 - val_loss: 1.7351 - val_mean_absolute_error: 1.7351\n",
      "Epoch 417/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.4031 - mean_absolute_error: 0.4031 - val_loss: 1.7332 - val_mean_absolute_error: 1.7332\n",
      "Epoch 418/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.4047 - mean_absolute_error: 0.4047 - val_loss: 1.7346 - val_mean_absolute_error: 1.7346\n",
      "Epoch 419/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.4020 - mean_absolute_error: 0.4020 - val_loss: 1.7337 - val_mean_absolute_error: 1.7337\n",
      "Epoch 420/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.4016 - mean_absolute_error: 0.4016 - val_loss: 1.7404 - val_mean_absolute_error: 1.7404\n",
      "Epoch 421/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.4024 - mean_absolute_error: 0.4024 - val_loss: 1.7335 - val_mean_absolute_error: 1.7335\n",
      "Epoch 422/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.4003 - mean_absolute_error: 0.4003 - val_loss: 1.7372 - val_mean_absolute_error: 1.7372\n",
      "Epoch 423/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.3994 - mean_absolute_error: 0.3994 - val_loss: 1.7356 - val_mean_absolute_error: 1.7356\n",
      "Epoch 424/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.3990 - mean_absolute_error: 0.3990 - val_loss: 1.7316 - val_mean_absolute_error: 1.7316\n",
      "Epoch 425/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.4022 - mean_absolute_error: 0.4022 - val_loss: 1.7326 - val_mean_absolute_error: 1.7326\n",
      "Epoch 426/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.4023 - mean_absolute_error: 0.4023 - val_loss: 1.7446 - val_mean_absolute_error: 1.7446\n",
      "Epoch 427/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.4044 - mean_absolute_error: 0.4044 - val_loss: 1.7319 - val_mean_absolute_error: 1.7319\n",
      "Epoch 428/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.4014 - mean_absolute_error: 0.4014 - val_loss: 1.7309 - val_mean_absolute_error: 1.7309\n",
      "Epoch 429/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.3991 - mean_absolute_error: 0.3991 - val_loss: 1.7350 - val_mean_absolute_error: 1.7350\n",
      "Epoch 430/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.3996 - mean_absolute_error: 0.3996 - val_loss: 1.7337 - val_mean_absolute_error: 1.7337\n",
      "Epoch 431/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.3974 - mean_absolute_error: 0.3974 - val_loss: 1.7326 - val_mean_absolute_error: 1.7326\n",
      "Epoch 432/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.3967 - mean_absolute_error: 0.3967 - val_loss: 1.7306 - val_mean_absolute_error: 1.7306\n",
      "Epoch 433/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.3943 - mean_absolute_error: 0.3943 - val_loss: 1.7315 - val_mean_absolute_error: 1.7315\n",
      "Epoch 434/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.3970 - mean_absolute_error: 0.3970 - val_loss: 1.7315 - val_mean_absolute_error: 1.7315\n",
      "Epoch 435/1000\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.3980 - mean_absolute_error: 0.3980 - val_loss: 1.7343 - val_mean_absolute_error: 1.7343\n",
      "Epoch 436/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.3941 - mean_absolute_error: 0.3941 - val_loss: 1.7316 - val_mean_absolute_error: 1.7316\n",
      "Epoch 437/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3912 - mean_absolute_error: 0.3912 - val_loss: 1.7340 - val_mean_absolute_error: 1.7340\n",
      "Epoch 438/1000\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.3895 - mean_absolute_error: 0.3895 - val_loss: 1.7318 - val_mean_absolute_error: 1.7318\n",
      "Epoch 439/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.3911 - mean_absolute_error: 0.3911 - val_loss: 1.7312 - val_mean_absolute_error: 1.7312\n",
      "Epoch 440/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.3910 - mean_absolute_error: 0.3910 - val_loss: 1.7330 - val_mean_absolute_error: 1.7330\n",
      "Epoch 441/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.3882 - mean_absolute_error: 0.3882 - val_loss: 1.7333 - val_mean_absolute_error: 1.7333\n",
      "Epoch 442/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.3871 - mean_absolute_error: 0.3871 - val_loss: 1.7304 - val_mean_absolute_error: 1.7304\n",
      "Epoch 443/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.3872 - mean_absolute_error: 0.3872 - val_loss: 1.7312 - val_mean_absolute_error: 1.7312\n",
      "Epoch 444/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3888 - mean_absolute_error: 0.3888 - val_loss: 1.7329 - val_mean_absolute_error: 1.7329\n",
      "Epoch 445/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.3880 - mean_absolute_error: 0.3880 - val_loss: 1.7380 - val_mean_absolute_error: 1.7380\n",
      "Epoch 446/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3883 - mean_absolute_error: 0.3883 - val_loss: 1.7349 - val_mean_absolute_error: 1.7349\n",
      "Epoch 447/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3848 - mean_absolute_error: 0.3848 - val_loss: 1.7340 - val_mean_absolute_error: 1.7340\n",
      "Epoch 448/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.3832 - mean_absolute_error: 0.3832 - val_loss: 1.7344 - val_mean_absolute_error: 1.7344\n",
      "Epoch 449/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.3830 - mean_absolute_error: 0.3830 - val_loss: 1.7338 - val_mean_absolute_error: 1.7338\n",
      "Epoch 450/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3830 - mean_absolute_error: 0.3830 - val_loss: 1.7361 - val_mean_absolute_error: 1.7361\n",
      "Epoch 451/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.3842 - mean_absolute_error: 0.3842 - val_loss: 1.7344 - val_mean_absolute_error: 1.7344\n",
      "Epoch 452/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.3822 - mean_absolute_error: 0.3822 - val_loss: 1.7334 - val_mean_absolute_error: 1.7334\n",
      "Epoch 453/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.3838 - mean_absolute_error: 0.3838 - val_loss: 1.7339 - val_mean_absolute_error: 1.7339\n",
      "Epoch 454/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.3837 - mean_absolute_error: 0.3837 - val_loss: 1.7311 - val_mean_absolute_error: 1.7311\n",
      "Epoch 455/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.3822 - mean_absolute_error: 0.3822 - val_loss: 1.7320 - val_mean_absolute_error: 1.7320\n",
      "Epoch 456/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.3813 - mean_absolute_error: 0.3813 - val_loss: 1.7330 - val_mean_absolute_error: 1.7330\n",
      "Epoch 457/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3806 - mean_absolute_error: 0.3806 - val_loss: 1.7320 - val_mean_absolute_error: 1.7320\n",
      "Epoch 458/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3795 - mean_absolute_error: 0.3795 - val_loss: 1.7333 - val_mean_absolute_error: 1.7333\n",
      "Epoch 459/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.3780 - mean_absolute_error: 0.3780 - val_loss: 1.7336 - val_mean_absolute_error: 1.7336\n",
      "Epoch 460/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.3776 - mean_absolute_error: 0.3776 - val_loss: 1.7352 - val_mean_absolute_error: 1.7352\n",
      "Epoch 461/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.3777 - mean_absolute_error: 0.3777 - val_loss: 1.7377 - val_mean_absolute_error: 1.7377\n",
      "Epoch 462/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.3769 - mean_absolute_error: 0.3769 - val_loss: 1.7321 - val_mean_absolute_error: 1.7321\n",
      "Epoch 463/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3778 - mean_absolute_error: 0.3778 - val_loss: 1.7307 - val_mean_absolute_error: 1.7307\n",
      "Epoch 464/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3769 - mean_absolute_error: 0.3769 - val_loss: 1.7320 - val_mean_absolute_error: 1.7320\n",
      "Epoch 465/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3836 - mean_absolute_error: 0.3836 - val_loss: 1.7387 - val_mean_absolute_error: 1.7387\n",
      "Epoch 466/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3850 - mean_absolute_error: 0.3850 - val_loss: 1.7331 - val_mean_absolute_error: 1.7331\n",
      "Epoch 467/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3817 - mean_absolute_error: 0.3817 - val_loss: 1.7323 - val_mean_absolute_error: 1.7323\n",
      "Epoch 468/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3761 - mean_absolute_error: 0.3761 - val_loss: 1.7340 - val_mean_absolute_error: 1.7340\n",
      "Epoch 469/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.3736 - mean_absolute_error: 0.3736 - val_loss: 1.7358 - val_mean_absolute_error: 1.7358\n",
      "Epoch 470/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3714 - mean_absolute_error: 0.3714 - val_loss: 1.7338 - val_mean_absolute_error: 1.7338\n",
      "Epoch 471/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.3730 - mean_absolute_error: 0.3730 - val_loss: 1.7323 - val_mean_absolute_error: 1.7323\n",
      "Epoch 472/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.3720 - mean_absolute_error: 0.3720 - val_loss: 1.7363 - val_mean_absolute_error: 1.7363\n",
      "Epoch 473/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.3705 - mean_absolute_error: 0.3705 - val_loss: 1.7351 - val_mean_absolute_error: 1.7351\n",
      "Epoch 474/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.3690 - mean_absolute_error: 0.3690 - val_loss: 1.7346 - val_mean_absolute_error: 1.7346\n",
      "Epoch 475/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.3686 - mean_absolute_error: 0.3686 - val_loss: 1.7328 - val_mean_absolute_error: 1.7328\n",
      "Epoch 476/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.3698 - mean_absolute_error: 0.3698 - val_loss: 1.7350 - val_mean_absolute_error: 1.7350\n",
      "Epoch 477/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.3712 - mean_absolute_error: 0.3712 - val_loss: 1.7341 - val_mean_absolute_error: 1.7341\n",
      "Epoch 478/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.3712 - mean_absolute_error: 0.3712 - val_loss: 1.7331 - val_mean_absolute_error: 1.7331\n",
      "Epoch 479/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.3680 - mean_absolute_error: 0.3680 - val_loss: 1.7328 - val_mean_absolute_error: 1.7328\n",
      "Epoch 480/1000\n",
      "17/17 [==============================] - 2s 140ms/step - loss: 0.3685 - mean_absolute_error: 0.3685 - val_loss: 1.7323 - val_mean_absolute_error: 1.7323\n",
      "Epoch 481/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.3671 - mean_absolute_error: 0.3671 - val_loss: 1.7350 - val_mean_absolute_error: 1.7350\n",
      "Epoch 482/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.3665 - mean_absolute_error: 0.3665 - val_loss: 1.7358 - val_mean_absolute_error: 1.7358\n",
      "Epoch 483/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.3690 - mean_absolute_error: 0.3690 - val_loss: 1.7402 - val_mean_absolute_error: 1.7402\n",
      "Epoch 484/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.3688 - mean_absolute_error: 0.3688 - val_loss: 1.7352 - val_mean_absolute_error: 1.7352\n",
      "Epoch 485/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.3694 - mean_absolute_error: 0.3694 - val_loss: 1.7325 - val_mean_absolute_error: 1.7325\n",
      "Epoch 486/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3659 - mean_absolute_error: 0.3659 - val_loss: 1.7315 - val_mean_absolute_error: 1.7315\n",
      "Epoch 487/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3693 - mean_absolute_error: 0.3693 - val_loss: 1.7380 - val_mean_absolute_error: 1.7380\n",
      "Epoch 488/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.3668 - mean_absolute_error: 0.3668 - val_loss: 1.7338 - val_mean_absolute_error: 1.7338\n",
      "Epoch 489/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.3622 - mean_absolute_error: 0.3622 - val_loss: 1.7341 - val_mean_absolute_error: 1.7341\n",
      "Epoch 490/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.3625 - mean_absolute_error: 0.3625 - val_loss: 1.7332 - val_mean_absolute_error: 1.7332\n",
      "Epoch 491/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.3623 - mean_absolute_error: 0.3623 - val_loss: 1.7338 - val_mean_absolute_error: 1.7338\n",
      "Epoch 492/1000\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.3591 - mean_absolute_error: 0.3591 - val_loss: 1.7351 - val_mean_absolute_error: 1.7351\n",
      "Epoch 493/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.3598 - mean_absolute_error: 0.3598 - val_loss: 1.7367 - val_mean_absolute_error: 1.7367\n",
      "Epoch 494/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3581 - mean_absolute_error: 0.3581 - val_loss: 1.7328 - val_mean_absolute_error: 1.7328\n",
      "Epoch 495/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3585 - mean_absolute_error: 0.3585 - val_loss: 1.7324 - val_mean_absolute_error: 1.7324\n",
      "Epoch 496/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3568 - mean_absolute_error: 0.3568 - val_loss: 1.7319 - val_mean_absolute_error: 1.7319\n",
      "Epoch 497/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3585 - mean_absolute_error: 0.3585 - val_loss: 1.7335 - val_mean_absolute_error: 1.7335\n",
      "Epoch 498/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3562 - mean_absolute_error: 0.3562 - val_loss: 1.7355 - val_mean_absolute_error: 1.7355\n",
      "Epoch 499/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3563 - mean_absolute_error: 0.3563 - val_loss: 1.7358 - val_mean_absolute_error: 1.7358\n",
      "Epoch 500/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3557 - mean_absolute_error: 0.3557 - val_loss: 1.7360 - val_mean_absolute_error: 1.7360\n",
      "Epoch 501/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3552 - mean_absolute_error: 0.3552 - val_loss: 1.7361 - val_mean_absolute_error: 1.7361\n",
      "Epoch 502/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3549 - mean_absolute_error: 0.3549 - val_loss: 1.7344 - val_mean_absolute_error: 1.7344\n",
      "Epoch 503/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3546 - mean_absolute_error: 0.3546 - val_loss: 1.7363 - val_mean_absolute_error: 1.7363\n",
      "Epoch 504/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3521 - mean_absolute_error: 0.3521 - val_loss: 1.7329 - val_mean_absolute_error: 1.7329\n",
      "Epoch 505/1000\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.3514 - mean_absolute_error: 0.3514 - val_loss: 1.7317 - val_mean_absolute_error: 1.7317\n",
      "Epoch 506/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3513 - mean_absolute_error: 0.3513 - val_loss: 1.7356 - val_mean_absolute_error: 1.7356\n",
      "Epoch 507/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3588 - mean_absolute_error: 0.3588 - val_loss: 1.7342 - val_mean_absolute_error: 1.7342\n",
      "Epoch 508/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3566 - mean_absolute_error: 0.3566 - val_loss: 1.7349 - val_mean_absolute_error: 1.7349\n",
      "Epoch 509/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3523 - mean_absolute_error: 0.3523 - val_loss: 1.7317 - val_mean_absolute_error: 1.7317\n",
      "Epoch 510/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3499 - mean_absolute_error: 0.3499 - val_loss: 1.7324 - val_mean_absolute_error: 1.7324\n",
      "Epoch 511/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3510 - mean_absolute_error: 0.3510 - val_loss: 1.7329 - val_mean_absolute_error: 1.7329\n",
      "Epoch 512/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.3499 - mean_absolute_error: 0.3499 - val_loss: 1.7330 - val_mean_absolute_error: 1.7330\n",
      "Epoch 513/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3506 - mean_absolute_error: 0.3506 - val_loss: 1.7340 - val_mean_absolute_error: 1.7340\n",
      "Epoch 514/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3472 - mean_absolute_error: 0.3472 - val_loss: 1.7325 - val_mean_absolute_error: 1.7325\n",
      "Epoch 515/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3477 - mean_absolute_error: 0.3477 - val_loss: 1.7342 - val_mean_absolute_error: 1.7342\n",
      "Epoch 516/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3448 - mean_absolute_error: 0.3448 - val_loss: 1.7332 - val_mean_absolute_error: 1.7332\n",
      "Epoch 517/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3482 - mean_absolute_error: 0.3482 - val_loss: 1.7336 - val_mean_absolute_error: 1.7336\n",
      "Epoch 518/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3517 - mean_absolute_error: 0.3517 - val_loss: 1.7326 - val_mean_absolute_error: 1.7326\n",
      "Epoch 519/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3466 - mean_absolute_error: 0.3466 - val_loss: 1.7306 - val_mean_absolute_error: 1.7306\n",
      "Epoch 520/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3466 - mean_absolute_error: 0.3466 - val_loss: 1.7378 - val_mean_absolute_error: 1.7378\n",
      "Epoch 521/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3428 - mean_absolute_error: 0.3428 - val_loss: 1.7344 - val_mean_absolute_error: 1.7344\n",
      "Epoch 522/1000\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.3426 - mean_absolute_error: 0.3426 - val_loss: 1.7301 - val_mean_absolute_error: 1.7301\n",
      "Epoch 523/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3430 - mean_absolute_error: 0.3430 - val_loss: 1.7332 - val_mean_absolute_error: 1.7332\n",
      "Epoch 524/1000\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.3414 - mean_absolute_error: 0.3414 - val_loss: 1.7345 - val_mean_absolute_error: 1.7345\n",
      "Epoch 525/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3411 - mean_absolute_error: 0.3411 - val_loss: 1.7296 - val_mean_absolute_error: 1.7296\n",
      "Epoch 526/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3403 - mean_absolute_error: 0.3403 - val_loss: 1.7329 - val_mean_absolute_error: 1.7329\n",
      "Epoch 527/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3390 - mean_absolute_error: 0.3390 - val_loss: 1.7340 - val_mean_absolute_error: 1.7340\n",
      "Epoch 528/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3393 - mean_absolute_error: 0.3393 - val_loss: 1.7350 - val_mean_absolute_error: 1.7350\n",
      "Epoch 529/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3402 - mean_absolute_error: 0.3402 - val_loss: 1.7304 - val_mean_absolute_error: 1.7304\n",
      "Epoch 530/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3391 - mean_absolute_error: 0.3391 - val_loss: 1.7332 - val_mean_absolute_error: 1.7332\n",
      "Epoch 531/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3392 - mean_absolute_error: 0.3392 - val_loss: 1.7396 - val_mean_absolute_error: 1.7396\n",
      "Epoch 532/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3385 - mean_absolute_error: 0.3385 - val_loss: 1.7315 - val_mean_absolute_error: 1.7315\n",
      "Epoch 533/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3399 - mean_absolute_error: 0.3399 - val_loss: 1.7309 - val_mean_absolute_error: 1.7309\n",
      "Epoch 534/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3379 - mean_absolute_error: 0.3379 - val_loss: 1.7326 - val_mean_absolute_error: 1.7326\n",
      "Epoch 535/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3367 - mean_absolute_error: 0.3367 - val_loss: 1.7318 - val_mean_absolute_error: 1.7318\n",
      "Epoch 536/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3366 - mean_absolute_error: 0.3366 - val_loss: 1.7331 - val_mean_absolute_error: 1.7331\n",
      "Epoch 537/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3380 - mean_absolute_error: 0.3380 - val_loss: 1.7320 - val_mean_absolute_error: 1.7320\n",
      "Epoch 538/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3372 - mean_absolute_error: 0.3372 - val_loss: 1.7342 - val_mean_absolute_error: 1.7342\n",
      "Epoch 539/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3361 - mean_absolute_error: 0.3361 - val_loss: 1.7308 - val_mean_absolute_error: 1.7308\n",
      "Epoch 540/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3352 - mean_absolute_error: 0.3352 - val_loss: 1.7350 - val_mean_absolute_error: 1.7350\n",
      "Epoch 541/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3354 - mean_absolute_error: 0.3354 - val_loss: 1.7383 - val_mean_absolute_error: 1.7383\n",
      "Epoch 542/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3375 - mean_absolute_error: 0.3375 - val_loss: 1.7321 - val_mean_absolute_error: 1.7321\n",
      "Epoch 543/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3345 - mean_absolute_error: 0.3345 - val_loss: 1.7323 - val_mean_absolute_error: 1.7323\n",
      "Epoch 544/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3327 - mean_absolute_error: 0.3327 - val_loss: 1.7332 - val_mean_absolute_error: 1.7332\n",
      "Epoch 545/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3321 - mean_absolute_error: 0.3321 - val_loss: 1.7316 - val_mean_absolute_error: 1.7316\n",
      "Epoch 546/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3318 - mean_absolute_error: 0.3318 - val_loss: 1.7298 - val_mean_absolute_error: 1.7298\n",
      "Epoch 547/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3319 - mean_absolute_error: 0.3319 - val_loss: 1.7328 - val_mean_absolute_error: 1.7328\n",
      "Epoch 548/1000\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.3325 - mean_absolute_error: 0.3325 - val_loss: 1.7294 - val_mean_absolute_error: 1.7294\n",
      "Epoch 549/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3363 - mean_absolute_error: 0.3363 - val_loss: 1.7299 - val_mean_absolute_error: 1.7299\n",
      "Epoch 550/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3313 - mean_absolute_error: 0.3313 - val_loss: 1.7336 - val_mean_absolute_error: 1.7336\n",
      "Epoch 551/1000\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.3300 - mean_absolute_error: 0.3300 - val_loss: 1.7316 - val_mean_absolute_error: 1.7316\n",
      "Epoch 552/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3306 - mean_absolute_error: 0.3306 - val_loss: 1.7330 - val_mean_absolute_error: 1.7330\n",
      "Epoch 553/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3316 - mean_absolute_error: 0.3316 - val_loss: 1.7330 - val_mean_absolute_error: 1.7330\n",
      "Epoch 554/1000\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.3306 - mean_absolute_error: 0.3306 - val_loss: 1.7315 - val_mean_absolute_error: 1.7315\n",
      "Epoch 555/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3288 - mean_absolute_error: 0.3288 - val_loss: 1.7295 - val_mean_absolute_error: 1.7295\n",
      "Epoch 556/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3288 - mean_absolute_error: 0.3288 - val_loss: 1.7347 - val_mean_absolute_error: 1.7347\n",
      "Epoch 557/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3275 - mean_absolute_error: 0.3275 - val_loss: 1.7302 - val_mean_absolute_error: 1.7302\n",
      "Epoch 558/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.3287 - mean_absolute_error: 0.3287 - val_loss: 1.7320 - val_mean_absolute_error: 1.7320\n",
      "Epoch 559/1000\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 0.3291 - mean_absolute_error: 0.3291 - val_loss: 1.7309 - val_mean_absolute_error: 1.7309\n",
      "Epoch 560/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3264 - mean_absolute_error: 0.3264 - val_loss: 1.7322 - val_mean_absolute_error: 1.7322\n",
      "Epoch 561/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3250 - mean_absolute_error: 0.3250 - val_loss: 1.7308 - val_mean_absolute_error: 1.7308\n",
      "Epoch 562/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3255 - mean_absolute_error: 0.3255 - val_loss: 1.7305 - val_mean_absolute_error: 1.7305\n",
      "Epoch 563/1000\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.3240 - mean_absolute_error: 0.3240 - val_loss: 1.7311 - val_mean_absolute_error: 1.7311\n",
      "Epoch 564/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3247 - mean_absolute_error: 0.3247 - val_loss: 1.7332 - val_mean_absolute_error: 1.7332\n",
      "Epoch 565/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3241 - mean_absolute_error: 0.3241 - val_loss: 1.7314 - val_mean_absolute_error: 1.7314\n",
      "Epoch 566/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3239 - mean_absolute_error: 0.3239 - val_loss: 1.7295 - val_mean_absolute_error: 1.7295\n",
      "Epoch 567/1000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.3293 - mean_absolute_error: 0.3293 - val_loss: 1.7284 - val_mean_absolute_error: 1.7284\n",
      "Epoch 568/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3285 - mean_absolute_error: 0.3285 - val_loss: 1.7295 - val_mean_absolute_error: 1.7295\n",
      "Epoch 569/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3299 - mean_absolute_error: 0.3299 - val_loss: 1.7350 - val_mean_absolute_error: 1.7350\n",
      "Epoch 570/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.3294 - mean_absolute_error: 0.3294 - val_loss: 1.7314 - val_mean_absolute_error: 1.7314\n",
      "Epoch 571/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3258 - mean_absolute_error: 0.3258 - val_loss: 1.7356 - val_mean_absolute_error: 1.7356\n",
      "Epoch 572/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3235 - mean_absolute_error: 0.3235 - val_loss: 1.7341 - val_mean_absolute_error: 1.7341\n",
      "Epoch 573/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.3244 - mean_absolute_error: 0.3244 - val_loss: 1.7305 - val_mean_absolute_error: 1.7305\n",
      "Epoch 574/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3218 - mean_absolute_error: 0.3218 - val_loss: 1.7321 - val_mean_absolute_error: 1.7321\n",
      "Epoch 575/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.3217 - mean_absolute_error: 0.3217 - val_loss: 1.7315 - val_mean_absolute_error: 1.7315\n",
      "Epoch 576/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.3259 - mean_absolute_error: 0.3259 - val_loss: 1.7320 - val_mean_absolute_error: 1.7320\n",
      "Epoch 577/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3248 - mean_absolute_error: 0.3248 - val_loss: 1.7343 - val_mean_absolute_error: 1.7343\n",
      "Epoch 578/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3221 - mean_absolute_error: 0.3221 - val_loss: 1.7285 - val_mean_absolute_error: 1.7285\n",
      "Epoch 579/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.3212 - mean_absolute_error: 0.3212 - val_loss: 1.7331 - val_mean_absolute_error: 1.7331\n",
      "Epoch 580/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.3243 - mean_absolute_error: 0.3243 - val_loss: 1.7337 - val_mean_absolute_error: 1.7337\n",
      "Epoch 581/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.3263 - mean_absolute_error: 0.3263 - val_loss: 1.7366 - val_mean_absolute_error: 1.7366\n",
      "Epoch 582/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3291 - mean_absolute_error: 0.3291 - val_loss: 1.7278 - val_mean_absolute_error: 1.7278\n",
      "Epoch 583/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.3261 - mean_absolute_error: 0.3261 - val_loss: 1.7277 - val_mean_absolute_error: 1.7277\n",
      "Epoch 584/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.3217 - mean_absolute_error: 0.3217 - val_loss: 1.7278 - val_mean_absolute_error: 1.7278\n",
      "Epoch 585/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.3195 - mean_absolute_error: 0.3195 - val_loss: 1.7331 - val_mean_absolute_error: 1.7331\n",
      "Epoch 586/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3185 - mean_absolute_error: 0.3185 - val_loss: 1.7287 - val_mean_absolute_error: 1.7287\n",
      "Epoch 587/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3192 - mean_absolute_error: 0.3192 - val_loss: 1.7292 - val_mean_absolute_error: 1.7292\n",
      "Epoch 588/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3206 - mean_absolute_error: 0.3206 - val_loss: 1.7283 - val_mean_absolute_error: 1.7283\n",
      "Epoch 589/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.3215 - mean_absolute_error: 0.3215 - val_loss: 1.7288 - val_mean_absolute_error: 1.7288\n",
      "Epoch 590/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3188 - mean_absolute_error: 0.3188 - val_loss: 1.7330 - val_mean_absolute_error: 1.7330\n",
      "Epoch 591/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3204 - mean_absolute_error: 0.3204 - val_loss: 1.7331 - val_mean_absolute_error: 1.7331\n",
      "Epoch 592/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.3174 - mean_absolute_error: 0.3174 - val_loss: 1.7313 - val_mean_absolute_error: 1.7313\n",
      "Epoch 593/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3159 - mean_absolute_error: 0.3159 - val_loss: 1.7311 - val_mean_absolute_error: 1.7311\n",
      "Epoch 594/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3151 - mean_absolute_error: 0.3151 - val_loss: 1.7303 - val_mean_absolute_error: 1.7303\n",
      "Epoch 595/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.3142 - mean_absolute_error: 0.3142 - val_loss: 1.7322 - val_mean_absolute_error: 1.7322\n",
      "Epoch 596/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3162 - mean_absolute_error: 0.3162 - val_loss: 1.7297 - val_mean_absolute_error: 1.7297\n",
      "Epoch 597/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3150 - mean_absolute_error: 0.3150 - val_loss: 1.7365 - val_mean_absolute_error: 1.7365\n",
      "Epoch 598/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3168 - mean_absolute_error: 0.3168 - val_loss: 1.7351 - val_mean_absolute_error: 1.7351\n",
      "Epoch 599/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3165 - mean_absolute_error: 0.3165 - val_loss: 1.7295 - val_mean_absolute_error: 1.7295\n",
      "Epoch 600/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3145 - mean_absolute_error: 0.3145 - val_loss: 1.7306 - val_mean_absolute_error: 1.7306\n",
      "Epoch 601/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.3140 - mean_absolute_error: 0.3140 - val_loss: 1.7306 - val_mean_absolute_error: 1.7306\n",
      "Epoch 602/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3144 - mean_absolute_error: 0.3144 - val_loss: 1.7303 - val_mean_absolute_error: 1.7303\n",
      "Epoch 603/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.3127 - mean_absolute_error: 0.3127 - val_loss: 1.7335 - val_mean_absolute_error: 1.7335\n",
      "Epoch 604/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3118 - mean_absolute_error: 0.3118 - val_loss: 1.7308 - val_mean_absolute_error: 1.7308\n",
      "Epoch 605/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3108 - mean_absolute_error: 0.3108 - val_loss: 1.7302 - val_mean_absolute_error: 1.7302\n",
      "Epoch 606/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3111 - mean_absolute_error: 0.3111 - val_loss: 1.7328 - val_mean_absolute_error: 1.7328\n",
      "Epoch 607/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3109 - mean_absolute_error: 0.3109 - val_loss: 1.7282 - val_mean_absolute_error: 1.7282\n",
      "Epoch 608/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.3121 - mean_absolute_error: 0.3121 - val_loss: 1.7303 - val_mean_absolute_error: 1.7303\n",
      "Epoch 609/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3117 - mean_absolute_error: 0.3117 - val_loss: 1.7321 - val_mean_absolute_error: 1.7321\n",
      "Epoch 610/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.3151 - mean_absolute_error: 0.3151 - val_loss: 1.7319 - val_mean_absolute_error: 1.7319\n",
      "Epoch 611/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3112 - mean_absolute_error: 0.3112 - val_loss: 1.7300 - val_mean_absolute_error: 1.7300\n",
      "Epoch 612/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.3101 - mean_absolute_error: 0.3101 - val_loss: 1.7321 - val_mean_absolute_error: 1.7321\n",
      "Epoch 613/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3146 - mean_absolute_error: 0.3146 - val_loss: 1.7270 - val_mean_absolute_error: 1.7270\n",
      "Epoch 614/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3168 - mean_absolute_error: 0.3168 - val_loss: 1.7316 - val_mean_absolute_error: 1.7316\n",
      "Epoch 615/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3134 - mean_absolute_error: 0.3134 - val_loss: 1.7348 - val_mean_absolute_error: 1.7348\n",
      "Epoch 616/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3144 - mean_absolute_error: 0.3144 - val_loss: 1.7332 - val_mean_absolute_error: 1.7332\n",
      "Epoch 617/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3143 - mean_absolute_error: 0.3143 - val_loss: 1.7358 - val_mean_absolute_error: 1.7358\n",
      "Epoch 618/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3112 - mean_absolute_error: 0.3112 - val_loss: 1.7312 - val_mean_absolute_error: 1.7312\n",
      "Epoch 619/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3109 - mean_absolute_error: 0.3109 - val_loss: 1.7294 - val_mean_absolute_error: 1.7294\n",
      "Epoch 620/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.3115 - mean_absolute_error: 0.3115 - val_loss: 1.7283 - val_mean_absolute_error: 1.7283\n",
      "Epoch 621/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3091 - mean_absolute_error: 0.3091 - val_loss: 1.7283 - val_mean_absolute_error: 1.7283\n",
      "Epoch 622/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3084 - mean_absolute_error: 0.3084 - val_loss: 1.7280 - val_mean_absolute_error: 1.7280\n",
      "Epoch 623/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.3101 - mean_absolute_error: 0.3101 - val_loss: 1.7331 - val_mean_absolute_error: 1.7331\n",
      "Epoch 624/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3083 - mean_absolute_error: 0.3083 - val_loss: 1.7313 - val_mean_absolute_error: 1.7313\n",
      "Epoch 625/1000\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.3084 - mean_absolute_error: 0.3084 - val_loss: 1.7350 - val_mean_absolute_error: 1.7350\n",
      "Epoch 626/1000\n",
      "17/17 [==============================] - 2s 146ms/step - loss: 0.3109 - mean_absolute_error: 0.3109 - val_loss: 1.7303 - val_mean_absolute_error: 1.7303\n",
      "Epoch 627/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.3088 - mean_absolute_error: 0.3088 - val_loss: 1.7300 - val_mean_absolute_error: 1.7300\n",
      "Epoch 628/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.3066 - mean_absolute_error: 0.3066 - val_loss: 1.7319 - val_mean_absolute_error: 1.7319\n",
      "Epoch 629/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.3082 - mean_absolute_error: 0.3082 - val_loss: 1.7340 - val_mean_absolute_error: 1.7340\n",
      "Epoch 630/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.3088 - mean_absolute_error: 0.3088 - val_loss: 1.7323 - val_mean_absolute_error: 1.7323\n",
      "Epoch 631/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.3073 - mean_absolute_error: 0.3073 - val_loss: 1.7330 - val_mean_absolute_error: 1.7330\n",
      "Epoch 632/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.3084 - mean_absolute_error: 0.3084 - val_loss: 1.7291 - val_mean_absolute_error: 1.7291\n",
      "Epoch 633/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.3079 - mean_absolute_error: 0.3079 - val_loss: 1.7391 - val_mean_absolute_error: 1.7391\n",
      "Epoch 634/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.3112 - mean_absolute_error: 0.3112 - val_loss: 1.7314 - val_mean_absolute_error: 1.7314\n",
      "Epoch 635/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3072 - mean_absolute_error: 0.3072 - val_loss: 1.7332 - val_mean_absolute_error: 1.7332\n",
      "Epoch 636/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3059 - mean_absolute_error: 0.3059 - val_loss: 1.7282 - val_mean_absolute_error: 1.7282\n",
      "Epoch 637/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3056 - mean_absolute_error: 0.3056 - val_loss: 1.7335 - val_mean_absolute_error: 1.7335\n",
      "Epoch 638/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3049 - mean_absolute_error: 0.3049 - val_loss: 1.7320 - val_mean_absolute_error: 1.7320\n",
      "Epoch 639/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.3033 - mean_absolute_error: 0.3033 - val_loss: 1.7331 - val_mean_absolute_error: 1.7331\n",
      "Epoch 640/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3040 - mean_absolute_error: 0.3040 - val_loss: 1.7323 - val_mean_absolute_error: 1.7323\n",
      "Epoch 641/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.3041 - mean_absolute_error: 0.3041 - val_loss: 1.7323 - val_mean_absolute_error: 1.7323\n",
      "Epoch 642/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.3080 - mean_absolute_error: 0.3080 - val_loss: 1.7284 - val_mean_absolute_error: 1.7284\n",
      "Epoch 643/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.3039 - mean_absolute_error: 0.3039 - val_loss: 1.7293 - val_mean_absolute_error: 1.7293\n",
      "Epoch 644/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.3061 - mean_absolute_error: 0.3061 - val_loss: 1.7318 - val_mean_absolute_error: 1.7318\n",
      "Epoch 645/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.3037 - mean_absolute_error: 0.3037 - val_loss: 1.7330 - val_mean_absolute_error: 1.7330\n",
      "Epoch 646/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.3032 - mean_absolute_error: 0.3032 - val_loss: 1.7312 - val_mean_absolute_error: 1.7312\n",
      "Epoch 647/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.3027 - mean_absolute_error: 0.3027 - val_loss: 1.7351 - val_mean_absolute_error: 1.7351\n",
      "Epoch 648/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.3037 - mean_absolute_error: 0.3037 - val_loss: 1.7320 - val_mean_absolute_error: 1.7320\n",
      "Epoch 649/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.3036 - mean_absolute_error: 0.3036 - val_loss: 1.7387 - val_mean_absolute_error: 1.7387\n",
      "Epoch 650/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.3102 - mean_absolute_error: 0.3102 - val_loss: 1.7362 - val_mean_absolute_error: 1.7362\n",
      "Epoch 651/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3072 - mean_absolute_error: 0.3072 - val_loss: 1.7346 - val_mean_absolute_error: 1.7346\n",
      "Epoch 652/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3050 - mean_absolute_error: 0.3050 - val_loss: 1.7318 - val_mean_absolute_error: 1.7318\n",
      "Epoch 653/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.3020 - mean_absolute_error: 0.3020 - val_loss: 1.7348 - val_mean_absolute_error: 1.7348\n",
      "Epoch 654/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3013 - mean_absolute_error: 0.3013 - val_loss: 1.7297 - val_mean_absolute_error: 1.7297\n",
      "Epoch 655/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.3013 - mean_absolute_error: 0.3013 - val_loss: 1.7297 - val_mean_absolute_error: 1.7297\n",
      "Epoch 656/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.3070 - mean_absolute_error: 0.3070 - val_loss: 1.7287 - val_mean_absolute_error: 1.7287\n",
      "Epoch 657/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3101 - mean_absolute_error: 0.3101 - val_loss: 1.7339 - val_mean_absolute_error: 1.7339\n",
      "Epoch 658/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.3082 - mean_absolute_error: 0.3082 - val_loss: 1.7320 - val_mean_absolute_error: 1.7320\n",
      "Epoch 659/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.3025 - mean_absolute_error: 0.3025 - val_loss: 1.7309 - val_mean_absolute_error: 1.7309\n",
      "Epoch 660/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3001 - mean_absolute_error: 0.3001 - val_loss: 1.7304 - val_mean_absolute_error: 1.7304\n",
      "Epoch 661/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3026 - mean_absolute_error: 0.3026 - val_loss: 1.7324 - val_mean_absolute_error: 1.7324\n",
      "Epoch 662/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3021 - mean_absolute_error: 0.3021 - val_loss: 1.7313 - val_mean_absolute_error: 1.7313\n",
      "Epoch 663/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.3027 - mean_absolute_error: 0.3027 - val_loss: 1.7292 - val_mean_absolute_error: 1.7292\n",
      "Epoch 664/1000\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.3014 - mean_absolute_error: 0.3014 - val_loss: 1.7322 - val_mean_absolute_error: 1.7322\n",
      "Epoch 665/1000\n",
      "17/17 [==============================] - 3s 157ms/step - loss: 0.3005 - mean_absolute_error: 0.3005 - val_loss: 1.7341 - val_mean_absolute_error: 1.7341\n",
      "Epoch 666/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.2991 - mean_absolute_error: 0.2991 - val_loss: 1.7343 - val_mean_absolute_error: 1.7343\n",
      "Epoch 667/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.3013 - mean_absolute_error: 0.3013 - val_loss: 1.7276 - val_mean_absolute_error: 1.7276\n",
      "Epoch 668/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.3061 - mean_absolute_error: 0.3061 - val_loss: 1.7347 - val_mean_absolute_error: 1.7347\n",
      "Epoch 669/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.3020 - mean_absolute_error: 0.3020 - val_loss: 1.7326 - val_mean_absolute_error: 1.7326\n",
      "Epoch 670/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3015 - mean_absolute_error: 0.3015 - val_loss: 1.7341 - val_mean_absolute_error: 1.7341\n",
      "Epoch 671/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3039 - mean_absolute_error: 0.3039 - val_loss: 1.7360 - val_mean_absolute_error: 1.7360\n",
      "Epoch 672/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.3003 - mean_absolute_error: 0.3003 - val_loss: 1.7314 - val_mean_absolute_error: 1.7314\n",
      "Epoch 673/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.3003 - mean_absolute_error: 0.3003 - val_loss: 1.7315 - val_mean_absolute_error: 1.7315\n",
      "Epoch 674/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2998 - mean_absolute_error: 0.2998 - val_loss: 1.7340 - val_mean_absolute_error: 1.7340\n",
      "Epoch 675/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.3001 - mean_absolute_error: 0.3001 - val_loss: 1.7311 - val_mean_absolute_error: 1.7311\n",
      "Epoch 676/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2984 - mean_absolute_error: 0.2984 - val_loss: 1.7349 - val_mean_absolute_error: 1.7349\n",
      "Epoch 677/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.3011 - mean_absolute_error: 0.3011 - val_loss: 1.7373 - val_mean_absolute_error: 1.7373\n",
      "Epoch 678/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.3054 - mean_absolute_error: 0.3054 - val_loss: 1.7276 - val_mean_absolute_error: 1.7276\n",
      "Epoch 679/1000\n",
      "17/17 [==============================] - 4s 216ms/step - loss: 0.3018 - mean_absolute_error: 0.3018 - val_loss: 1.7322 - val_mean_absolute_error: 1.7322\n",
      "Epoch 680/1000\n",
      "17/17 [==============================] - 3s 174ms/step - loss: 0.3005 - mean_absolute_error: 0.3005 - val_loss: 1.7339 - val_mean_absolute_error: 1.7339\n",
      "Epoch 681/1000\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 0.3013 - mean_absolute_error: 0.3013 - val_loss: 1.7313 - val_mean_absolute_error: 1.7313\n",
      "Epoch 682/1000\n",
      "17/17 [==============================] - 3s 162ms/step - loss: 0.2984 - mean_absolute_error: 0.2984 - val_loss: 1.7320 - val_mean_absolute_error: 1.7320\n",
      "Epoch 683/1000\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 0.2974 - mean_absolute_error: 0.2974 - val_loss: 1.7331 - val_mean_absolute_error: 1.7331\n",
      "Epoch 684/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.2973 - mean_absolute_error: 0.2973 - val_loss: 1.7322 - val_mean_absolute_error: 1.7322\n",
      "Epoch 685/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.2961 - mean_absolute_error: 0.2961 - val_loss: 1.7324 - val_mean_absolute_error: 1.7324\n",
      "Epoch 686/1000\n",
      "17/17 [==============================] - 2s 139ms/step - loss: 0.2967 - mean_absolute_error: 0.2967 - val_loss: 1.7314 - val_mean_absolute_error: 1.7314\n",
      "Epoch 687/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.2964 - mean_absolute_error: 0.2964 - val_loss: 1.7327 - val_mean_absolute_error: 1.7327\n",
      "Epoch 688/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.2967 - mean_absolute_error: 0.2967 - val_loss: 1.7356 - val_mean_absolute_error: 1.7356\n",
      "Epoch 689/1000\n",
      "17/17 [==============================] - 2s 141ms/step - loss: 0.2996 - mean_absolute_error: 0.2996 - val_loss: 1.7303 - val_mean_absolute_error: 1.7303\n",
      "Epoch 690/1000\n",
      "17/17 [==============================] - 2s 138ms/step - loss: 0.3048 - mean_absolute_error: 0.3048 - val_loss: 1.7305 - val_mean_absolute_error: 1.7305\n",
      "Epoch 691/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.3069 - mean_absolute_error: 0.3069 - val_loss: 1.7358 - val_mean_absolute_error: 1.7358\n",
      "Epoch 692/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.3014 - mean_absolute_error: 0.3014 - val_loss: 1.7303 - val_mean_absolute_error: 1.7303\n",
      "Epoch 693/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.3000 - mean_absolute_error: 0.3000 - val_loss: 1.7332 - val_mean_absolute_error: 1.7332\n",
      "Epoch 694/1000\n",
      "17/17 [==============================] - 3s 152ms/step - loss: 0.2960 - mean_absolute_error: 0.2960 - val_loss: 1.7361 - val_mean_absolute_error: 1.7361\n",
      "Epoch 695/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3017 - mean_absolute_error: 0.3017 - val_loss: 1.7291 - val_mean_absolute_error: 1.7291\n",
      "Epoch 696/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.2997 - mean_absolute_error: 0.2997 - val_loss: 1.7308 - val_mean_absolute_error: 1.7308\n",
      "Epoch 697/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2964 - mean_absolute_error: 0.2964 - val_loss: 1.7312 - val_mean_absolute_error: 1.7312\n",
      "Epoch 698/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.2975 - mean_absolute_error: 0.2975 - val_loss: 1.7365 - val_mean_absolute_error: 1.7365\n",
      "Epoch 699/1000\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.3027 - mean_absolute_error: 0.3027 - val_loss: 1.7308 - val_mean_absolute_error: 1.7308\n",
      "Epoch 700/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.3052 - mean_absolute_error: 0.3052 - val_loss: 1.7398 - val_mean_absolute_error: 1.7398\n",
      "Epoch 701/1000\n",
      "17/17 [==============================] - 3s 156ms/step - loss: 0.3009 - mean_absolute_error: 0.3009 - val_loss: 1.7331 - val_mean_absolute_error: 1.7331\n",
      "Epoch 702/1000\n",
      "17/17 [==============================] - 3s 150ms/step - loss: 0.2962 - mean_absolute_error: 0.2962 - val_loss: 1.7311 - val_mean_absolute_error: 1.7311\n",
      "Epoch 703/1000\n",
      "17/17 [==============================] - 3s 202ms/step - loss: 0.2963 - mean_absolute_error: 0.2963 - val_loss: 1.7293 - val_mean_absolute_error: 1.7293\n",
      "Epoch 704/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2958 - mean_absolute_error: 0.2958 - val_loss: 1.7312 - val_mean_absolute_error: 1.7312\n",
      "Epoch 705/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2953 - mean_absolute_error: 0.2953 - val_loss: 1.7344 - val_mean_absolute_error: 1.7344\n",
      "Epoch 706/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.2962 - mean_absolute_error: 0.2962 - val_loss: 1.7305 - val_mean_absolute_error: 1.7305\n",
      "Epoch 707/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.2947 - mean_absolute_error: 0.2947 - val_loss: 1.7293 - val_mean_absolute_error: 1.7293\n",
      "Epoch 708/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.2964 - mean_absolute_error: 0.2964 - val_loss: 1.7283 - val_mean_absolute_error: 1.7283\n",
      "Epoch 709/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.2997 - mean_absolute_error: 0.2997 - val_loss: 1.7293 - val_mean_absolute_error: 1.7293\n",
      "Epoch 710/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.2976 - mean_absolute_error: 0.2976 - val_loss: 1.7284 - val_mean_absolute_error: 1.7284\n",
      "Epoch 711/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.2970 - mean_absolute_error: 0.2970 - val_loss: 1.7318 - val_mean_absolute_error: 1.7318\n",
      "Epoch 712/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.2965 - mean_absolute_error: 0.2965 - val_loss: 1.7338 - val_mean_absolute_error: 1.7338\n",
      "Epoch 713/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.2946 - mean_absolute_error: 0.2946 - val_loss: 1.7325 - val_mean_absolute_error: 1.7325\n",
      "Epoch 714/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.2962 - mean_absolute_error: 0.2962 - val_loss: 1.7282 - val_mean_absolute_error: 1.7282\n",
      "Epoch 715/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.2964 - mean_absolute_error: 0.2964 - val_loss: 1.7300 - val_mean_absolute_error: 1.7300\n",
      "Epoch 716/1000\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.2949 - mean_absolute_error: 0.2949 - val_loss: 1.7309 - val_mean_absolute_error: 1.7309\n",
      "Epoch 717/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2939 - mean_absolute_error: 0.2939 - val_loss: 1.7307 - val_mean_absolute_error: 1.7307\n",
      "Epoch 718/1000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2929 - mean_absolute_error: 0.2929 - val_loss: 1.7281 - val_mean_absolute_error: 1.7281\n",
      "Epoch 719/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.2950 - mean_absolute_error: 0.2950 - val_loss: 1.7267 - val_mean_absolute_error: 1.7267\n",
      "Epoch 720/1000\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.2952 - mean_absolute_error: 0.2952 - val_loss: 1.7309 - val_mean_absolute_error: 1.7309\n",
      "Epoch 721/1000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2925 - mean_absolute_error: 0.2925 - val_loss: 1.7329 - val_mean_absolute_error: 1.7329\n",
      "Epoch 722/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.2938 - mean_absolute_error: 0.2938 - val_loss: 1.7301 - val_mean_absolute_error: 1.7301\n",
      "Epoch 723/1000\n",
      "17/17 [==============================] - 2s 112ms/step - loss: 0.2961 - mean_absolute_error: 0.2961 - val_loss: 1.7311 - val_mean_absolute_error: 1.7311\n",
      "Epoch 724/1000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2971 - mean_absolute_error: 0.2971 - val_loss: 1.7272 - val_mean_absolute_error: 1.7272\n",
      "Epoch 725/1000\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.2964 - mean_absolute_error: 0.2964 - val_loss: 1.7295 - val_mean_absolute_error: 1.7295\n",
      "Epoch 726/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2953 - mean_absolute_error: 0.2953 - val_loss: 1.7281 - val_mean_absolute_error: 1.7281\n",
      "Epoch 727/1000\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.2959 - mean_absolute_error: 0.2959 - val_loss: 1.7312 - val_mean_absolute_error: 1.7312\n",
      "Epoch 728/1000\n",
      "17/17 [==============================] - 2s 141ms/step - loss: 0.2934 - mean_absolute_error: 0.2934 - val_loss: 1.7299 - val_mean_absolute_error: 1.7299\n",
      "Epoch 729/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.2922 - mean_absolute_error: 0.2922 - val_loss: 1.7293 - val_mean_absolute_error: 1.7293\n",
      "Epoch 730/1000\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 0.2925 - mean_absolute_error: 0.2925 - val_loss: 1.7285 - val_mean_absolute_error: 1.7285\n",
      "Epoch 731/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2954 - mean_absolute_error: 0.2954 - val_loss: 1.7286 - val_mean_absolute_error: 1.7286\n",
      "Epoch 732/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2954 - mean_absolute_error: 0.2954 - val_loss: 1.7273 - val_mean_absolute_error: 1.7273\n",
      "Epoch 733/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2949 - mean_absolute_error: 0.2949 - val_loss: 1.7351 - val_mean_absolute_error: 1.7351\n",
      "Epoch 734/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.2965 - mean_absolute_error: 0.2965 - val_loss: 1.7320 - val_mean_absolute_error: 1.7320\n",
      "Epoch 735/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2951 - mean_absolute_error: 0.2951 - val_loss: 1.7311 - val_mean_absolute_error: 1.7311\n",
      "Epoch 736/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.2962 - mean_absolute_error: 0.2962 - val_loss: 1.7300 - val_mean_absolute_error: 1.7300\n",
      "Epoch 737/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2928 - mean_absolute_error: 0.2928 - val_loss: 1.7332 - val_mean_absolute_error: 1.7332\n",
      "Epoch 738/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2930 - mean_absolute_error: 0.2930 - val_loss: 1.7355 - val_mean_absolute_error: 1.7355\n",
      "Epoch 739/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2954 - mean_absolute_error: 0.2954 - val_loss: 1.7339 - val_mean_absolute_error: 1.7339\n",
      "Epoch 740/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2940 - mean_absolute_error: 0.2940 - val_loss: 1.7300 - val_mean_absolute_error: 1.7300\n",
      "Epoch 741/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2933 - mean_absolute_error: 0.2933 - val_loss: 1.7302 - val_mean_absolute_error: 1.7302\n",
      "Epoch 742/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.2940 - mean_absolute_error: 0.2940 - val_loss: 1.7305 - val_mean_absolute_error: 1.7305\n",
      "Epoch 743/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2910 - mean_absolute_error: 0.2910 - val_loss: 1.7291 - val_mean_absolute_error: 1.7291\n",
      "Epoch 744/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.2905 - mean_absolute_error: 0.2905 - val_loss: 1.7303 - val_mean_absolute_error: 1.7303\n",
      "Epoch 745/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2904 - mean_absolute_error: 0.2904 - val_loss: 1.7309 - val_mean_absolute_error: 1.7309\n",
      "Epoch 746/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.2916 - mean_absolute_error: 0.2916 - val_loss: 1.7310 - val_mean_absolute_error: 1.7310\n",
      "Epoch 747/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2906 - mean_absolute_error: 0.2906 - val_loss: 1.7316 - val_mean_absolute_error: 1.7316\n",
      "Epoch 748/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2906 - mean_absolute_error: 0.2906 - val_loss: 1.7297 - val_mean_absolute_error: 1.7297\n",
      "Epoch 749/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2922 - mean_absolute_error: 0.2922 - val_loss: 1.7268 - val_mean_absolute_error: 1.7268\n",
      "Epoch 750/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2928 - mean_absolute_error: 0.2928 - val_loss: 1.7330 - val_mean_absolute_error: 1.7330\n",
      "Epoch 751/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.2929 - mean_absolute_error: 0.2929 - val_loss: 1.7261 - val_mean_absolute_error: 1.7261\n",
      "Epoch 752/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.2952 - mean_absolute_error: 0.2952 - val_loss: 1.7308 - val_mean_absolute_error: 1.7308\n",
      "Epoch 753/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2938 - mean_absolute_error: 0.2938 - val_loss: 1.7273 - val_mean_absolute_error: 1.7273\n",
      "Epoch 754/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2919 - mean_absolute_error: 0.2919 - val_loss: 1.7274 - val_mean_absolute_error: 1.7274\n",
      "Epoch 755/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2902 - mean_absolute_error: 0.2902 - val_loss: 1.7293 - val_mean_absolute_error: 1.7293\n",
      "Epoch 756/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2900 - mean_absolute_error: 0.2900 - val_loss: 1.7298 - val_mean_absolute_error: 1.7298\n",
      "Epoch 757/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.2926 - mean_absolute_error: 0.2926 - val_loss: 1.7265 - val_mean_absolute_error: 1.7265\n",
      "Epoch 758/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.2914 - mean_absolute_error: 0.2914 - val_loss: 1.7259 - val_mean_absolute_error: 1.7259\n",
      "Epoch 759/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2899 - mean_absolute_error: 0.2899 - val_loss: 1.7269 - val_mean_absolute_error: 1.7269\n",
      "Epoch 760/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2917 - mean_absolute_error: 0.2917 - val_loss: 1.7289 - val_mean_absolute_error: 1.7289\n",
      "Epoch 761/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2901 - mean_absolute_error: 0.2901 - val_loss: 1.7295 - val_mean_absolute_error: 1.7295\n",
      "Epoch 762/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2890 - mean_absolute_error: 0.2890 - val_loss: 1.7303 - val_mean_absolute_error: 1.7303\n",
      "Epoch 763/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2931 - mean_absolute_error: 0.2931 - val_loss: 1.7299 - val_mean_absolute_error: 1.7299\n",
      "Epoch 764/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2936 - mean_absolute_error: 0.2936 - val_loss: 1.7303 - val_mean_absolute_error: 1.7303\n",
      "Epoch 765/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2900 - mean_absolute_error: 0.2900 - val_loss: 1.7302 - val_mean_absolute_error: 1.7302\n",
      "Epoch 766/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2911 - mean_absolute_error: 0.2911 - val_loss: 1.7321 - val_mean_absolute_error: 1.7321\n",
      "Epoch 767/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2900 - mean_absolute_error: 0.2900 - val_loss: 1.7289 - val_mean_absolute_error: 1.7289\n",
      "Epoch 768/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2895 - mean_absolute_error: 0.2895 - val_loss: 1.7272 - val_mean_absolute_error: 1.7272\n",
      "Epoch 769/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.2901 - mean_absolute_error: 0.2901 - val_loss: 1.7289 - val_mean_absolute_error: 1.7289\n",
      "Epoch 770/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.2923 - mean_absolute_error: 0.2923 - val_loss: 1.7322 - val_mean_absolute_error: 1.7322\n",
      "Epoch 771/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2934 - mean_absolute_error: 0.2934 - val_loss: 1.7298 - val_mean_absolute_error: 1.7298\n",
      "Epoch 772/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2902 - mean_absolute_error: 0.2902 - val_loss: 1.7280 - val_mean_absolute_error: 1.7280\n",
      "Epoch 773/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2906 - mean_absolute_error: 0.2906 - val_loss: 1.7288 - val_mean_absolute_error: 1.7288\n",
      "Epoch 774/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2895 - mean_absolute_error: 0.2895 - val_loss: 1.7323 - val_mean_absolute_error: 1.7323\n",
      "Epoch 775/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2894 - mean_absolute_error: 0.2894 - val_loss: 1.7321 - val_mean_absolute_error: 1.7321\n",
      "Epoch 776/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2885 - mean_absolute_error: 0.2885 - val_loss: 1.7262 - val_mean_absolute_error: 1.7262\n",
      "Epoch 777/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2921 - mean_absolute_error: 0.2921 - val_loss: 1.7281 - val_mean_absolute_error: 1.7281\n",
      "Epoch 778/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2909 - mean_absolute_error: 0.2909 - val_loss: 1.7330 - val_mean_absolute_error: 1.7330\n",
      "Epoch 779/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2925 - mean_absolute_error: 0.2925 - val_loss: 1.7280 - val_mean_absolute_error: 1.7280\n",
      "Epoch 780/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2900 - mean_absolute_error: 0.2900 - val_loss: 1.7297 - val_mean_absolute_error: 1.7297\n",
      "Epoch 781/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2901 - mean_absolute_error: 0.2901 - val_loss: 1.7296 - val_mean_absolute_error: 1.7296\n",
      "Epoch 782/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2886 - mean_absolute_error: 0.2886 - val_loss: 1.7274 - val_mean_absolute_error: 1.7274\n",
      "Epoch 783/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2899 - mean_absolute_error: 0.2899 - val_loss: 1.7274 - val_mean_absolute_error: 1.7274\n",
      "Epoch 784/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2893 - mean_absolute_error: 0.2893 - val_loss: 1.7319 - val_mean_absolute_error: 1.7319\n",
      "Epoch 785/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2874 - mean_absolute_error: 0.2874 - val_loss: 1.7297 - val_mean_absolute_error: 1.7297\n",
      "Epoch 786/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.2879 - mean_absolute_error: 0.2879 - val_loss: 1.7324 - val_mean_absolute_error: 1.7324\n",
      "Epoch 787/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2925 - mean_absolute_error: 0.2925 - val_loss: 1.7282 - val_mean_absolute_error: 1.7282\n",
      "Epoch 788/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2903 - mean_absolute_error: 0.2903 - val_loss: 1.7311 - val_mean_absolute_error: 1.7311\n",
      "Epoch 789/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2887 - mean_absolute_error: 0.2887 - val_loss: 1.7305 - val_mean_absolute_error: 1.7305\n",
      "Epoch 790/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2878 - mean_absolute_error: 0.2878 - val_loss: 1.7268 - val_mean_absolute_error: 1.7268\n",
      "Epoch 791/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2873 - mean_absolute_error: 0.2873 - val_loss: 1.7311 - val_mean_absolute_error: 1.7311\n",
      "Epoch 792/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2866 - mean_absolute_error: 0.2866 - val_loss: 1.7299 - val_mean_absolute_error: 1.7299\n",
      "Epoch 793/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.2865 - mean_absolute_error: 0.2865 - val_loss: 1.7270 - val_mean_absolute_error: 1.7270\n",
      "Epoch 794/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2857 - mean_absolute_error: 0.2857 - val_loss: 1.7273 - val_mean_absolute_error: 1.7273\n",
      "Epoch 795/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2856 - mean_absolute_error: 0.2856 - val_loss: 1.7267 - val_mean_absolute_error: 1.7267\n",
      "Epoch 796/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2851 - mean_absolute_error: 0.2851 - val_loss: 1.7272 - val_mean_absolute_error: 1.7272\n",
      "Epoch 797/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.2860 - mean_absolute_error: 0.2860 - val_loss: 1.7257 - val_mean_absolute_error: 1.7257\n",
      "Epoch 798/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.2893 - mean_absolute_error: 0.2893 - val_loss: 1.7256 - val_mean_absolute_error: 1.7256\n",
      "Epoch 799/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2910 - mean_absolute_error: 0.2910 - val_loss: 1.7247 - val_mean_absolute_error: 1.7247\n",
      "Epoch 800/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.2891 - mean_absolute_error: 0.2891 - val_loss: 1.7310 - val_mean_absolute_error: 1.7310\n",
      "Epoch 801/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.2880 - mean_absolute_error: 0.2880 - val_loss: 1.7281 - val_mean_absolute_error: 1.7281\n",
      "Epoch 802/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.2872 - mean_absolute_error: 0.2872 - val_loss: 1.7307 - val_mean_absolute_error: 1.7307\n",
      "Epoch 803/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.2863 - mean_absolute_error: 0.2863 - val_loss: 1.7271 - val_mean_absolute_error: 1.7271\n",
      "Epoch 804/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2850 - mean_absolute_error: 0.2850 - val_loss: 1.7302 - val_mean_absolute_error: 1.7302\n",
      "Epoch 805/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.2895 - mean_absolute_error: 0.2895 - val_loss: 1.7278 - val_mean_absolute_error: 1.7278\n",
      "Epoch 806/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2865 - mean_absolute_error: 0.2865 - val_loss: 1.7290 - val_mean_absolute_error: 1.7290\n",
      "Epoch 807/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2852 - mean_absolute_error: 0.2852 - val_loss: 1.7290 - val_mean_absolute_error: 1.7290\n",
      "Epoch 808/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2852 - mean_absolute_error: 0.2852 - val_loss: 1.7307 - val_mean_absolute_error: 1.7307\n",
      "Epoch 809/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2854 - mean_absolute_error: 0.2854 - val_loss: 1.7285 - val_mean_absolute_error: 1.7285\n",
      "Epoch 810/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.2861 - mean_absolute_error: 0.2861 - val_loss: 1.7261 - val_mean_absolute_error: 1.7261\n",
      "Epoch 811/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2949 - mean_absolute_error: 0.2949 - val_loss: 1.7364 - val_mean_absolute_error: 1.7364\n",
      "Epoch 812/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2982 - mean_absolute_error: 0.2982 - val_loss: 1.7274 - val_mean_absolute_error: 1.7274\n",
      "Epoch 813/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2926 - mean_absolute_error: 0.2926 - val_loss: 1.7320 - val_mean_absolute_error: 1.7320\n",
      "Epoch 814/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2906 - mean_absolute_error: 0.2906 - val_loss: 1.7340 - val_mean_absolute_error: 1.7340\n",
      "Epoch 815/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2885 - mean_absolute_error: 0.2885 - val_loss: 1.7349 - val_mean_absolute_error: 1.7349\n",
      "Epoch 816/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2888 - mean_absolute_error: 0.2888 - val_loss: 1.7319 - val_mean_absolute_error: 1.7319\n",
      "Epoch 817/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.2870 - mean_absolute_error: 0.2870 - val_loss: 1.7282 - val_mean_absolute_error: 1.7282\n",
      "Epoch 818/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2878 - mean_absolute_error: 0.2878 - val_loss: 1.7303 - val_mean_absolute_error: 1.7303\n",
      "Epoch 819/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2846 - mean_absolute_error: 0.2846 - val_loss: 1.7309 - val_mean_absolute_error: 1.7309\n",
      "Epoch 820/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2871 - mean_absolute_error: 0.2871 - val_loss: 1.7265 - val_mean_absolute_error: 1.7265\n",
      "Epoch 821/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.2915 - mean_absolute_error: 0.2915 - val_loss: 1.7337 - val_mean_absolute_error: 1.7337\n",
      "Epoch 822/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2890 - mean_absolute_error: 0.2890 - val_loss: 1.7306 - val_mean_absolute_error: 1.7306\n",
      "Epoch 823/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2878 - mean_absolute_error: 0.2878 - val_loss: 1.7282 - val_mean_absolute_error: 1.7282\n",
      "Epoch 824/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2858 - mean_absolute_error: 0.2858 - val_loss: 1.7272 - val_mean_absolute_error: 1.7272\n",
      "Epoch 825/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2878 - mean_absolute_error: 0.2878 - val_loss: 1.7303 - val_mean_absolute_error: 1.7303\n",
      "Epoch 826/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.2891 - mean_absolute_error: 0.2891 - val_loss: 1.7266 - val_mean_absolute_error: 1.7266\n",
      "Epoch 827/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.2928 - mean_absolute_error: 0.2928 - val_loss: 1.7299 - val_mean_absolute_error: 1.7299\n",
      "Epoch 828/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.2877 - mean_absolute_error: 0.2877 - val_loss: 1.7289 - val_mean_absolute_error: 1.7289\n",
      "Epoch 829/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.2854 - mean_absolute_error: 0.2854 - val_loss: 1.7251 - val_mean_absolute_error: 1.7251\n",
      "Epoch 830/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.2857 - mean_absolute_error: 0.2857 - val_loss: 1.7278 - val_mean_absolute_error: 1.7278\n",
      "Epoch 831/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.2845 - mean_absolute_error: 0.2845 - val_loss: 1.7288 - val_mean_absolute_error: 1.7288\n",
      "Epoch 832/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.2838 - mean_absolute_error: 0.2838 - val_loss: 1.7236 - val_mean_absolute_error: 1.7236\n",
      "Epoch 833/1000\n",
      "17/17 [==============================] - 2s 138ms/step - loss: 0.2908 - mean_absolute_error: 0.2908 - val_loss: 1.7316 - val_mean_absolute_error: 1.7316\n",
      "Epoch 834/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.2937 - mean_absolute_error: 0.2937 - val_loss: 1.7265 - val_mean_absolute_error: 1.7265\n",
      "Epoch 835/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.2960 - mean_absolute_error: 0.2960 - val_loss: 1.7362 - val_mean_absolute_error: 1.7362\n",
      "Epoch 836/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.2933 - mean_absolute_error: 0.2933 - val_loss: 1.7277 - val_mean_absolute_error: 1.7277\n",
      "Epoch 837/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.2946 - mean_absolute_error: 0.2946 - val_loss: 1.7306 - val_mean_absolute_error: 1.7306\n",
      "Epoch 838/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.2916 - mean_absolute_error: 0.2916 - val_loss: 1.7270 - val_mean_absolute_error: 1.7270\n",
      "Epoch 839/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.2853 - mean_absolute_error: 0.2853 - val_loss: 1.7301 - val_mean_absolute_error: 1.7301\n",
      "Epoch 840/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2839 - mean_absolute_error: 0.2839 - val_loss: 1.7260 - val_mean_absolute_error: 1.7260\n",
      "Epoch 841/1000\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 0.2850 - mean_absolute_error: 0.2850 - val_loss: 1.7272 - val_mean_absolute_error: 1.7272\n",
      "Epoch 842/1000\n",
      "17/17 [==============================] - 4s 234ms/step - loss: 0.2815 - mean_absolute_error: 0.2815 - val_loss: 1.7286 - val_mean_absolute_error: 1.7286\n",
      "Epoch 843/1000\n",
      "17/17 [==============================] - 3s 179ms/step - loss: 0.2821 - mean_absolute_error: 0.2821 - val_loss: 1.7306 - val_mean_absolute_error: 1.7306\n",
      "Epoch 844/1000\n",
      "17/17 [==============================] - 3s 186ms/step - loss: 0.2844 - mean_absolute_error: 0.2844 - val_loss: 1.7268 - val_mean_absolute_error: 1.7268\n",
      "Epoch 845/1000\n",
      "17/17 [==============================] - 3s 158ms/step - loss: 0.2864 - mean_absolute_error: 0.2864 - val_loss: 1.7277 - val_mean_absolute_error: 1.7277\n",
      "Epoch 846/1000\n",
      "17/17 [==============================] - 3s 163ms/step - loss: 0.2839 - mean_absolute_error: 0.2839 - val_loss: 1.7287 - val_mean_absolute_error: 1.7287\n",
      "Epoch 847/1000\n",
      "17/17 [==============================] - 3s 163ms/step - loss: 0.2854 - mean_absolute_error: 0.2854 - val_loss: 1.7261 - val_mean_absolute_error: 1.7261\n",
      "Epoch 848/1000\n",
      "17/17 [==============================] - 3s 160ms/step - loss: 0.2858 - mean_absolute_error: 0.2858 - val_loss: 1.7271 - val_mean_absolute_error: 1.7271\n",
      "Epoch 849/1000\n",
      "17/17 [==============================] - 3s 162ms/step - loss: 0.2859 - mean_absolute_error: 0.2859 - val_loss: 1.7271 - val_mean_absolute_error: 1.7271\n",
      "Epoch 850/1000\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 0.2850 - mean_absolute_error: 0.2850 - val_loss: 1.7288 - val_mean_absolute_error: 1.7288\n",
      "Epoch 851/1000\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 0.2851 - mean_absolute_error: 0.2851 - val_loss: 1.7253 - val_mean_absolute_error: 1.7253\n",
      "Epoch 852/1000\n",
      "17/17 [==============================] - 2s 147ms/step - loss: 0.2844 - mean_absolute_error: 0.2844 - val_loss: 1.7240 - val_mean_absolute_error: 1.7240\n",
      "Epoch 853/1000\n",
      "17/17 [==============================] - 2s 141ms/step - loss: 0.2891 - mean_absolute_error: 0.2891 - val_loss: 1.7272 - val_mean_absolute_error: 1.7272\n",
      "Epoch 854/1000\n",
      "17/17 [==============================] - 2s 141ms/step - loss: 0.2840 - mean_absolute_error: 0.2840 - val_loss: 1.7266 - val_mean_absolute_error: 1.7266\n",
      "Epoch 855/1000\n",
      "17/17 [==============================] - 2s 140ms/step - loss: 0.2825 - mean_absolute_error: 0.2825 - val_loss: 1.7261 - val_mean_absolute_error: 1.7261\n",
      "Epoch 856/1000\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.2829 - mean_absolute_error: 0.2829 - val_loss: 1.7269 - val_mean_absolute_error: 1.7269\n",
      "Epoch 857/1000\n",
      "17/17 [==============================] - 2s 141ms/step - loss: 0.2820 - mean_absolute_error: 0.2820 - val_loss: 1.7276 - val_mean_absolute_error: 1.7276\n",
      "Epoch 858/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.2817 - mean_absolute_error: 0.2817 - val_loss: 1.7284 - val_mean_absolute_error: 1.7284\n",
      "Epoch 859/1000\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.2808 - mean_absolute_error: 0.2808 - val_loss: 1.7297 - val_mean_absolute_error: 1.7297\n",
      "Epoch 860/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.2825 - mean_absolute_error: 0.2825 - val_loss: 1.7282 - val_mean_absolute_error: 1.7282\n",
      "Epoch 861/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2824 - mean_absolute_error: 0.2824 - val_loss: 1.7263 - val_mean_absolute_error: 1.7263\n",
      "Epoch 862/1000\n",
      "17/17 [==============================] - 3s 154ms/step - loss: 0.2825 - mean_absolute_error: 0.2825 - val_loss: 1.7273 - val_mean_absolute_error: 1.7273\n",
      "Epoch 863/1000\n",
      "17/17 [==============================] - 2s 132ms/step - loss: 0.2812 - mean_absolute_error: 0.2812 - val_loss: 1.7257 - val_mean_absolute_error: 1.7257\n",
      "Epoch 864/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2839 - mean_absolute_error: 0.2839 - val_loss: 1.7272 - val_mean_absolute_error: 1.7272\n",
      "Epoch 865/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.2839 - mean_absolute_error: 0.2839 - val_loss: 1.7299 - val_mean_absolute_error: 1.7299\n",
      "Epoch 866/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.2827 - mean_absolute_error: 0.2827 - val_loss: 1.7278 - val_mean_absolute_error: 1.7278\n",
      "Epoch 867/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2807 - mean_absolute_error: 0.2807 - val_loss: 1.7300 - val_mean_absolute_error: 1.7300\n",
      "Epoch 868/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2803 - mean_absolute_error: 0.2803 - val_loss: 1.7272 - val_mean_absolute_error: 1.7272\n",
      "Epoch 869/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2805 - mean_absolute_error: 0.2805 - val_loss: 1.7291 - val_mean_absolute_error: 1.7291\n",
      "Epoch 870/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.2806 - mean_absolute_error: 0.2806 - val_loss: 1.7257 - val_mean_absolute_error: 1.7257\n",
      "Epoch 871/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.2819 - mean_absolute_error: 0.2819 - val_loss: 1.7303 - val_mean_absolute_error: 1.7303\n",
      "Epoch 872/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.2818 - mean_absolute_error: 0.2818 - val_loss: 1.7275 - val_mean_absolute_error: 1.7275\n",
      "Epoch 873/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.2812 - mean_absolute_error: 0.2812 - val_loss: 1.7280 - val_mean_absolute_error: 1.7280\n",
      "Epoch 874/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.2821 - mean_absolute_error: 0.2821 - val_loss: 1.7286 - val_mean_absolute_error: 1.7286\n",
      "Epoch 875/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.2824 - mean_absolute_error: 0.2824 - val_loss: 1.7262 - val_mean_absolute_error: 1.7262\n",
      "Epoch 876/1000\n",
      "17/17 [==============================] - 2s 139ms/step - loss: 0.2811 - mean_absolute_error: 0.2811 - val_loss: 1.7283 - val_mean_absolute_error: 1.7283\n",
      "Epoch 877/1000\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.2792 - mean_absolute_error: 0.2792 - val_loss: 1.7279 - val_mean_absolute_error: 1.7279\n",
      "Epoch 878/1000\n",
      "17/17 [==============================] - 4s 220ms/step - loss: 0.2790 - mean_absolute_error: 0.2790 - val_loss: 1.7295 - val_mean_absolute_error: 1.7295\n",
      "Epoch 879/1000\n",
      "17/17 [==============================] - 3s 190ms/step - loss: 0.2792 - mean_absolute_error: 0.2792 - val_loss: 1.7299 - val_mean_absolute_error: 1.7299\n",
      "Epoch 880/1000\n",
      "17/17 [==============================] - 3s 193ms/step - loss: 0.2803 - mean_absolute_error: 0.2803 - val_loss: 1.7325 - val_mean_absolute_error: 1.7325\n",
      "Epoch 881/1000\n",
      "17/17 [==============================] - 3s 177ms/step - loss: 0.2798 - mean_absolute_error: 0.2798 - val_loss: 1.7262 - val_mean_absolute_error: 1.7262\n",
      "Epoch 882/1000\n",
      "17/17 [==============================] - 3s 159ms/step - loss: 0.2824 - mean_absolute_error: 0.2824 - val_loss: 1.7314 - val_mean_absolute_error: 1.7314\n",
      "Epoch 883/1000\n",
      "17/17 [==============================] - 3s 158ms/step - loss: 0.2827 - mean_absolute_error: 0.2827 - val_loss: 1.7312 - val_mean_absolute_error: 1.7312\n",
      "Epoch 884/1000\n",
      "17/17 [==============================] - 2s 144ms/step - loss: 0.2817 - mean_absolute_error: 0.2817 - val_loss: 1.7268 - val_mean_absolute_error: 1.7268\n",
      "Epoch 885/1000\n",
      "17/17 [==============================] - 2s 139ms/step - loss: 0.2797 - mean_absolute_error: 0.2797 - val_loss: 1.7276 - val_mean_absolute_error: 1.7276\n",
      "Epoch 886/1000\n",
      "17/17 [==============================] - 2s 136ms/step - loss: 0.2812 - mean_absolute_error: 0.2812 - val_loss: 1.7269 - val_mean_absolute_error: 1.7269\n",
      "Epoch 887/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.2833 - mean_absolute_error: 0.2833 - val_loss: 1.7272 - val_mean_absolute_error: 1.7272\n",
      "Epoch 888/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.2814 - mean_absolute_error: 0.2814 - val_loss: 1.7261 - val_mean_absolute_error: 1.7261\n",
      "Epoch 889/1000\n",
      "17/17 [==============================] - 2s 116ms/step - loss: 0.2812 - mean_absolute_error: 0.2812 - val_loss: 1.7291 - val_mean_absolute_error: 1.7291\n",
      "Epoch 890/1000\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.2785 - mean_absolute_error: 0.2785 - val_loss: 1.7285 - val_mean_absolute_error: 1.7285\n",
      "Epoch 891/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.2814 - mean_absolute_error: 0.2814 - val_loss: 1.7312 - val_mean_absolute_error: 1.7312\n",
      "Epoch 892/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.2820 - mean_absolute_error: 0.2820 - val_loss: 1.7320 - val_mean_absolute_error: 1.7320\n",
      "Epoch 893/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.2811 - mean_absolute_error: 0.2811 - val_loss: 1.7267 - val_mean_absolute_error: 1.7267\n",
      "Epoch 894/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.2809 - mean_absolute_error: 0.2809 - val_loss: 1.7265 - val_mean_absolute_error: 1.7265\n",
      "Epoch 895/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.2789 - mean_absolute_error: 0.2789 - val_loss: 1.7325 - val_mean_absolute_error: 1.7325\n",
      "Epoch 896/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.2786 - mean_absolute_error: 0.2786 - val_loss: 1.7317 - val_mean_absolute_error: 1.7317\n",
      "Epoch 897/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.2788 - mean_absolute_error: 0.2788 - val_loss: 1.7294 - val_mean_absolute_error: 1.7294\n",
      "Epoch 898/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.2789 - mean_absolute_error: 0.2789 - val_loss: 1.7310 - val_mean_absolute_error: 1.7310\n",
      "Epoch 899/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.2808 - mean_absolute_error: 0.2808 - val_loss: 1.7300 - val_mean_absolute_error: 1.7300\n",
      "Epoch 900/1000\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.2796 - mean_absolute_error: 0.2796 - val_loss: 1.7296 - val_mean_absolute_error: 1.7296\n",
      "Epoch 901/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.2795 - mean_absolute_error: 0.2795 - val_loss: 1.7283 - val_mean_absolute_error: 1.7283\n",
      "Epoch 902/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2790 - mean_absolute_error: 0.2790 - val_loss: 1.7302 - val_mean_absolute_error: 1.7302\n",
      "Epoch 903/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.2791 - mean_absolute_error: 0.2791 - val_loss: 1.7273 - val_mean_absolute_error: 1.7273\n",
      "Epoch 904/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.2771 - mean_absolute_error: 0.2771 - val_loss: 1.7280 - val_mean_absolute_error: 1.7280\n",
      "Epoch 905/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2770 - mean_absolute_error: 0.2770 - val_loss: 1.7268 - val_mean_absolute_error: 1.7268\n",
      "Epoch 906/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.2793 - mean_absolute_error: 0.2793 - val_loss: 1.7306 - val_mean_absolute_error: 1.7306\n",
      "Epoch 907/1000\n",
      "17/17 [==============================] - 2s 108ms/step - loss: 0.2797 - mean_absolute_error: 0.2797 - val_loss: 1.7275 - val_mean_absolute_error: 1.7275\n",
      "Epoch 908/1000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2837 - mean_absolute_error: 0.2837 - val_loss: 1.7271 - val_mean_absolute_error: 1.7271\n",
      "Epoch 909/1000\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.2823 - mean_absolute_error: 0.2823 - val_loss: 1.7275 - val_mean_absolute_error: 1.7275\n",
      "Epoch 910/1000\n",
      "17/17 [==============================] - 2s 115ms/step - loss: 0.2785 - mean_absolute_error: 0.2785 - val_loss: 1.7286 - val_mean_absolute_error: 1.7286\n",
      "Epoch 911/1000\n",
      "17/17 [==============================] - 2s 111ms/step - loss: 0.2777 - mean_absolute_error: 0.2777 - val_loss: 1.7303 - val_mean_absolute_error: 1.7303\n",
      "Epoch 912/1000\n",
      "17/17 [==============================] - 2s 113ms/step - loss: 0.2780 - mean_absolute_error: 0.2780 - val_loss: 1.7273 - val_mean_absolute_error: 1.7273\n",
      "Epoch 913/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2793 - mean_absolute_error: 0.2793 - val_loss: 1.7254 - val_mean_absolute_error: 1.7254\n",
      "Epoch 914/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.2794 - mean_absolute_error: 0.2794 - val_loss: 1.7287 - val_mean_absolute_error: 1.7287\n",
      "Epoch 915/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.2772 - mean_absolute_error: 0.2772 - val_loss: 1.7328 - val_mean_absolute_error: 1.7328\n",
      "Epoch 916/1000\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.2770 - mean_absolute_error: 0.2770 - val_loss: 1.7270 - val_mean_absolute_error: 1.7270\n",
      "Epoch 917/1000\n",
      "17/17 [==============================] - 2s 117ms/step - loss: 0.2772 - mean_absolute_error: 0.2772 - val_loss: 1.7279 - val_mean_absolute_error: 1.7279\n",
      "Epoch 918/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.2791 - mean_absolute_error: 0.2791 - val_loss: 1.7306 - val_mean_absolute_error: 1.7306\n",
      "Epoch 919/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.2800 - mean_absolute_error: 0.2800 - val_loss: 1.7293 - val_mean_absolute_error: 1.7293\n",
      "Epoch 920/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.2803 - mean_absolute_error: 0.2803 - val_loss: 1.7307 - val_mean_absolute_error: 1.7307\n",
      "Epoch 921/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.2795 - mean_absolute_error: 0.2795 - val_loss: 1.7286 - val_mean_absolute_error: 1.7286\n",
      "Epoch 922/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2776 - mean_absolute_error: 0.2776 - val_loss: 1.7304 - val_mean_absolute_error: 1.7304\n",
      "Epoch 923/1000\n",
      "17/17 [==============================] - 2s 114ms/step - loss: 0.2777 - mean_absolute_error: 0.2777 - val_loss: 1.7320 - val_mean_absolute_error: 1.7320\n",
      "Epoch 924/1000\n",
      "17/17 [==============================] - 2s 106ms/step - loss: 0.2808 - mean_absolute_error: 0.2808 - val_loss: 1.7287 - val_mean_absolute_error: 1.7287\n",
      "Epoch 925/1000\n",
      "17/17 [==============================] - 2s 109ms/step - loss: 0.2791 - mean_absolute_error: 0.2791 - val_loss: 1.7296 - val_mean_absolute_error: 1.7296\n",
      "Epoch 926/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.2788 - mean_absolute_error: 0.2788 - val_loss: 1.7286 - val_mean_absolute_error: 1.7286\n",
      "Epoch 927/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2831 - mean_absolute_error: 0.2831 - val_loss: 1.7256 - val_mean_absolute_error: 1.7256\n",
      "Epoch 928/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.2824 - mean_absolute_error: 0.2824 - val_loss: 1.7287 - val_mean_absolute_error: 1.7287\n",
      "Epoch 929/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2779 - mean_absolute_error: 0.2779 - val_loss: 1.7294 - val_mean_absolute_error: 1.7294\n",
      "Epoch 930/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2781 - mean_absolute_error: 0.2781 - val_loss: 1.7298 - val_mean_absolute_error: 1.7298\n",
      "Epoch 931/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2767 - mean_absolute_error: 0.2767 - val_loss: 1.7326 - val_mean_absolute_error: 1.7326\n",
      "Epoch 932/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2772 - mean_absolute_error: 0.2772 - val_loss: 1.7311 - val_mean_absolute_error: 1.7311\n",
      "Epoch 933/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.2766 - mean_absolute_error: 0.2766 - val_loss: 1.7288 - val_mean_absolute_error: 1.7288\n",
      "Epoch 934/1000\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.2767 - mean_absolute_error: 0.2767 - val_loss: 1.7309 - val_mean_absolute_error: 1.7309\n",
      "Epoch 935/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.2784 - mean_absolute_error: 0.2784 - val_loss: 1.7294 - val_mean_absolute_error: 1.7294\n",
      "Epoch 936/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.2774 - mean_absolute_error: 0.2774 - val_loss: 1.7288 - val_mean_absolute_error: 1.7288\n",
      "Epoch 937/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2782 - mean_absolute_error: 0.2782 - val_loss: 1.7296 - val_mean_absolute_error: 1.7296\n",
      "Epoch 938/1000\n",
      "17/17 [==============================] - 3s 155ms/step - loss: 0.2788 - mean_absolute_error: 0.2788 - val_loss: 1.7275 - val_mean_absolute_error: 1.7275\n",
      "Epoch 939/1000\n",
      "17/17 [==============================] - 3s 204ms/step - loss: 0.2800 - mean_absolute_error: 0.2800 - val_loss: 1.7293 - val_mean_absolute_error: 1.7293\n",
      "Epoch 940/1000\n",
      "17/17 [==============================] - 3s 197ms/step - loss: 0.2787 - mean_absolute_error: 0.2787 - val_loss: 1.7302 - val_mean_absolute_error: 1.7302\n",
      "Epoch 941/1000\n",
      "17/17 [==============================] - 4s 208ms/step - loss: 0.2785 - mean_absolute_error: 0.2785 - val_loss: 1.7284 - val_mean_absolute_error: 1.7284\n",
      "Epoch 942/1000\n",
      "17/17 [==============================] - 3s 198ms/step - loss: 0.2772 - mean_absolute_error: 0.2772 - val_loss: 1.7260 - val_mean_absolute_error: 1.7260\n",
      "Epoch 943/1000\n",
      "17/17 [==============================] - 3s 185ms/step - loss: 0.2778 - mean_absolute_error: 0.2778 - val_loss: 1.7300 - val_mean_absolute_error: 1.7300\n",
      "Epoch 944/1000\n",
      "17/17 [==============================] - 3s 163ms/step - loss: 0.2776 - mean_absolute_error: 0.2776 - val_loss: 1.7265 - val_mean_absolute_error: 1.7265\n",
      "Epoch 945/1000\n",
      "17/17 [==============================] - 3s 178ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - val_loss: 1.7266 - val_mean_absolute_error: 1.7266\n",
      "Epoch 946/1000\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.2764 - mean_absolute_error: 0.2764 - val_loss: 1.7280 - val_mean_absolute_error: 1.7280\n",
      "Epoch 947/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - val_loss: 1.7278 - val_mean_absolute_error: 1.7278\n",
      "Epoch 948/1000\n",
      "17/17 [==============================] - 2s 119ms/step - loss: 0.2767 - mean_absolute_error: 0.2767 - val_loss: 1.7301 - val_mean_absolute_error: 1.7301\n",
      "Epoch 949/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.2824 - mean_absolute_error: 0.2824 - val_loss: 1.7359 - val_mean_absolute_error: 1.7359\n",
      "Epoch 950/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2835 - mean_absolute_error: 0.2835 - val_loss: 1.7307 - val_mean_absolute_error: 1.7307\n",
      "Epoch 951/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.2784 - mean_absolute_error: 0.2784 - val_loss: 1.7272 - val_mean_absolute_error: 1.7272\n",
      "Epoch 952/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.2789 - mean_absolute_error: 0.2789 - val_loss: 1.7276 - val_mean_absolute_error: 1.7276\n",
      "Epoch 953/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.2782 - mean_absolute_error: 0.2782 - val_loss: 1.7242 - val_mean_absolute_error: 1.7242\n",
      "Epoch 954/1000\n",
      "17/17 [==============================] - 2s 121ms/step - loss: 0.2789 - mean_absolute_error: 0.2789 - val_loss: 1.7261 - val_mean_absolute_error: 1.7261\n",
      "Epoch 955/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2773 - mean_absolute_error: 0.2773 - val_loss: 1.7252 - val_mean_absolute_error: 1.7252\n",
      "Epoch 956/1000\n",
      "17/17 [==============================] - 2s 118ms/step - loss: 0.2761 - mean_absolute_error: 0.2761 - val_loss: 1.7242 - val_mean_absolute_error: 1.7242\n",
      "Epoch 957/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2802 - mean_absolute_error: 0.2802 - val_loss: 1.7232 - val_mean_absolute_error: 1.7232\n",
      "Epoch 958/1000\n",
      "17/17 [==============================] - 2s 137ms/step - loss: 0.2813 - mean_absolute_error: 0.2813 - val_loss: 1.7250 - val_mean_absolute_error: 1.7250\n",
      "Epoch 959/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.2801 - mean_absolute_error: 0.2801 - val_loss: 1.7253 - val_mean_absolute_error: 1.7253\n",
      "Epoch 960/1000\n",
      "17/17 [==============================] - 2s 129ms/step - loss: 0.2796 - mean_absolute_error: 0.2796 - val_loss: 1.7274 - val_mean_absolute_error: 1.7274\n",
      "Epoch 961/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2762 - mean_absolute_error: 0.2762 - val_loss: 1.7280 - val_mean_absolute_error: 1.7280\n",
      "Epoch 962/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2753 - mean_absolute_error: 0.2753 - val_loss: 1.7268 - val_mean_absolute_error: 1.7268\n",
      "Epoch 963/1000\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.2755 - mean_absolute_error: 0.2755 - val_loss: 1.7261 - val_mean_absolute_error: 1.7261\n",
      "Epoch 964/1000\n",
      "17/17 [==============================] - 2s 123ms/step - loss: 0.2780 - mean_absolute_error: 0.2780 - val_loss: 1.7264 - val_mean_absolute_error: 1.7264\n",
      "Epoch 965/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.2761 - mean_absolute_error: 0.2761 - val_loss: 1.7308 - val_mean_absolute_error: 1.7308\n",
      "Epoch 966/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.2770 - mean_absolute_error: 0.2770 - val_loss: 1.7353 - val_mean_absolute_error: 1.7353\n",
      "Epoch 967/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2803 - mean_absolute_error: 0.2803 - val_loss: 1.7283 - val_mean_absolute_error: 1.7283\n",
      "Epoch 968/1000\n",
      "17/17 [==============================] - 2s 137ms/step - loss: 0.2781 - mean_absolute_error: 0.2781 - val_loss: 1.7304 - val_mean_absolute_error: 1.7304\n",
      "Epoch 969/1000\n",
      "17/17 [==============================] - 2s 140ms/step - loss: 0.2777 - mean_absolute_error: 0.2777 - val_loss: 1.7325 - val_mean_absolute_error: 1.7325\n",
      "Epoch 970/1000\n",
      "17/17 [==============================] - 2s 137ms/step - loss: 0.2819 - mean_absolute_error: 0.2819 - val_loss: 1.7305 - val_mean_absolute_error: 1.7305\n",
      "Epoch 971/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.2781 - mean_absolute_error: 0.2781 - val_loss: 1.7274 - val_mean_absolute_error: 1.7274\n",
      "Epoch 972/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.2767 - mean_absolute_error: 0.2767 - val_loss: 1.7298 - val_mean_absolute_error: 1.7298\n",
      "Epoch 973/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2815 - mean_absolute_error: 0.2815 - val_loss: 1.7300 - val_mean_absolute_error: 1.7300\n",
      "Epoch 974/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2799 - mean_absolute_error: 0.2799 - val_loss: 1.7293 - val_mean_absolute_error: 1.7293\n",
      "Epoch 975/1000\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 0.2766 - mean_absolute_error: 0.2766 - val_loss: 1.7277 - val_mean_absolute_error: 1.7277\n",
      "Epoch 976/1000\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.2752 - mean_absolute_error: 0.2752 - val_loss: 1.7307 - val_mean_absolute_error: 1.7307\n",
      "Epoch 977/1000\n",
      "17/17 [==============================] - 2s 137ms/step - loss: 0.2784 - mean_absolute_error: 0.2784 - val_loss: 1.7273 - val_mean_absolute_error: 1.7273\n",
      "Epoch 978/1000\n",
      "17/17 [==============================] - 2s 134ms/step - loss: 0.2767 - mean_absolute_error: 0.2767 - val_loss: 1.7294 - val_mean_absolute_error: 1.7294\n",
      "Epoch 979/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2747 - mean_absolute_error: 0.2747 - val_loss: 1.7296 - val_mean_absolute_error: 1.7296\n",
      "Epoch 980/1000\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.2759 - mean_absolute_error: 0.2759 - val_loss: 1.7269 - val_mean_absolute_error: 1.7269\n",
      "Epoch 981/1000\n",
      "17/17 [==============================] - 2s 145ms/step - loss: 0.2769 - mean_absolute_error: 0.2769 - val_loss: 1.7283 - val_mean_absolute_error: 1.7283\n",
      "Epoch 982/1000\n",
      "17/17 [==============================] - 2s 138ms/step - loss: 0.2770 - mean_absolute_error: 0.2770 - val_loss: 1.7291 - val_mean_absolute_error: 1.7291\n",
      "Epoch 983/1000\n",
      "17/17 [==============================] - 2s 141ms/step - loss: 0.2757 - mean_absolute_error: 0.2757 - val_loss: 1.7313 - val_mean_absolute_error: 1.7313\n",
      "Epoch 984/1000\n",
      "17/17 [==============================] - 2s 137ms/step - loss: 0.2782 - mean_absolute_error: 0.2782 - val_loss: 1.7324 - val_mean_absolute_error: 1.7324\n",
      "Epoch 985/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2760 - mean_absolute_error: 0.2760 - val_loss: 1.7278 - val_mean_absolute_error: 1.7278\n",
      "Epoch 986/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2747 - mean_absolute_error: 0.2747 - val_loss: 1.7283 - val_mean_absolute_error: 1.7283\n",
      "Epoch 987/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2760 - mean_absolute_error: 0.2760 - val_loss: 1.7347 - val_mean_absolute_error: 1.7347\n",
      "Epoch 988/1000\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.2837 - mean_absolute_error: 0.2837 - val_loss: 1.7323 - val_mean_absolute_error: 1.7323\n",
      "Epoch 989/1000\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.2780 - mean_absolute_error: 0.2780 - val_loss: 1.7283 - val_mean_absolute_error: 1.7283\n",
      "Epoch 990/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2761 - mean_absolute_error: 0.2761 - val_loss: 1.7319 - val_mean_absolute_error: 1.7319\n",
      "Epoch 991/1000\n",
      "17/17 [==============================] - 2s 128ms/step - loss: 0.2764 - mean_absolute_error: 0.2764 - val_loss: 1.7261 - val_mean_absolute_error: 1.7261\n",
      "Epoch 992/1000\n",
      "17/17 [==============================] - 3s 153ms/step - loss: 0.2740 - mean_absolute_error: 0.2740 - val_loss: 1.7270 - val_mean_absolute_error: 1.7270\n",
      "Epoch 993/1000\n",
      "17/17 [==============================] - 2s 147ms/step - loss: 0.2740 - mean_absolute_error: 0.2740 - val_loss: 1.7281 - val_mean_absolute_error: 1.7281\n",
      "Epoch 994/1000\n",
      "17/17 [==============================] - 2s 148ms/step - loss: 0.2746 - mean_absolute_error: 0.2746 - val_loss: 1.7293 - val_mean_absolute_error: 1.7293\n",
      "Epoch 995/1000\n",
      "17/17 [==============================] - 2s 141ms/step - loss: 0.2748 - mean_absolute_error: 0.2748 - val_loss: 1.7250 - val_mean_absolute_error: 1.7250\n",
      "Epoch 996/1000\n",
      "17/17 [==============================] - 2s 137ms/step - loss: 0.2772 - mean_absolute_error: 0.2772 - val_loss: 1.7260 - val_mean_absolute_error: 1.7260\n",
      "Epoch 997/1000\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.2768 - mean_absolute_error: 0.2768 - val_loss: 1.7275 - val_mean_absolute_error: 1.7275\n",
      "Epoch 998/1000\n",
      "17/17 [==============================] - 2s 120ms/step - loss: 0.2784 - mean_absolute_error: 0.2784 - val_loss: 1.7256 - val_mean_absolute_error: 1.7256\n",
      "Epoch 999/1000\n",
      "17/17 [==============================] - 3s 151ms/step - loss: 0.2797 - mean_absolute_error: 0.2797 - val_loss: 1.7367 - val_mean_absolute_error: 1.7367\n",
      "Epoch 1000/1000\n",
      "17/17 [==============================] - 2s 122ms/step - loss: 0.2779 - mean_absolute_error: 0.2779 - val_loss: 1.7313 - val_mean_absolute_error: 1.7313\n",
      "Evaluate on test data\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 1.7362 - mean_absolute_error: 1.7362\n",
      "test loss, test acc: [1.736152172088623, 1.736152172088623]\n",
      "Generate predictions for 3 samples\n",
      "1/1 [==============================] - 1s 536ms/step\n",
      "predictions shape: (3, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_data(train_dataset, test_dataset, label):\n",
    "    \n",
    "    # Break into target and features\n",
    "    train_dataset = train_dataset.dropna(subset=[label])\n",
    "    test_dataset = test_dataset.dropna(subset=[label])\n",
    "\n",
    "    # only the peptide percentage in the protein\n",
    "    train_features = train_dataset.drop(target, axis=1).copy()\n",
    "    test_features = test_dataset.drop(target, axis=1).copy()\n",
    "\n",
    "    # only the updrs\n",
    "    train_labels = train_dataset[label]\n",
    "    test_labels = test_dataset[label]\n",
    "    \n",
    "    # Fill the Nan values by the mean value of the column\n",
    "    for c in train_features.columns:\n",
    "        m = train_features[c].mean()\n",
    "        train_features[c] = train_features[c].fillna(m)\n",
    "\n",
    "    for c in test_features.columns:\n",
    "        m = test_features[c].mean()\n",
    "        test_features[c] = test_features[c].fillna(m)\n",
    "        \n",
    "    return train_features, test_features, train_labels, test_labels\n",
    "\n",
    "# target = [\"updrs_1\", \"updrs_2\", \"updrs_3\", \"updrs_4\"]\n",
    "target = [\"updrs_4\"]\n",
    "\n",
    "train_dataset = df1.sample(frac=0.5, random_state=0)\n",
    "test_dataset = df1.drop(train_dataset.index)\n",
    "\n",
    "for label in target:\n",
    "\n",
    "    train_features, test_features, train_labels, test_labels = prepare_data(train_dataset, test_dataset,label)\n",
    "    train__features_val = train_features[-200:]\n",
    "    train_labels_val = train_labels[-200:]\n",
    "    \n",
    "    # Normalization\n",
    "\n",
    "    features = np.array(train_features)\n",
    "    feat_normalizer = layers.Normalization(axis=-1)\n",
    "    feat_normalizer.adapt(features)\n",
    "\n",
    "    # Model\n",
    "    model = tf.keras.Sequential([\n",
    "        feat_normalizer,\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(969, activation='relu'),\n",
    "        layers.Dense(484, activation='relu'),\n",
    "        layers.Dense(242, activation='relu'),\n",
    "        layers.Dense(121, activation='relu'),\n",
    "        layers.Dense(60, activation='relu'),\n",
    "        layers.Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    # Training\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6),\n",
    "        loss='mean_absolute_error',\n",
    "        metrics=\"mean_absolute_error\"\n",
    "        )\n",
    "\n",
    "    # Fitting\n",
    "    history = model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        batch_size=64,\n",
    "        epochs=1000,\n",
    "        # Calculate validation results on 20% of the training data.\n",
    "        validation_split = 0.2,\n",
    "        )\n",
    "\n",
    "    # Evaluate the model on the test data using `evaluate`\n",
    "    print(\"Evaluate on test data\")\n",
    "    results = model.evaluate(test_features, test_labels, batch_size=128)\n",
    "    print(\"test loss, test acc:\", results)\n",
    "\n",
    "    # Generate predictions (probabilities -- the output of the last layer)\n",
    "    # on new data using `predict`\n",
    "    print(\"Generate predictions for 3 samples\")\n",
    "    predictions = model.predict(test_features[:3])\n",
    "    print(\"predictions shape:\", predictions.shape\n",
    "    )\n",
    "    \n",
    "    # summarize history for mean_absolute_percentage_error\n",
    "    plt.plot(history.history['mean_absolute_error'])\n",
    "    plt.plot(history.history['val_mean_absolute_error'])\n",
    "    plt.title('model mean_absolute_error')\n",
    "    plt.ylabel('mean_absolute_error')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
